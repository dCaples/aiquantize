{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random MIDI file: ./lmd_full/6/60f4f7f37aa4dae34d541673cfc956ff.mid\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_random_midi_file(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    \n",
    "    return random.choice(midi_files)\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "random_midi_file = get_random_midi_file(root_directory)\n",
    "print(f\"Random MIDI file: {random_midi_file}\")\n",
    "\n",
    "filename = random_midi_file\n",
    "\n",
    "def get_random_pretty_midi_file():\n",
    "    filename = get_random_midi_file(root_directory)\n",
    "    return pretty_midi.PrettyMIDI(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegocaples/miniconda3/envs/ai/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file: data byte must be in range 0..127\n",
      "Error reading file: data byte must be in range 0..127\n",
      "Total memory used: 5545296 bytes\n",
      "Total memory used: 5.2884063720703125 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def process_midi_data(midi_data):\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32)\n",
    "\n",
    "song_lengths = []\n",
    "songs = []\n",
    "for i in range(100):\n",
    "    try:\n",
    "        midi_data = get_random_pretty_midi_file()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        continue\n",
    "\n",
    "    song = process_midi_data(midi_data)\n",
    "    songs.append(song)\n",
    "\n",
    "def get_memory_used(tensor):\n",
    "    return tensor.element_size() * tensor.nelement()\n",
    "\n",
    "total_memory = 0\n",
    "for song in songs:\n",
    "    total_memory += get_memory_used(song)\n",
    "\n",
    "print(f\"Total memory used: {total_memory} bytes\")\n",
    "# in MB\n",
    "print(f\"Total memory used: {total_memory / 1024 / 1024} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPQN: 96\n",
      "Time per tick: 0.005208333333333333\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "\n",
    "def process_midi(filename, track_index=0):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "\n",
    "    # Print the TPQN (Ticks Per Quarter Note)\n",
    "    print(\"TPQN:\", midi_data.resolution)\n",
    "\n",
    "    # Print the time per tick\n",
    "    time_per_tick = midi_data.tick_to_time(1)\n",
    "    print(\"Time per tick:\", time_per_tick)\n",
    "\n",
    "    # Get the specified track of the MIDI file\n",
    "    track = midi_data.instruments[track_index]\n",
    "\n",
    "    # Get all the \"note on\" messages in the track\n",
    "    note_on_messages = [note for note in track.notes if note.velocity > 0]\n",
    "\n",
    "    # Assuming 4/4 time signature\n",
    "    ticks_per_measure = 4 * midi_data.resolution\n",
    "\n",
    "    # Create a list to store the annotated notes\n",
    "    annotated_notes = []\n",
    "\n",
    "    # Iterate over each note on message\n",
    "    for i, note in enumerate(note_on_messages):\n",
    "        # Get the measure index of the note\n",
    "        tick = midi_data.time_to_tick(note.start)\n",
    "        measure_index = tick // ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last measure for the note\n",
    "        ticks_since_last_measure = tick % ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last note\n",
    "        if i > 0:\n",
    "            ticks_since_last_note = tick - midi_data.time_to_tick(note_on_messages[i-1].start)\n",
    "        else:\n",
    "            ticks_since_last_note = tick\n",
    "\n",
    "        # Create a dictionary to store the annotated note information\n",
    "        annotated_note = {\n",
    "            'note': note,\n",
    "            'measure_index': measure_index,\n",
    "            'ticks_since_last_measure': ticks_since_last_measure,\n",
    "            'ticks_since_last_note': ticks_since_last_note\n",
    "        }\n",
    "\n",
    "        # Append the annotated note to the list\n",
    "        annotated_notes.append(annotated_note)\n",
    "\n",
    "    # Print the first 10 annotated notes\n",
    "    # for i, annotated_note in enumerate(annotated_notes[:10]):\n",
    "    #     print(\"Note:\", annotated_note['note'])\n",
    "    #     print(\"Measure Index:\", annotated_note['measure_index'])\n",
    "    #     print(\"Ticks since last measure:\", annotated_note['ticks_since_last_measure'])\n",
    "    #     print(\"Ticks since last note:\", annotated_note['ticks_since_last_note'])\n",
    "    #     print(\"---\")\n",
    "\n",
    "    return annotated_notes\n",
    "\n",
    "# Usage\n",
    "filename = random_midi_file\n",
    "track_index = 0\n",
    "\n",
    "annotated_notes = process_midi(filename, track_index)\n",
    "print(len(annotated_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "# class for a song\n",
    "\n",
    "class Song:\n",
    "    def __init__(self, filename):\n",
    "        midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "        self.time_per_measure = midi_data.tick_to_time(midi_data.resolution * 4)\n",
    "        self.tracks = []\n",
    "        for track in midi_data.instruments:\n",
    "            track_data = []\n",
    "            for note in track.notes:\n",
    "                start_tick = note.start\n",
    "                pitch = note.pitch\n",
    "                track_data.append([start_tick, pitch])\n",
    "            # add the track data to the list of tracks in a torch tensor\n",
    "            self.tracks.append(torch.tensor(track_data, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m songs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m errors_processing \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPool\u001b[49m(cpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     24\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tqdm(pool\u001b[38;5;241m.\u001b[39mimap(process_midi_file, midi_files), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(midi_files)))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m song, error \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pool' is not defined"
     ]
    }
   ],
   "source": [
    "def process_midi_file(filename):\n",
    "    try:\n",
    "        song = Song(filename)\n",
    "        return song, None\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "\n",
    "def collect_midi_files(root_directory):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    return midi_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_directory = './lmd_full'\n",
    "    midi_files = collect_midi_files(root_directory)\n",
    "\n",
    "    songs = []\n",
    "    errors_processing = 0\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(process_midi_file, midi_files), total=len(midi_files)))\n",
    "\n",
    "    for song, error in results:\n",
    "        if song:\n",
    "            songs.append(song)\n",
    "        if error:\n",
    "            errors_processing += 1\n",
    "\n",
    "    print(f\"Errors processing: {errors_processing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of MIDI files: 178561\n",
      "MThd not found. Probably not a MIDI file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegocaples/miniconda3/envs/ai/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of notes: 380165\n",
      "notes per file: 3801.65\n"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "import os\n",
    "root_directory = './lmd_full'\n",
    "midi_files = []\n",
    "for dirpath, _, filenames in os.walk(root_directory):\n",
    "    for filename in filenames:\n",
    "\n",
    "        if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "            midi_files.append(\n",
    "                os.path.join(dirpath, filename)\n",
    "            )\n",
    "\n",
    "songs_to_count = 100\n",
    "print(f\"Total number of MIDI files: {len(midi_files)}\")\n",
    "errors_processing = 0\n",
    "total_number_of_notes = 0\n",
    "for i, midi_data in enumerate(midi_files):\n",
    "    try:\n",
    "        # get the total number of notes\n",
    "        notes_per_song = 0\n",
    "        midi_file = pretty_midi.PrettyMIDI(midi_data)\n",
    "        for track in midi_file.instruments:\n",
    "            notes_per_song += len(track.notes)\n",
    "        # print(f\"notes per song: {notes_per_song}\")\n",
    "        total_number_of_notes += notes_per_song\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors_processing += 1\n",
    "    if i > songs_to_count:\n",
    "        break\n",
    "\n",
    "    \n",
    "print(f\"Total number of notes: {total_number_of_notes}\")\n",
    "notes_per_file = total_number_of_notes / songs_to_count\n",
    "print(f\"notes per file: {notes_per_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "songs = []\n",
    "errors_processing = 0\n",
    "for midi_data in tqdm(midi_files):\n",
    "    try:\n",
    "        song = Song(midi_data)\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        errors_processing += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# Generate some random data\n",
    "data = torch.randn(100, 3, 32, 32)  # 100 samples, 3 channels, 32x32 images\n",
    "labels = torch.randint(0, 10, (100,))  # 100 labels for 10 classes\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MyDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch in dataloader:\n",
    "    data_batch = batch['data']\n",
    "    labels_batch = batch['label']\n",
    "    print(data_batch.shape, labels_batch.shape)\n",
    "    # Add your training code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SongData(Dataset):\n",
    "    def __init__(self, songs):\n",
    "        self.songs = songs\n",
    "        self.global_track_id_to_song_id = []\n",
    "        for i, song in enumerate(songs):\n",
    "            for track in song.instruments:\n",
    "                self.global_track_id_to_song_id.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_track_id_to_song_id)\n",
    "\n",
    "    def __getitem__(self, instance_idx):  \n",
    "        song_idx = self.global_track_id_to_song_id[instance_idx]\n",
    "        song = self.songs[song_idx]\n",
    "        track_idx = instance_idx - song_idx\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# Generate some random data\n",
    "data = torch.randn(100, 3, 32, 32)  # 100 samples, 3 channels, 32x32 images\n",
    "labels = torch.randint(0, 10, (100,))  # 100 labels for 10 classes\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MyDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch in dataloader:\n",
    "    data_batch = batch['data']\n",
    "    labels_batch = batch['label']\n",
    "    print(data_batch.shape, labels_batch.shape)\n",
    "    # Add your training code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4972757\n",
      "TPQN: 384\n",
      "Time per tick: 0.0015625\n",
      "103823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Instrument(program=57, is_drum=False, name=\"Melodie 4\"),\n",
       " Instrument(program=42, is_drum=False, name=\"Violoncl2\"),\n",
       " Instrument(program=24, is_drum=False, name=\"GuitarAc3\"),\n",
       " Instrument(program=24, is_drum=False, name=\"GuitarAc5\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "def get_recursive_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds the total memory usage of an object.\"\"\"\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    \n",
    "    obj_id = id(obj)\n",
    "    \n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    \n",
    "    seen.add(obj_id)\n",
    "    \n",
    "    size = sys.getsizeof(obj)\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(get_recursive_size(v, seen) for v in obj.values())\n",
    "        size += sum(get_recursive_size(k, seen) for k in obj.keys())\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_recursive_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum(get_recursive_size(i, seen) for i in obj)\n",
    "    \n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "example_object = [1, 2, {3: \"a\", 4: [\"b\", \"c\"]}]\n",
    "object_1 = [([1.1]*500) for _ in range(1000)]\n",
    "object_2 = [torch.tensor([1.1]*500) for _ in range(1000)]\n",
    "\n",
    "print(\"python\")\n",
    "print(get_recursive_size(object_1))\n",
    "print(\"tensors\")\n",
    "print(get_recursive_size(object_2))\n",
    "\n",
    "# # get the number of instruments\n",
    "# midi_data.instruments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing All Data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the filenames\n",
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_all_filenames(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    return midi_files\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "files = get_all_filenames(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: ./lmd_full/4/4b92c0017472387bb37419940849a6cb.mid\n",
      "first note:\n",
      "track: 0.0\n",
      "start: 2.0\n",
      "duration: 0.296875\n",
      "pitch: 69.0\n",
      "\n",
      "second note:\n",
      "track: 3.0\n",
      "start: 2.0\n",
      "duration: 0.296875\n",
      "pitch: 69.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "def process_midi_data(midi_filename):\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_filename)\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32), None\n",
    "\n",
    "# Usage\n",
    "filename = random.choice(files)\n",
    "song, error = process_midi_data(filename)\n",
    "print(f\"Filename: {filename}\")\n",
    "\n",
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "print(f\"first note:\")\n",
    "print(f\"track: {song[1][0]}\")\n",
    "print(f\"start: {song[1][1]}\")\n",
    "print(f\"duration: {song[1][2]}\")\n",
    "print(f\"pitch: {song[1][3]}\")\n",
    "\n",
    "print(f\"\\nsecond note:\")\n",
    "print(f\"track: {song[2][0]}\")\n",
    "print(f\"start: {song[2][1]}\")\n",
    "print(f\"duration: {song[2][2]}\")\n",
    "print(f\"pitch: {song[2][3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00% complete (0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegocaples/miniconda3/envs/ai/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01% complete (1000)\n",
      "0.01% complete (2000)\n",
      "0.02% complete (3000)\n",
      "0.02% complete (4000)\n",
      "0.03% complete (5000)\n",
      "0.03% complete (6000)\n",
      "0.04% complete (7000)\n",
      "0.04% complete (8000)\n",
      "0.05% complete (9000)\n",
      "0.06% complete (10000)\n",
      "0.06% complete (11000)\n",
      "0.07% complete (12000)\n",
      "0.07% complete (13000)\n",
      "0.08% complete (14000)\n",
      "0.08% complete (15000)\n",
      "0.09% complete (16000)\n",
      "0.10% complete (17000)\n",
      "0.10% complete (18000)\n",
      "0.11% complete (19000)\n",
      "0.11% complete (20000)\n",
      "0.12% complete (21000)\n",
      "0.12% complete (22000)\n",
      "0.13% complete (23000)\n",
      "0.13% complete (24000)\n",
      "unusual error: index -16307 is out of bounds for axis 0 with size 14508\n",
      "0.14% complete (25000)\n",
      "0.15% complete (26000)\n",
      "0.15% complete (27000)\n",
      "0.16% complete (28000)\n",
      "0.16% complete (29000)\n",
      "0.17% complete (30000)\n",
      "0.17% complete (31000)\n",
      "0.18% complete (32000)\n",
      "0.18% complete (33000)\n",
      "0.19% complete (34000)\n",
      "0.20% complete (35000)\n",
      "0.20% complete (36000)\n",
      "0.21% complete (37000)\n",
      "0.21% complete (38000)\n",
      "0.22% complete (39000)\n",
      "0.22% complete (40000)\n",
      "0.23% complete (41000)\n",
      "0.24% complete (42000)\n",
      "0.24% complete (43000)\n",
      "0.25% complete (44000)\n",
      "0.25% complete (45000)\n",
      "0.26% complete (46000)\n",
      "0.26% complete (47000)\n",
      "0.27% complete (48000)\n",
      "0.27% complete (49000)\n",
      "0.28% complete (50000)\n",
      "0.29% complete (51000)\n",
      "0.29% complete (52000)\n",
      "0.30% complete (53000)\n",
      "0.30% complete (54000)\n",
      "0.31% complete (55000)\n",
      "0.31% complete (56000)\n",
      "0.32% complete (57000)\n",
      "0.32% complete (58000)\n",
      "0.33% complete (59000)\n",
      "0.34% complete (60000)\n",
      "0.34% complete (61000)\n",
      "0.35% complete (62000)\n",
      "0.35% complete (63000)\n",
      "0.36% complete (64000)\n",
      "unusual error: index -7580 is out of bounds for axis 0 with size 2\n",
      "0.36% complete (65000)\n",
      "0.37% complete (66000)\n",
      "0.38% complete (67000)\n",
      "0.38% complete (68000)\n",
      "0.39% complete (69000)\n",
      "0.39% complete (70000)\n",
      "0.40% complete (71000)\n",
      "0.40% complete (72000)\n",
      "0.41% complete (73000)\n",
      "0.41% complete (74000)\n",
      "0.42% complete (75000)\n",
      "0.43% complete (76000)\n",
      "0.43% complete (77000)\n",
      "0.44% complete (78000)\n",
      "0.44% complete (79000)\n",
      "0.45% complete (80000)\n",
      "0.45% complete (81000)\n",
      "0.46% complete (82000)\n",
      "0.46% complete (83000)\n",
      "0.47% complete (84000)\n",
      "0.48% complete (85000)\n",
      "0.48% complete (86000)\n",
      "0.49% complete (87000)\n",
      "0.49% complete (88000)\n",
      "0.50% complete (89000)\n",
      "0.50% complete (90000)\n",
      "0.51% complete (91000)\n",
      "0.52% complete (92000)\n",
      "0.52% complete (93000)\n",
      "0.53% complete (94000)\n",
      "0.53% complete (95000)\n",
      "0.54% complete (96000)\n",
      "0.54% complete (97000)\n",
      "0.55% complete (98000)\n",
      "0.55% complete (99000)\n",
      "0.56% complete (100000)\n",
      "0.57% complete (101000)\n",
      "0.57% complete (102000)\n",
      "0.58% complete (103000)\n",
      "0.58% complete (104000)\n",
      "0.59% complete (105000)\n",
      "0.59% complete (106000)\n",
      "0.60% complete (107000)\n",
      "0.60% complete (108000)\n",
      "0.61% complete (109000)\n",
      "0.62% complete (110000)\n",
      "0.62% complete (111000)\n",
      "0.63% complete (112000)\n",
      "0.63% complete (113000)\n",
      "0.64% complete (114000)\n",
      "0.64% complete (115000)\n",
      "0.65% complete (116000)\n",
      "0.66% complete (117000)\n",
      "0.66% complete (118000)\n",
      "0.67% complete (119000)\n",
      "0.67% complete (120000)\n",
      "0.68% complete (121000)\n",
      "0.68% complete (122000)\n",
      "0.69% complete (123000)\n",
      "0.69% complete (124000)\n",
      "0.70% complete (125000)\n",
      "0.71% complete (126000)\n",
      "0.71% complete (127000)\n",
      "0.72% complete (128000)\n",
      "0.72% complete (129000)\n",
      "0.73% complete (130000)\n",
      "0.73% complete (131000)\n",
      "0.74% complete (132000)\n",
      "0.74% complete (133000)\n",
      "0.75% complete (134000)\n",
      "0.76% complete (135000)\n",
      "0.76% complete (136000)\n",
      "0.77% complete (137000)\n",
      "0.77% complete (138000)\n",
      "0.78% complete (139000)\n",
      "0.78% complete (140000)\n",
      "0.79% complete (141000)\n",
      "0.80% complete (142000)\n",
      "0.80% complete (143000)\n",
      "0.81% complete (144000)\n",
      "0.81% complete (145000)\n",
      "0.82% complete (146000)\n",
      "0.82% complete (147000)\n",
      "0.83% complete (148000)\n",
      "0.83% complete (149000)\n",
      "0.84% complete (150000)\n",
      "0.85% complete (151000)\n",
      "0.85% complete (152000)\n",
      "0.86% complete (153000)\n",
      "0.86% complete (154000)\n",
      "0.87% complete (155000)\n",
      "0.87% complete (156000)\n",
      "0.88% complete (157000)\n",
      "0.88% complete (158000)\n",
      "0.89% complete (159000)\n",
      "0.90% complete (160000)\n",
      "0.90% complete (161000)\n",
      "0.91% complete (162000)\n",
      "0.91% complete (163000)\n",
      "0.92% complete (164000)\n",
      "0.92% complete (165000)\n",
      "0.93% complete (166000)\n",
      "0.94% complete (167000)\n",
      "0.94% complete (168000)\n",
      "0.95% complete (169000)\n",
      "0.95% complete (170000)\n",
      "0.96% complete (171000)\n",
      "0.96% complete (172000)\n",
      "0.97% complete (173000)\n",
      "0.97% complete (174000)\n",
      "0.98% complete (175000)\n",
      "0.99% complete (176000)\n",
      "0.99% complete (177000)\n",
      "1.00% complete (178000)\n"
     ]
    }
   ],
   "source": [
    "# simple example of loading all the files (single process)\n",
    "errors = []\n",
    "songs = []\n",
    "totalsongs = len(files)\n",
    "for i, file in enumerate(files):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i/totalsongs:.2f}% complete ({i})\")\n",
    "    try:\n",
    "        song, error = process_midi_data(file)\n",
    "        if error is not None:\n",
    "            errors.append(error)\n",
    "            continue\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        print(f\"unusual error: {e}\")\n",
    "    # if i > 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of groups to split into\n",
    "num_groups = 10\n",
    "\n",
    "# Calculate the size of each group\n",
    "group_size = len(songs) // num_groups\n",
    "\n",
    "# Save each group directly\n",
    "for i in range(num_groups):\n",
    "    start_idx = i * group_size\n",
    "    end_idx = (i + 1) * group_size if i < num_groups - 1 else len(songs)\n",
    "    torch.save(songs[start_idx:end_idx], f'./dataset/song_group_{i}.pth')\n",
    "\n",
    "# Handle any remaining tensors if the list size isn't perfectly divisible\n",
    "if len(songs) % num_groups != 0:\n",
    "    torch.save(songs[num_groups * group_size:], f'./dataset/song_group_{num_groups - 1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# loading all the data\n",
    "# loaded_song_groups = []\n",
    "# for i in range(num_groups):\n",
    "#     loaded_song_groups.append(torch.load(f'./dataset/song_group_{i}.pth'))\n",
    "\n",
    "# # Optionally, you can concatenate all groups back into a single list if needed\n",
    "# loaded_songs = [song for group in loaded_song_groups for song in group]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the first group of tensors\n",
    "first_group = torch.load('./dataset/song_group_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7059,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  1.4118,  0.1765, 43.0000],\n",
       "        [ 0.0000,  1.4118,  1.0588, 55.0000],\n",
       "        ...,\n",
       "        [ 0.0000, 57.9706,  2.6471, 50.0000],\n",
       "        [ 0.0000, 58.0147,  2.6471, 55.0000],\n",
       "        [ 0.0000, 58.0588,  2.6471, 59.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "first_group[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how the dataset building would work on a mock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "train_idxs = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(0, len(song), test_context_length):\n",
    "        train_idxs.append([i, j])\n",
    "\n",
    "\n",
    "\n",
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "data = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(len(song) - test_context_length):\n",
    "        if j % test_context_length == 0:\n",
    "            data.append([i, j])\n",
    "\n",
    "\n",
    "def build_idxs(dataset, context_length, ignore_header=True):\n",
    "    idxs = []\n",
    "    if ignore_header:\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    for i, song in enumerate(dataset):\n",
    "        for j in range(start_idx, len(song) - context_length, context_length):\n",
    "            idxs.append([i, j])\n",
    "    return idxs\n",
    "\n",
    "# Usage\n",
    "test_context_length = 3\n",
    "dataset = [a, b]\n",
    "train_idxs = build_idxs(dataset, test_context_length)\n",
    "\n",
    "song, note_start = train_idxs[0]\n",
    "dataset[song][note_start:note_start + test_context_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is the first_group, but remove the first element of each song\n",
    "# first element is the header\n",
    "# each song is a tensor\n",
    "dataset = []\n",
    "for song in first_group:\n",
    "    dataset.append(song)\n",
    "\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "dataset_idxs = build_idxs(dataset, context_length)\n",
    "\n",
    "song, note_start = dataset_idxs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SongDataSet(Dataset):\n",
    "    def __init__(self, songs, context_length=128, quantize_divisor=64):\n",
    "        self.songs = songs\n",
    "        self.dataset_idxs = build_idxs(songs, context_length)\n",
    "        self.quantize_divisor = quantize_divisor\n",
    "        self.perturbation_std = 0.05\n",
    "        self.context_length = context_length\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_idxs)\n",
    "\n",
    "    def __getitem__(self, idx, different_perturbation_std=False):\n",
    "        song_idx, note_start_idx = self.dataset_idxs[idx]\n",
    "        song = self.songs[song_idx]\n",
    "        notes = song[note_start_idx:note_start_idx + context_length]\n",
    "        # Note format: [track, start, duration, pitch]\n",
    "\n",
    "        time_per_quarter_note = song[0][0] # header, first element\n",
    "        time_per_tick = (time_per_quarter_note * 4 / self.quantize_divisor).item() # quantize_divisor is ticks per measure\n",
    "\n",
    "        perturbed_notes = notes.clone()\n",
    "        # add gausian perturbations to start duration, note index 1 and 2\n",
    "        # return torch.normal(0, self.perturbation_std, perturbed_notes[:, 1].shape).shape\n",
    "        perturbation_std = self.perturbation_std\n",
    "        if different_perturbation_std is not False:\n",
    "            perturbation_std = different_perturbation_std\n",
    "        perturbed_notes[:, 1] += torch.normal(0, self.perturbation_std, perturbed_notes[:, 1].shape)\n",
    "        perturbed_notes[:, 2] += torch.normal(0, self.perturbation_std, perturbed_notes[:, 2].shape)\n",
    "        \n",
    "        \n",
    "        perturbed_notes_quantized = self.quantize_notes(perturbed_notes, time_per_tick)\n",
    "\n",
    "        notes_quantized = self.quantize_notes(notes, time_per_tick)\n",
    "\n",
    "        return {\"notes\": notes_quantized, \"perturbed_notes\": perturbed_notes_quantized, \"time_per_tick\": time_per_tick}\n",
    "        \n",
    "\n",
    "\n",
    "    def quantize_notes(self, notes, time_per_tick):\n",
    "        # subtract the start time of the note with the smallest start time from all notes (zero-based start times, even if first is negative or positive)\n",
    "        min_start_time = notes[:, 1].min()\n",
    "        notes[:, 1] -= min_start_time\n",
    "\n",
    "        quantized_notes = []\n",
    "        for note in notes:\n",
    "            quantized_note = self.quantize_note(note, time_per_tick)\n",
    "            quantized_notes.append(quantized_note) # quantizes to about 3/100 of a second (depending on the song's time signature)\n",
    "        \n",
    "        while len(quantized_notes) < self.context_length:\n",
    "            quantized_notes.append(torch.tensor([99999, 99999, 99999, 99999, 99999], dtype=torch.int32))\n",
    "        return torch.stack(quantized_notes)\n",
    "    \n",
    "        \n",
    "\n",
    "    def quantize_note(self, note, time_per_tick):\n",
    "        track, start, duration, pitch = note\n",
    "\n",
    "        track = track.item()\n",
    "        start = start.item()\n",
    "        duration = duration.item()\n",
    "        pitch = pitch.item()\n",
    "\n",
    "        start = round(start / time_per_tick)\n",
    "        duration = round(duration / time_per_tick)\n",
    "\n",
    "        measure = start // self.quantize_divisor\n",
    "        measuretick = start % self.quantize_divisor\n",
    "\n",
    "        # get the modulo of the measure so it doesn't exceed 32\n",
    "        measure = measure % 32\n",
    "        track = track % 32\n",
    "        duration = min(min(duration, 128), 0)\n",
    "        pitch = min(min(pitch, 127), 0)\n",
    "\n",
    "        # track: 0-31\n",
    "        # measure: 0-31\n",
    "        # measuretick: 0-63\n",
    "        # duration: 0-128\n",
    "        # pitch: 0-127\n",
    "        return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)\n",
    "\n",
    "song_dataset = SongDataSet(dataset, context_length=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with the loop\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlyUlEQVR4nO3df0xUd77/8dcsP+ZaFs4VcWaYK6XkrvVqoSaLvTCkrdYfKCmlrs3VXRKiWaPtVjF81bTa5qb05kasm9Vuwl2vt7epW2svzTe3dJvoUmlUeg3iDyKputa4KbYYGbFenAHLDpae7x/G890Rq4Lo8IHnI5nEOfOe4TOnp+GZMz9w2bZtCwAAwDA/ivUCAAAABoOIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGCk+Fgv4F75/vvvdf78eSUnJ8vlcsV6OQAA4A7Ytq2uri75/X796Ee3PtcyYiPm/PnzysjIiPUyAADAILS1tWnChAm3nBmxEZOcnCzp2k5ISUmJ8WoAAMCdCIfDysjIcH6P38qIjZjrLyGlpKQQMQAAGOZO3grCG3sBAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGCk+FgvAHfmoXW7BnW/sxufHuKVAAAwPHAmBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABG4iPW99lgPyoNAACicSYGAAAYiYgBAABGImIAAICRiBgAAGCkAUXM1q1b9eijjyolJUUpKSkKBAL64x//6Nxu27YqKyvl9/s1ZswYzZgxQydPnox6jEgkovLycqWlpSkpKUklJSU6d+5c1ExnZ6fKyspkWZYsy1JZWZkuX748+GcJAABGnAFFzIQJE7Rx40YdPXpUR48e1cyZM/Xss886obJp0yZt3rxZ1dXVOnLkiHw+n+bMmaOuri7nMSoqKlRbW6uamhodOHBA3d3dKi4uVl9fnzNTWlqqlpYW1dXVqa6uTi0tLSorKxuipwwAAEYCl23b9t08QGpqqn7961/rl7/8pfx+vyoqKvTyyy9LunbWxev16o033tDzzz+vUCik8ePHa8eOHVq0aJEk6fz588rIyNDu3bs1d+5cnTp1SlOmTFFTU5Py8vIkSU1NTQoEAvriiy80adKkO1pXOByWZVkKhUJKSUm5m6c4pO73R6z5K9YAAJMM5Pf3oN8T09fXp5qaGl25ckWBQECtra0KBoMqLCx0Ztxut6ZPn67GxkZJUnNzs65evRo14/f7lZ2d7cwcPHhQlmU5ASNJ+fn5sizLmQEAABjwl90dP35cgUBAf/nLX/TjH/9YtbW1mjJlihMYXq83at7r9eqrr76SJAWDQSUmJmrs2LH9ZoLBoDPj8Xj6/VyPx+PM3EwkElEkEnGuh8PhgT41AABgkAGfiZk0aZJaWlrU1NSkX/3qV1q8eLH+9Kc/Obe7XK6oedu2+2270Y0zN5u/3eNUVVU5bwS2LEsZGRl3+pQAAICBBhwxiYmJ+slPfqJp06apqqpKU6dO1W9/+1v5fD5J6ne2pKOjwzk74/P51Nvbq87OzlvOXLhwod/PvXjxYr+zPH9t/fr1CoVCzqWtrW2gTw0AABjkrr8nxrZtRSIRZWVlyefzqb6+3rmtt7dXDQ0NKigokCTl5uYqISEhaqa9vV0nTpxwZgKBgEKhkA4fPuzMHDp0SKFQyJm5Gbfb7Xz0+/oFAACMXAN6T8wrr7yioqIiZWRkqKurSzU1Ndq/f7/q6urkcrlUUVGhDRs2aOLEiZo4caI2bNigBx54QKWlpZIky7K0dOlSrVmzRuPGjVNqaqrWrl2rnJwczZ49W5I0efJkzZs3T8uWLdO2bdskScuXL1dxcfEdfzIJAACMfAOKmAsXLqisrEzt7e2yLEuPPvqo6urqNGfOHEnSSy+9pJ6eHr344ovq7OxUXl6e9uzZo+TkZOcxtmzZovj4eC1cuFA9PT2aNWuWtm/frri4OGdm586dWrVqlfMpppKSElVXVw/F8wUAACPEXX9PzHDF98Rcw/fEAABMcl++JwYAACCWiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkQYUMVVVVXrssceUnJwsj8ej+fPn6/Tp01EzS5Yskcvlirrk5+dHzUQiEZWXlystLU1JSUkqKSnRuXPnomY6OztVVlYmy7JkWZbKysp0+fLlwT1LAAAw4gwoYhoaGrRixQo1NTWpvr5e3333nQoLC3XlypWouXnz5qm9vd257N69O+r2iooK1dbWqqamRgcOHFB3d7eKi4vV19fnzJSWlqqlpUV1dXWqq6tTS0uLysrK7uKpAgCAkSR+IMN1dXVR19955x15PB41NzfrySefdLa73W75fL6bPkYoFNLbb7+tHTt2aPbs2ZKk9957TxkZGfr00081d+5cnTp1SnV1dWpqalJeXp4k6a233lIgENDp06c1adKkAT1JAAAw8tzVe2JCoZAkKTU1NWr7/v375fF49PDDD2vZsmXq6OhwbmtubtbVq1dVWFjobPP7/crOzlZjY6Mk6eDBg7IsywkYScrPz5dlWc4MAAAY3QZ0Juav2bat1atX6/HHH1d2drazvaioSP/0T/+kzMxMtba26p//+Z81c+ZMNTc3y+12KxgMKjExUWPHjo16PK/Xq2AwKEkKBoPyeDz9fqbH43FmbhSJRBSJRJzr4XB4sE8NAAAYYNARs3LlSn3++ec6cOBA1PZFixY5/87Ozta0adOUmZmpXbt2acGCBT/4eLZty+VyOdf/+t8/NPPXqqqq9Prrrw/0aQAAAEMN6uWk8vJyffzxx9q3b58mTJhwy9n09HRlZmbqzJkzkiSfz6fe3l51dnZGzXV0dMjr9TozFy5c6PdYFy9edGZutH79eoVCIefS1tY2mKcGAAAMMaCIsW1bK1eu1Icffqi9e/cqKyvrtve5dOmS2tralJ6eLknKzc1VQkKC6uvrnZn29nadOHFCBQUFkqRAIKBQKKTDhw87M4cOHVIoFHJmbuR2u5WSkhJ1AQAAI9eAXk5asWKF3n//ff3hD39QcnKy8/4Uy7I0ZswYdXd3q7KyUs8995zS09N19uxZvfLKK0pLS9PPfvYzZ3bp0qVas2aNxo0bp9TUVK1du1Y5OTnOp5UmT56sefPmadmyZdq2bZskafny5SouLuaTSQAAQNIAI2br1q2SpBkzZkRtf+edd7RkyRLFxcXp+PHjevfdd3X58mWlp6frqaee0gcffKDk5GRnfsuWLYqPj9fChQvV09OjWbNmafv27YqLi3Nmdu7cqVWrVjmfYiopKVF1dfVgnycAABhhXLZt27FexL0QDodlWZZCodCwemnpoXW77uvPO7vx6fv68wAAuBsD+f3N304CAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARhpQxFRVVemxxx5TcnKyPB6P5s+fr9OnT0fN2LatyspK+f1+jRkzRjNmzNDJkyejZiKRiMrLy5WWlqakpCSVlJTo3LlzUTOdnZ0qKyuTZVmyLEtlZWW6fPny4J4lAAAYcQYUMQ0NDVqxYoWamppUX1+v7777ToWFhbpy5Yozs2nTJm3evFnV1dU6cuSIfD6f5syZo66uLmemoqJCtbW1qqmp0YEDB9Td3a3i4mL19fU5M6WlpWppaVFdXZ3q6urU0tKisrKyIXjKAABgJHDZtm0P9s4XL16Ux+NRQ0ODnnzySdm2Lb/fr4qKCr388suSrp118Xq9euONN/T8888rFApp/Pjx2rFjhxYtWiRJOn/+vDIyMrR7927NnTtXp06d0pQpU9TU1KS8vDxJUlNTkwKBgL744gtNmjTptmsLh8OyLEuhUEgpKSmDfYpD7qF1u+7rzzu78en7+vMAALgbA/n9fVfviQmFQpKk1NRUSVJra6uCwaAKCwudGbfbrenTp6uxsVGS1NzcrKtXr0bN+P1+ZWdnOzMHDx6UZVlOwEhSfn6+LMtyZgAAwOgWP9g72rat1atX6/HHH1d2drYkKRgMSpK8Xm/UrNfr1VdffeXMJCYmauzYsf1mrt8/GAzK4/H0+5kej8eZuVEkElEkEnGuh8PhQT4zAABggkGfiVm5cqU+//xz/dd//Ve/21wuV9R127b7bbvRjTM3m7/V41RVVTlvArYsSxkZGXfyNAAAgKEGFTHl5eX6+OOPtW/fPk2YMMHZ7vP5JKnf2ZKOjg7n7IzP51Nvb686OztvOXPhwoV+P/fixYv9zvJct379eoVCIefS1tY2mKcGAAAMMaCIsW1bK1eu1Icffqi9e/cqKysr6vasrCz5fD7V19c723p7e9XQ0KCCggJJUm5urhISEqJm2tvbdeLECWcmEAgoFArp8OHDzsyhQ4cUCoWcmRu53W6lpKREXQAAwMg1oPfErFixQu+//77+8Ic/KDk52TnjYlmWxowZI5fLpYqKCm3YsEETJ07UxIkTtWHDBj3wwAMqLS11ZpcuXao1a9Zo3LhxSk1N1dq1a5WTk6PZs2dLkiZPnqx58+Zp2bJl2rZtmyRp+fLlKi4uvqNPJgEAgJFvQBGzdetWSdKMGTOitr/zzjtasmSJJOmll15ST0+PXnzxRXV2diovL0979uxRcnKyM79lyxbFx8dr4cKF6unp0axZs7R9+3bFxcU5Mzt37tSqVaucTzGVlJSourp6MM8RAACMQHf1PTHDGd8Tcw3fEwMAMMl9+54YAACAWCFiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkeJjvQDcWw+t2zWo+53d+PQQrwQAgKHFmRgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGGnAEfPZZ5/pmWeekd/vl8vl0kcffRR1+5IlS+RyuaIu+fn5UTORSETl5eVKS0tTUlKSSkpKdO7cuaiZzs5OlZWVybIsWZalsrIyXb58ecBPEAAAjEwDjpgrV65o6tSpqq6u/sGZefPmqb293bns3r076vaKigrV1taqpqZGBw4cUHd3t4qLi9XX1+fMlJaWqqWlRXV1daqrq1NLS4vKysoGulwAADBCDfjL7oqKilRUVHTLGbfbLZ/Pd9PbQqGQ3n77be3YsUOzZ8+WJL333nvKyMjQp59+qrlz5+rUqVOqq6tTU1OT8vLyJElvvfWWAoGATp8+rUmTJg102QAAYIS5J++J2b9/vzwejx5++GEtW7ZMHR0dzm3Nzc26evWqCgsLnW1+v1/Z2dlqbGyUJB08eFCWZTkBI0n5+fmyLMuZuVEkElE4HI66AACAkWvII6aoqEg7d+7U3r179Zvf/EZHjhzRzJkzFYlEJEnBYFCJiYkaO3Zs1P28Xq+CwaAz4/F4+j22x+NxZm5UVVXlvH/GsixlZGQM8TMDAADDyZD/7aRFixY5/87Ozta0adOUmZmpXbt2acGCBT94P9u25XK5nOt//e8fmvlr69ev1+rVq53r4XCYkAEAYAS75x+xTk9PV2Zmps6cOSNJ8vl86u3tVWdnZ9RcR0eHvF6vM3PhwoV+j3Xx4kVn5kZut1spKSlRFwAAMHLd84i5dOmS2tralJ6eLknKzc1VQkKC6uvrnZn29nadOHFCBQUFkqRAIKBQKKTDhw87M4cOHVIoFHJmAADA6Dbgl5O6u7v15z//2bne2tqqlpYWpaamKjU1VZWVlXruueeUnp6us2fP6pVXXlFaWpp+9rOfSZIsy9LSpUu1Zs0ajRs3TqmpqVq7dq1ycnKcTytNnjxZ8+bN07Jly7Rt2zZJ0vLly1VcXMwnkwAAgKRBRMzRo0f11FNPOdevvw9l8eLF2rp1q44fP653331Xly9fVnp6up566il98MEHSk5Odu6zZcsWxcfHa+HCherp6dGsWbO0fft2xcXFOTM7d+7UqlWrnE8xlZSU3PK7aQAAwOjism3bjvUi7oVwOCzLshQKhYbV+2MeWrcr1ku4I2c3Ph3rJQAARqGB/P7mbycBAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIw04Yj777DM988wz8vv9crlc+uijj6Jut21blZWV8vv9GjNmjGbMmKGTJ09GzUQiEZWXlystLU1JSUkqKSnRuXPnomY6OztVVlYmy7JkWZbKysp0+fLlAT9BAAAwMg04Yq5cuaKpU6equrr6prdv2rRJmzdvVnV1tY4cOSKfz6c5c+aoq6vLmamoqFBtba1qamp04MABdXd3q7i4WH19fc5MaWmpWlpaVFdXp7q6OrW0tKisrGwQTxEAAIxELtu27UHf2eVSbW2t5s+fL+naWRi/36+Kigq9/PLLkq6ddfF6vXrjjTf0/PPPKxQKafz48dqxY4cWLVokSTp//rwyMjK0e/duzZ07V6dOndKUKVPU1NSkvLw8SVJTU5MCgYC++OILTZo06bZrC4fDsixLoVBIKSkpg32KQ+6hdbtivYQ7cnbj07FeAgBgFBrI7+8hfU9Ma2urgsGgCgsLnW1ut1vTp09XY2OjJKm5uVlXr16NmvH7/crOznZmDh48KMuynICRpPz8fFmW5czcKBKJKBwOR10AAMDINaQREwwGJUlerzdqu9frdW4LBoNKTEzU2LFjbznj8Xj6Pb7H43FmblRVVeW8f8ayLGVkZNz18wEAAMPXPfl0ksvlirpu23a/bTe6ceZm87d6nPXr1ysUCjmXtra2QawcAACYYkgjxufzSVK/syUdHR3O2Rmfz6fe3l51dnbecubChQv9Hv/ixYv9zvJc53a7lZKSEnUBAAAj15BGTFZWlnw+n+rr651tvb29amhoUEFBgSQpNzdXCQkJUTPt7e06ceKEMxMIBBQKhXT48GFn5tChQwqFQs4MAAAY3eIHeofu7m79+c9/dq63traqpaVFqampevDBB1VRUaENGzZo4sSJmjhxojZs2KAHHnhApaWlkiTLsrR06VKtWbNG48aNU2pqqtauXaucnBzNnj1bkjR58mTNmzdPy5Yt07Zt2yRJy5cvV3Fx8R19MgkAAIx8A46Yo0eP6qmnnnKur169WpK0ePFibd++XS+99JJ6enr04osvqrOzU3l5edqzZ4+Sk5Od+2zZskXx8fFauHChenp6NGvWLG3fvl1xcXHOzM6dO7Vq1SrnU0wlJSU/+N00AABg9Lmr74kZzviemLvD98QAAGIhZt8TAwAAcL8QMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADBSfKwXgOHpoXW7BnW/sxufHuKVAABwc5yJAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYa8oiprKyUy+WKuvh8Pud227ZVWVkpv9+vMWPGaMaMGTp58mTUY0QiEZWXlystLU1JSUkqKSnRuXPnhnqpAADAYPfkTMwjjzyi9vZ253L8+HHntk2bNmnz5s2qrq7WkSNH5PP5NGfOHHV1dTkzFRUVqq2tVU1NjQ4cOKDu7m4VFxerr6/vXiwXAAAYKP6ePGh8fNTZl+ts29abb76pV199VQsWLJAk/f73v5fX69X777+v559/XqFQSG+//bZ27Nih2bNnS5Lee+89ZWRk6NNPP9XcuXPvxZIBAIBh7smZmDNnzsjv9ysrK0s///nP9eWXX0qSWltbFQwGVVhY6My63W5Nnz5djY2NkqTm5mZdvXo1asbv9ys7O9uZuZlIJKJwOBx1AQAAI9eQR0xeXp7effddffLJJ3rrrbcUDAZVUFCgS5cuKRgMSpK8Xm/Ufbxer3NbMBhUYmKixo4d+4MzN1NVVSXLspxLRkbGED8zAAAwnAx5xBQVFem5555TTk6OZs+erV27dkm69rLRdS6XK+o+tm3323aj282sX79eoVDIubS1td3FswAAAMPdPf+IdVJSknJycnTmzBnnfTI3nlHp6Ohwzs74fD719vaqs7PzB2duxu12KyUlJeoCAABGrnseMZFIRKdOnVJ6erqysrLk8/lUX1/v3N7b26uGhgYVFBRIknJzc5WQkBA1097erhMnTjgzAAAAQ/7ppLVr1+qZZ57Rgw8+qI6ODv3rv/6rwuGwFi9eLJfLpYqKCm3YsEETJ07UxIkTtWHDBj3wwAMqLS2VJFmWpaVLl2rNmjUaN26cUlNTtXbtWuflKQAAAOkeRMy5c+f0i1/8Qt98843Gjx+v/Px8NTU1KTMzU5L00ksvqaenRy+++KI6OzuVl5enPXv2KDk52XmMLVu2KD4+XgsXLlRPT49mzZql7du3Ky4ubqiXCwAADOWybduO9SLuhXA4LMuyFAqFhtX7Yx5atyvWS7inzm58OtZLAAAYbCC/v/nbSQAAwEhEDAAAMNI9+bMDo8FIf1kIAIDhjjMxAADASEQMAAAwEi8nYUgN9mU2PtUEABgozsQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAI8XHegGAJD20bteg7nd249NDvBIAgCk4EwMAAIxExAAAACMRMQAAwEhEDAAAMBJv7IXReEMwAIxenIkBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJH4iDVGpcF+NFvi49kAMFxwJgYAABiJiAEAAEYiYgAAgJGIGAAAYCTe2AsMEH+vCQCGh2EfMb/73e/061//Wu3t7XrkkUf05ptv6oknnoj1soABI34AYGgN65eTPvjgA1VUVOjVV1/VsWPH9MQTT6ioqEhff/11rJcGAABizGXbth3rRfyQvLw8/fSnP9XWrVudbZMnT9b8+fNVVVV1y/uGw2FZlqVQKKSUlJQhX9vdfM8IMJxx5gdALA3k9/ewfTmpt7dXzc3NWrduXdT2wsJCNTY29puPRCKKRCLO9VAoJOnazrgXvo98e08eF4i1B//P/x3U/U68PneIV3Jr2a99Mqj73e91AiYYTv8/Xf+9fSfnWIZtxHzzzTfq6+uT1+uN2u71ehUMBvvNV1VV6fXXX++3PSMj456tEcD/Z70Z6xXcGVPWCZjgXv7/1NXVJcuybjkzbCPmOpfLFXXdtu1+2yRp/fr1Wr16tXP9+++/1//+7/9q3LhxN52/G+FwWBkZGWpra7snL1WZjv1ze+yj22Mf3R776NbYP7c3HPeRbdvq6uqS3++/7eywjZi0tDTFxcX1O+vS0dHR7+yMJLndbrnd7qhtf/u3f3svl6iUlJRh8x99OGL/3B776PbYR7fHPro19s/tDbd9dLszMNcN208nJSYmKjc3V/X19VHb6+vrVVBQEKNVAQCA4WLYnomRpNWrV6usrEzTpk1TIBDQf/zHf+jrr7/WCy+8EOulAQCAGBvWEbNo0SJdunRJ//Iv/6L29nZlZ2dr9+7dyszMjOm63G63XnvttX4vX+Ea9s/tsY9uj310e+yjW2P/3J7p+2hYf08MAADADxm274kBAAC4FSIGAAAYiYgBAABGImIAAICRiJgB+t3vfqesrCz9zd/8jXJzc/U///M/sV7SsFFZWSmXyxV18fl8sV5WTH322Wd65pln5Pf75XK59NFHH0Xdbtu2Kisr5ff7NWbMGM2YMUMnT56MzWJj5Hb7aMmSJf2Oq/z8/NgsNgaqqqr02GOPKTk5WR6PR/Pnz9fp06ejZkb7cXQn+2g0H0dbt27Vo48+6nyhXSAQ0B//+EfndpOPHyJmAD744ANVVFTo1Vdf1bFjx/TEE0+oqKhIX3/9dayXNmw88sgjam9vdy7Hjx+P9ZJi6sqVK5o6daqqq6tvevumTZu0efNmVVdX68iRI/L5fJozZ466urru80pj53b7SJLmzZsXdVzt3r37Pq4wthoaGrRixQo1NTWpvr5e3333nQoLC3XlyhVnZrQfR3eyj6TRexxNmDBBGzdu1NGjR3X06FHNnDlTzz77rBMqRh8/Nu7YP/7jP9ovvPBC1LZ/+Id/sNetWxejFQ0vr732mj116tRYL2PYkmTX1tY617///nvb5/PZGzdudLb95S9/sS3Lsv/93/89BiuMvRv3kW3b9uLFi+1nn302JusZjjo6OmxJdkNDg23bHEc3c+M+sm2OoxuNHTvW/s///E/jjx/OxNyh3t5eNTc3q7CwMGp7YWGhGhsbY7Sq4efMmTPy+/3KysrSz3/+c3355ZexXtKw1draqmAwGHVMud1uTZ8+nWPqBvv375fH49HDDz+sZcuWqaOjI9ZLiplQKCRJSk1NlcRxdDM37qPrOI6kvr4+1dTU6MqVKwoEAsYfP0TMHfrmm2/U19fX749Per3efn+kcrTKy8vTu+++q08++URvvfWWgsGgCgoKdOnSpVgvbVi6ftxwTN1aUVGRdu7cqb179+o3v/mNjhw5opkzZyoSicR6afedbdtavXq1Hn/8cWVnZ0viOLrRzfaRxHF0/Phx/fjHP5bb7dYLL7yg2tpaTZkyxfjjZ1j/2YHhyOVyRV23bbvfttGqqKjI+XdOTo4CgYD+/u//Xr///e+1evXqGK5seOOYurVFixY5/87Ozta0adOUmZmpXbt2acGCBTFc2f23cuVKff755zpw4EC/2ziOrvmhfTTaj6NJkyappaVFly9f1n//939r8eLFamhocG439fjhTMwdSktLU1xcXL8y7ejo6FewuCYpKUk5OTk6c+ZMrJcyLF3/5BbH1MCkp6crMzNz1B1X5eXl+vjjj7Vv3z5NmDDB2c5x9P/90D66mdF2HCUmJuonP/mJpk2bpqqqKk2dOlW//e1vjT9+iJg7lJiYqNzcXNXX10dtr6+vV0FBQYxWNbxFIhGdOnVK6enpsV7KsJSVlSWfzxd1TPX29qqhoYFj6hYuXbqktra2UXNc2batlStX6sMPP9TevXuVlZUVdTvH0e330c2MtuPoRrZtKxKJmH/8xOwtxQaqqamxExIS7Lffftv+05/+ZFdUVNhJSUn22bNnY720YWHNmjX2/v377S+//NJuamqyi4uL7eTk5FG9f7q6uuxjx47Zx44dsyXZmzdvto8dO2Z/9dVXtm3b9saNG23LsuwPP/zQPn78uP2LX/zCTk9Pt8PhcIxXfv/cah91dXXZa9assRsbG+3W1lZ73759diAQsP/u7/5u1OyjX/3qV7ZlWfb+/fvt9vZ25/Ltt986M6P9OLrdPhrtx9H69evtzz77zG5tbbU///xz+5VXXrF/9KMf2Xv27LFt2+zjh4gZoH/7t3+zMzMz7cTERPunP/1p1Ef4RrtFixbZ6enpdkJCgu33++0FCxbYJ0+ejPWyYmrfvn22pH6XxYsX27Z97eOxr732mu3z+Wy3220/+eST9vHjx2O76PvsVvvo22+/tQsLC+3x48fbCQkJ9oMPPmgvXrzY/vrrr2O97PvmZvtGkv3OO+84M6P9OLrdPhrtx9Evf/lL5/fW+PHj7VmzZjkBY9tmHz8u27bt+3feBwAAYGjwnhgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICR/h8toAPAGgGSkwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maximum_measure_indexes = []\n",
    "\n",
    "for i in range(10000):\n",
    "    songsection = song_dataset[i]['notes']\n",
    "    maximum_measure_indexes.append(songsection[:, 1].max().item())\n",
    "\n",
    "print(\"done with the loop\")\n",
    "# for song in song_dataset.songs:\n",
    "#     maximum_time_indexes.append(song[:, 1].max())\n",
    "\n",
    "# plot the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(maximum_measure_indexes, bins=32)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Format Analysis and Music Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Assuming song_dataset is an instance of SongDataSet and song_tensor is a tensor from the dataset\u001b[39;00m\n\u001b[1;32m     43\u001b[0m song_tensor \u001b[38;5;241m=\u001b[39m song_dataset[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Get the first song tensor\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mplot_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43msong_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Plot the first track\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 12\u001b[0m, in \u001b[0;36mplot_track\u001b[0;34m(song_tensor, track_number)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mPlots the notes of a specific track from a song tensor.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m- track_number: int, the track number to plot.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Filter notes for the specified track\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m track_notes \u001b[38;5;241m=\u001b[39m song_tensor[\u001b[43msong_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m track_number]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m track_notes\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo notes found for track \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrack_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_track(song_tensor, track_number):\n",
    "    \"\"\"\n",
    "    Plots the notes of a specific track from a song tensor.\n",
    "\n",
    "    Paramet        # track: 0-31\n",
    "        # measure: 0-31\n",
    "        # measuretick: 0-63\n",
    "        # duration: 0-128\n",
    "        # pitch: 0-127\n",
    "        return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)ers:\n",
    "    - song_tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    - track_number: int, the track number to plot.\n",
    "    \"\"\"\n",
    "    # Filter notes for the specified track\n",
    "    track_notes = song_tensor[song_tensor[:, 0] == track_number]\n",
    "\n",
    "    if track_notes.size(0) == 0:\n",
    "        print(f\"No notes found for track {track_number}\")\n",
    "        return\n",
    "\n",
    "    # Extract start times, durations, and pitches\n",
    "    start_times = track_notes[:, 1].numpy()\n",
    "    durations = track_notes[:, 2].numpy()\n",
    "    pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot each note as a horizontal bar\n",
    "    for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "        ax.broken_barh([(start, duration)], (pitch - 0.4, 0.8), facecolors='blue')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Time (quantized ticks)')\n",
    "    ax.set_ylabel('Pitch')\n",
    "    ax.set_title(f'Track {track_number} Notes')\n",
    "\n",
    "    # Show grid\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming song_dataset is an instance of SongDataSet and song_tensor is a tensor from the dataset\n",
    "song_tensor = song_dataset[1]  # Get the first song tensor\n",
    "plot_track(song_tensor, track_number=0)  # Plot the first track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_tracks(song_tensor):\n",
    "    \"\"\"\n",
    "    Plots the notes of all tracks from a song tensor, each in its own subplot.\n",
    "\n",
    "    Parameters:\n",
    "    - song_tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    \"\"\"\n",
    "    # Get unique track numbers\n",
    "    track_numbers = song_tensor[:, 0].unique().numpy()\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(len(track_numbers), 1, figsize=(10, 5 * len(track_numbers)), sharex=True)\n",
    "\n",
    "    if len(track_numbers) == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable if there's only one track\n",
    "\n",
    "    # Plot each track\n",
    "    for ax, track_number in zip(axes, track_numbers):\n",
    "        # Filter notes for the current track\n",
    "        track_notes = song_tensor[song_tensor[:, 0] == track_number]\n",
    "\n",
    "        if track_notes.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract start times, durations, and pitches\n",
    "        start_times = track_notes[:, 1].numpy()\n",
    "        durations = track_notes[:, 2].numpy()\n",
    "        pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "        # Plot each note as a horizontal bar\n",
    "        for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "            ax.broken_barh([(start, duration)], (pitch - 0.4, 0.8), facecolors='blue')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_ylabel('Pitch')\n",
    "        ax.set_title(f'Track {track_number} Notes')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Set common x-label\n",
    "    axes[-1].set_xlabel('Time (quantized ticks)')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming song_dataset is an instance of SongDataSet and song_tensor is a tensor from the dataset\n",
    "song_tensor = song_dataset[1]  # Get the first song tensor\n",
    "plot_all_tracks(song_tensor)  # Plot all tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import torch\n",
    "\n",
    "def tensor_to_midi(tensor, output_filename, time_per_tick):\n",
    "    \"\"\"\n",
    "    Converts a tensor representation of a song back to a MIDI file.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    - output_filename: str, the filename for the output MIDI file.\n",
    "    \"\"\"\n",
    "    # Create a PrettyMIDI object\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "\n",
    "    # Get unique track numbers\n",
    "    track_numbers = tensor[:, 0].unique().numpy()\n",
    "\n",
    "    for track_number in track_numbers:\n",
    "        # Create an Instrument instance for each track\n",
    "        instrument = pretty_midi.Instrument(program=0)  # Default to Acoustic Grand Piano\n",
    "\n",
    "        # Filter notes for the current track\n",
    "        track_notes = tensor[tensor[:, 0] == track_number]\n",
    "\n",
    "        # Extract start times, durations, and pitches\n",
    "        start_times = track_notes[:, 1].numpy()\n",
    "        durations = track_notes[:, 2].numpy()\n",
    "        pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "        # Create Note objects and add them to the instrument\n",
    "        for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=100,  # Default velocity\n",
    "                pitch=int(pitch),\n",
    "                start=(start*time_per_tick).item(),\n",
    "                end=((start + duration)*time_per_tick).item()\n",
    "            )\n",
    "            instrument.notes.append(note)\n",
    "\n",
    "        # Add the instrument to the PrettyMIDI object\n",
    "        midi.instruments.append(instrument)\n",
    "\n",
    "    # Write out the MIDI data\n",
    "    midi.write(output_filename)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "song_example = song_dataset[1]  # Get the first song tensor\n",
    "notes = song_example[\"notes\"]\n",
    "perturbed_notes = song_example[\"perturbed_notes\"]\n",
    "time_per_tick = song_example[\"time_per_tick\"]\n",
    "\n",
    "tensor_to_midi(perturbed_notes, 'output_song.mid', time_per_tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "def play_midi_file(midi_filename):\n",
    "    \"\"\"\n",
    "    Plays a MIDI file using pygame.\n",
    "\n",
    "    Parameters:\n",
    "    - midi_filename: str, the filename of the MIDI file to play.\n",
    "    \"\"\"\n",
    "    # Initialize pygame\n",
    "    pygame.init()\n",
    "\n",
    "    # Set up the mixer to play MIDI\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(midi_filename)\n",
    "\n",
    "    # Play the MIDI file\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Keep the program running until the music stops\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Example usage\n",
    "midi_filename = 'output_song.mid'\n",
    "play_midi_file(midi_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 300,\n",
    "    \"max_iters\": 3000, \n",
    "    \"H\": 16,\n",
    "    \"B\": 64,\n",
    "    \"T\": 128,\n",
    "    \"C\": 128,\n",
    "    \"pitches\": 128,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 6,\n",
    "    \"dropout\": 0.4,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 6,\n",
    "    \"tokenizer_vocab_size\": 4096,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H, cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.cross_attention = cross_attention\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "\n",
    "        if self.cross_attention == True:\n",
    "            key_vectors = self.key(cross_attention_sequence)\n",
    "            assert key_vectors.shape[-2] == T, \"cross_attention_sequence must be the same length as the input sequence\"\n",
    "        else:\n",
    "            key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        if self.cross_attention == False:\n",
    "            tril = self.tril\n",
    "            wei = torch.zeros(T, T) \n",
    "            wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "\n",
    "        if self.cross_attention == False: # we are doing self-attention, causal masking\n",
    "            attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        else:\n",
    "            # we are doing cross attention, so we don't need to mask the attention weights\n",
    "            attention_weights = F.softmax(attention_pattern, dim=-1)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "# x = torch.randn(B,T,C)\n",
    "# x = torch.randn(T,C)\n",
    "# head_self_attention = Head(H, cross_attention=False)\n",
    "\n",
    "# print(head_cross_attention(x, x))\n",
    "# print(\"=\"*40)\n",
    "# print(head_self_attention(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000,  0.3373,  0.0875,  ..., -0.0000,  0.1044, -0.0394],\n",
      "        [-0.2252,  0.2746,  0.0063,  ..., -0.0468,  0.1242, -0.0421],\n",
      "        [-0.1656,  0.2630,  0.1539,  ..., -0.0227,  0.1937, -0.0510],\n",
      "        ...,\n",
      "        [-0.0000,  0.1527,  0.1089,  ..., -0.1539,  0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0768,  ..., -0.0000,  0.0000,  0.0326],\n",
      "        [-0.0000,  0.0000,  0.1228,  ..., -0.0224,  0.0000, -0.0622]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention or cross-attention'''\n",
    "    def __init__(self, H, C, n_heads, cross_attention=False): # H is head embedding space size, n_heads is number of heads, cross_attention flag\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H, cross_attention=cross_attention) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cross_attention = cross_attention\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        if self.cross_attention and cross_attention_sequence is not None:\n",
    "            x = torch.cat([head(x, cross_attention_sequence) for head in self.heads], dim=-1)\n",
    "        else:\n",
    "            x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "multi_head_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "print(multi_head_attention(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            # nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n",
      "128 128\n"
     ]
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    '''Transformer encoder block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attention(x, x))\n",
    "        x = self.norm2(x + self.feedforward(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.randn(T,C)\n",
    "\n",
    "enc = EncoderBlock(H, C, n_heads)\n",
    "\n",
    "print(enc(x).shape)\n",
    "print(T,C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    '''Transformer decoder block'''\n",
    "    def __init__(self, H, C, n_heads, feedforward_factor):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.cross_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "        self.norm3 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence):\n",
    "        x = x + self.self_attention(self.norm1(x))\n",
    "        x = x + self.cross_attention(self.norm2(x), cross_attention_sequence)\n",
    "        x = x + self.feedforward(self.norm3(x))\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "cross_attention_sequence = torch.randn(T,C)\n",
    "\n",
    "dec = DecoderBlock(H, C, n_heads, feedforward_factor)\n",
    "dec(x, cross_attention_sequence).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 230\u001b[0m\n\u001b[1;32m    228\u001b[0m model \u001b[38;5;241m=\u001b[39m TimingTransformer(n_layers)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# logits, loss = model(xb, yb)\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack dist shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrack_dist\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasure dist shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeasure_dist\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 145\u001b[0m, in \u001b[0;36mTimingTransformer.forward\u001b[0;34m(self, idx_dirty, targets)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx_dirty, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# ---- old GPT forward for reference ----\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# idx shape: Batch, Time, [track, measure, measuretick, duration, pitch]\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# idx shape: Batch, Time, Channel\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     b, t, c \u001b[38;5;241m=\u001b[39m idx_dirty\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 145\u001b[0m     encoder_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_dirty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     decoder_sequence \u001b[38;5;241m=\u001b[39m encoder_sequence\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoderlayer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoderlayers:\n",
      "Cell \u001b[0;32mIn[23], line 76\u001b[0m, in \u001b[0;36mTimingTransformer.encode\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pitch_emb\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (B, T, \u001b[38;5;241m32\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpitch_emb shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpitch_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pos_emb\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (T, \u001b[38;5;241m8\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_emb shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasuretick_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpitch_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_emb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "class TimingTransformer(nn.Module):\n",
    "\n",
    "    # track: 0-31\n",
    "    # measure: 0-31\n",
    "    # measuretick: 0-63\n",
    "    # duration: 0-128\n",
    "    # pitch: 0-127\n",
    "    # return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        # embedding dimensions (go into model)\n",
    "        a,b,c,d,e,f = 8, 32, 32, 16, 32, 8\n",
    "        assert a + b + c + d + e + f == C, f\"embedding dimensions must sum to C:{C}\"\n",
    "        self.track_embedding_table = nn.Embedding(32, a) # one-hot vector of length 32 (because there are 32 possible tracks) -> a\n",
    "        self.measure_embedding_table = nn.Embedding(32, b) # 32 possible measures, stored in b dimensions 32 -> b\n",
    "        self.measuretick_embedding_table = nn.Embedding(64, c)\n",
    "        self.duration_embedding_table = nn.Embedding(128, d)\n",
    "        self.pitch_embedding_table = nn.Embedding(128, e)\n",
    "        self.position_embedding_table = nn.Embedding(T, f) \n",
    "\n",
    "\n",
    "        # model\n",
    "        self.encoderlayers = nn.ModuleList([EncoderBlock(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.decoderlayers = nn.ModuleList([DecoderBlock(H, C, n_heads, feedforward_factor) for _ in range(n_layers)])\n",
    "\n",
    "        # de-embedding dimensions (go out of model)\n",
    "        # Output scheme: a:b track, b:c measure, c:d measuretick, d:e duration, e:f pitch, f:g position\n",
    "        # it doesn't really matter because we just try and minimize log-likelyhood of the target for each token\n",
    "\n",
    "        # first 8 dimensions are track, so we want to predict the next track, which is a distribution over the 32 tracks, so output is 32\n",
    "        # 8 -> 32\n",
    "        self.track_head = nn.Linear(a, 32)\n",
    "        # next 32 dimensions are measure, so we want to predict the next measure, which is a distribution over the 32 measures, so output is 32\n",
    "        # 32 -> 32\n",
    "        self.measure_head = nn.Linear(b, 32)\n",
    "        self.measuretick_head = nn.Linear(c, 64)\n",
    "        self.duration_head = nn.Linear(d, 128)\n",
    "        self.pitch_head = nn.Linear(e, 128)\n",
    "        self.position_head = nn.Linear(f, T) # last f dimensions are position, so we want to predict the next position, which is a distribution over the T positions, so output is T\n",
    "\n",
    "\n",
    "\n",
    "        # LEGACY GPT REFERENCE CODE\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, C) # REMOVE\n",
    "        # self.position_embedding_table = nn.Embedding(T, C) # REMOVE\n",
    "\n",
    "        # self.lm_head = nn.Linear(C, vocab_size) # REMOVE\n",
    "        # self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        # self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def encode(self, idx):\n",
    "        B, T, C = idx.shape # idx is a list of the indices of the tokens, without batch it would be:\n",
    "        # [track, measure, measuretick, duration, pitch], [track, measure, measuretick, duration, pitch], ...\n",
    "        # Channel for a given token is whether we are looking at track, measure, measuretick, duration, or pitch\n",
    "\n",
    "        tracks = idx[:, :, 0]\n",
    "        measures = idx[:, :, 1]\n",
    "        measureticks = idx[:, :, 2]\n",
    "        durations = idx[:, :, 3]\n",
    "        pitches = idx[:, :, 4]\n",
    "\n",
    "        track_emb = self.track_embedding_table(tracks)\n",
    "        measure_emb = self.measure_embedding_table(measures)\n",
    "        measuretick_emb = self.measuretick_embedding_table(measureticks)\n",
    "        duration_emb = self.duration_embedding_table(durations)\n",
    "        pitch_emb = self.pitch_embedding_table(pitches)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "\n",
    "        assert track_emb.shape == (B, T, 8), f\"track_emb shape is {track_emb.shape}\"\n",
    "        assert measure_emb.shape == (B, T, 32), f\"measure_emb shape is {measure_emb.shape}\"\n",
    "        assert measuretick_emb.shape == (B, T, 32), f\"measuretick_emb shape is {measuretick_emb.shape}\"\n",
    "        assert duration_emb.shape == (B, T, 16), f\"duration_emb shape is {duration_emb.shape}\"\n",
    "        assert pitch_emb.shape == (B, T, 32), f\"pitch_emb shape is {pitch_emb.shape}\"\n",
    "        assert pos_emb.shape == (T, 8), f\"pos_emb shape is {pos_emb.shape}\"\n",
    "\n",
    "        return torch.cat((track_emb, measure_emb, measuretick_emb, duration_emb, pitch_emb, pos_emb), dim=-1)\n",
    "\n",
    "    def decode(self, latent_dimension):\n",
    "        a_start, a_end = 0, a\n",
    "        b_start, b_end = a_end, a_end + b\n",
    "        c_start, c_end = b_end, b_end + c\n",
    "        d_start, d_end = c_end, c_end + d\n",
    "        e_start, e_end = d_end, d_end + e\n",
    "        f_start, f_end = e_end, e_end + f\n",
    "\n",
    "        track_emb = latent_dimension[:,:,a_start:a_end]\n",
    "        measure_emb = latent_dimension[:,:,b_start:b_end]\n",
    "        measuretick_emb = latent_dimension[:,:,c_start:c_end]\n",
    "        duration_emb = latent_dimension[:,:,d_start:d_end]\n",
    "        pitch_emb = latent_dimension[:,:,e_start:e_end]\n",
    "\n",
    "        # input representation: [track, measure, measuretick, duration, pitch, pos] (all idx's)\n",
    "\n",
    "        track_dist = self.track_head(track_emb)\n",
    "        measure_dist = self.measure_head(measure_emb)\n",
    "        measuretick_dist = self.measuretick_head(measuretick_emb)\n",
    "        duration_dist = self.duration_head(duration_emb)\n",
    "        pitch_dist = self.pitch_head(pitch_emb)\n",
    "\n",
    "        return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, idx_dirty, targets=None):\n",
    "        # ---- old GPT forward for reference ----\n",
    "\n",
    "        # B, T = idx.shape # idx is the indices of the tokens\n",
    "        # token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        # pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        # x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     x = layer(x)\n",
    "\n",
    "        # logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size. basically next-token output distribution\n",
    "\n",
    "        # batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        # if targets is None:\n",
    "        #     return logits, None\n",
    "        # else:\n",
    "        #     # a list of all the predictions, reguardles of batch.\n",
    "        #     # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "        #     # ydim: all predictions for all batches and sequences flattened (batch_dim*sequence_dim)\n",
    "        #     logits_loss_view = logits.view(-1, vocab_size) \n",
    "        #     # targets loss view\n",
    "        #     # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "        #     # so this would be like, [1,4,5,1,2,3, ...]\n",
    "        #     # where each number is the correct next index of the one hot vector\n",
    "        #     targets_loss_view = targets.view(-1)\n",
    "        #     loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "        #     return logits, loss\n",
    "        # ---- end old GPT forward for reference ----\n",
    "        \n",
    "\n",
    "\n",
    "        # idx shape: Batch, Time, [track, measure, measuretick, duration, pitch]\n",
    "        # idx shape: Batch, Time, Channel\n",
    "        b, t, c = idx_dirty.shape\n",
    "\n",
    "        encoder_sequence = self.encode(idx_dirty)\n",
    "        decoder_sequence = encoder_sequence.clone()\n",
    "\n",
    "\n",
    "        for encoderlayer in self.encoderlayers:\n",
    "            encoder_sequence = encoderlayer(encoder_sequence)\n",
    "\n",
    "        for decoderlayer in self.decoderlayers:\n",
    "            decoder_sequence = decoderlayer(decoder_sequence, encoder_sequence)\n",
    "\n",
    "        # decode the output\n",
    "        track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist = self.decode(decoder_sequence)\n",
    "\n",
    "        if targets is None:\n",
    "            return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, None\n",
    "        else:\n",
    "            track_targets = targets[:, :, 0]\n",
    "            measure_targets = targets[:, :, 1]\n",
    "            measuretick_targets = targets[:, :, 2]\n",
    "            duration_targets = targets[:, :, 3]\n",
    "            pitch_targets = targets[:, :, 4]\n",
    "\n",
    "            track_loss = F.cross_entropy(track_dist.view(b*t, 32), track_targets.view(b*t))\n",
    "            measure_loss = F.cross_entropy(measure_dist.view(b*t, 32), measure_targets.view(b*t))\n",
    "            measuretick_loss = F.cross_entropy(measuretick_dist.view(b*t, 64), measuretick_targets.view(b*t))\n",
    "            duration_loss = F.cross_entropy(duration_dist.view(b*t, 128), duration_targets.view(b*t))\n",
    "            pitch_loss = F.cross_entropy(pitch_dist.view(b*t, 128), pitch_targets.view(b*t))\n",
    "\n",
    "            total_loss = track_loss + measure_loss + measuretick_loss + duration_loss + pitch_loss\n",
    "\n",
    "\n",
    "\n",
    "            return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, _ = self(idx)\n",
    "            # get the predictions of the last token\n",
    "            last_note_track_logits = track_dist[:, -1, :] / temperature\n",
    "            last_note_measure_logits = measure_dist[:, -1, :] / temperature\n",
    "            last_note_measuretick_logits = measuretick_dist[:, -1, :] / temperature\n",
    "            last_note_duration_logits = duration_dist[:, -1, :] / temperature\n",
    "            last_note_pitch_logits = pitch_dist[:, -1, :] / temperature\n",
    "\n",
    "            # softmax to get probabilities\n",
    "            track_probabilities = F.softmax(last_note_track_logits, dim=-1)\n",
    "            measure_probabilities = F.softmax(last_note_measure_logits, dim=-1)\n",
    "            measuretick_probabilities = F.softmax(last_note_measuretick_logits, dim=-1)\n",
    "            duration_probabilities = F.softmax(last_note_duration_logits, dim=-1)\n",
    "            pitch_probabilities = F.softmax(last_note_pitch_logits, dim=-1)\n",
    "\n",
    "            # sample from the probabilities\n",
    "            next_track = torch.multinomial(track_probabilities, num_samples=1)\n",
    "            next_measure = torch.multinomial(measure_probabilities, num_samples=1)\n",
    "            next_measuretick = torch.multinomial(measuretick_probabilities, num_samples=1)\n",
    "            next_duration = torch.multinomial(duration_probabilities, num_samples=1)\n",
    "            next_pitch = torch.multinomial(pitch_probabilities, num_samples=1)\n",
    "\n",
    "            # Create the concatenated tensor\n",
    "            next_note = torch.stack([next_track, next_measure, next_measuretick, next_duration, next_pitch], dim=-1)\n",
    "            # data: a batch of sequences of notes, each note is a tensor of [track, measure, measuretick, duration, pitch]\n",
    "            idx = torch.cat((idx, next_note), dim=1)\n",
    "        return idx\n",
    "        # for _ in range(max_new_tokens):\n",
    "        #     logits, loss = self(idx[:,-T:])\n",
    "        #     # get the predictions of the last token\n",
    "        #     last_token_logits = logits[:, -1, :] / temperature # all batches, last token, all probabilities\n",
    "        #     # softmax to get probabilities\n",
    "        #     probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "        #     # sample from the probabilities\n",
    "        #     next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        #     # add the new token to the idx tensor\n",
    "        #     idx = torch.cat((idx, next_token), dim=1)\n",
    "        # return idx\n",
    "    \n",
    "\n",
    "\n",
    "xb = torch.randint(0, 8, (B, T, 5))\n",
    "yb = torch.randint(0, 8, (B, T, 5))\n",
    "\n",
    "model = TimingTransformer(n_layers)\n",
    "# logits, loss = model(xb, yb)\n",
    "track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, loss = model.forward(xb, targets=yb)\n",
    "print(f\"track dist shape: {track_dist.shape}\")\n",
    "print(f\"measure dist shape: {measure_dist.shape}\")\n",
    "print(f\"measuretick dist shape: {measuretick_dist.shape}\")\n",
    "print(f\"duration dist shape: {duration_dist.shape}\")\n",
    "print(f\"pitch dist shape: {pitch_dist.shape}\")\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_idx = torch.zeros(1, T).long()\n",
    "# model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  6.0000],\n",
       "         [ 2.0000,  7.0000],\n",
       "         [ 3.0000,  8.0000],\n",
       "         [ 4.0000,  9.0000],\n",
       "         [ 5.0000, 10.0000]],\n",
       "\n",
       "        [[ 1.1000,  6.6000],\n",
       "         [ 2.2000,  7.7000],\n",
       "         [ 3.3000,  8.8000],\n",
       "         [ 4.4000,  9.9000],\n",
       "         [ 5.5000, 10.1000]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "track= torch.tensor([[1,2,3,4,5], [1.1, 2.2, 3.3, 4.4, 5.5]])\n",
    "note = torch.tensor([[6,7,8,9,10], [6.6, 7.7, 8.8, 9.9, 10.10]])\n",
    "torch.stack([track,note], dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
