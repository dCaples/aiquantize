{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random MIDI file: ./lmd_full/6/60f4f7f37aa4dae34d541673cfc956ff.mid\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_random_midi_file(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    \n",
    "    return random.choice(midi_files)\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "random_midi_file = get_random_midi_file(root_directory)\n",
    "print(f\"Random MIDI file: {random_midi_file}\")\n",
    "\n",
    "filename = random_midi_file\n",
    "\n",
    "def get_random_pretty_midi_file():\n",
    "    filename = get_random_midi_file(root_directory)\n",
    "    return pretty_midi.PrettyMIDI(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegocaples/miniconda3/envs/ai/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file: data byte must be in range 0..127\n",
      "Error reading file: data byte must be in range 0..127\n",
      "Total memory used: 5545296 bytes\n",
      "Total memory used: 5.2884063720703125 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def process_midi_data(midi_data):\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32)\n",
    "\n",
    "song_lengths = []\n",
    "songs = []\n",
    "for i in range(100):\n",
    "    try:\n",
    "        midi_data = get_random_pretty_midi_file()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        continue\n",
    "\n",
    "    song = process_midi_data(midi_data)\n",
    "    songs.append(song)\n",
    "\n",
    "def get_memory_used(tensor):\n",
    "    return tensor.element_size() * tensor.nelement()\n",
    "\n",
    "total_memory = 0\n",
    "for song in songs:\n",
    "    total_memory += get_memory_used(song)\n",
    "\n",
    "print(f\"Total memory used: {total_memory} bytes\")\n",
    "# in MB\n",
    "print(f\"Total memory used: {total_memory / 1024 / 1024} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPQN: 96\n",
      "Time per tick: 0.005208333333333333\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "\n",
    "def process_midi(filename, track_index=0):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "\n",
    "    # Print the TPQN (Ticks Per Quarter Note)\n",
    "    print(\"TPQN:\", midi_data.resolution)\n",
    "\n",
    "    # Print the time per tick\n",
    "    time_per_tick = midi_data.tick_to_time(1)\n",
    "    print(\"Time per tick:\", time_per_tick)\n",
    "\n",
    "    # Get the specified track of the MIDI file\n",
    "    track = midi_data.instruments[track_index]\n",
    "\n",
    "    # Get all the \"note on\" messages in the track\n",
    "    note_on_messages = [note for note in track.notes if note.velocity > 0]\n",
    "\n",
    "    # Assuming 4/4 time signature\n",
    "    ticks_per_measure = 4 * midi_data.resolution\n",
    "\n",
    "    # Create a list to store the annotated notes\n",
    "    annotated_notes = []\n",
    "\n",
    "    # Iterate over each note on message\n",
    "    for i, note in enumerate(note_on_messages):\n",
    "        # Get the measure index of the note\n",
    "        tick = midi_data.time_to_tick(note.start)\n",
    "        measure_index = tick // ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last measure for the note\n",
    "        ticks_since_last_measure = tick % ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last note\n",
    "        if i > 0:\n",
    "            ticks_since_last_note = tick - midi_data.time_to_tick(note_on_messages[i-1].start)\n",
    "        else:\n",
    "            ticks_since_last_note = tick\n",
    "\n",
    "        # Create a dictionary to store the annotated note information\n",
    "        annotated_note = {\n",
    "            'note': note,\n",
    "            'measure_index': measure_index,\n",
    "            'ticks_since_last_measure': ticks_since_last_measure,\n",
    "            'ticks_since_last_note': ticks_since_last_note\n",
    "        }\n",
    "\n",
    "        # Append the annotated note to the list\n",
    "        annotated_notes.append(annotated_note)\n",
    "\n",
    "    # Print the first 10 annotated notes\n",
    "    # for i, annotated_note in enumerate(annotated_notes[:10]):\n",
    "    #     print(\"Note:\", annotated_note['note'])\n",
    "    #     print(\"Measure Index:\", annotated_note['measure_index'])\n",
    "    #     print(\"Ticks since last measure:\", annotated_note['ticks_since_last_measure'])\n",
    "    #     print(\"Ticks since last note:\", annotated_note['ticks_since_last_note'])\n",
    "    #     print(\"---\")\n",
    "\n",
    "    return annotated_notes\n",
    "\n",
    "# Usage\n",
    "filename = random_midi_file\n",
    "track_index = 0\n",
    "\n",
    "annotated_notes = process_midi(filename, track_index)\n",
    "print(len(annotated_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "# class for a song\n",
    "\n",
    "class Song:\n",
    "    def __init__(self, filename):\n",
    "        midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "        self.time_per_measure = midi_data.tick_to_time(midi_data.resolution * 4)\n",
    "        self.tracks = []\n",
    "        for track in midi_data.instruments:\n",
    "            track_data = []\n",
    "            for note in track.notes:\n",
    "                start_tick = note.start\n",
    "                pitch = note.pitch\n",
    "                track_data.append([start_tick, pitch])\n",
    "            # add the track data to the list of tracks in a torch tensor\n",
    "            self.tracks.append(torch.tensor(track_data, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m songs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m errors_processing \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPool\u001b[49m(cpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     24\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tqdm(pool\u001b[38;5;241m.\u001b[39mimap(process_midi_file, midi_files), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(midi_files)))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m song, error \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pool' is not defined"
     ]
    }
   ],
   "source": [
    "def process_midi_file(filename):\n",
    "    try:\n",
    "        song = Song(filename)\n",
    "        return song, None\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "\n",
    "def collect_midi_files(root_directory):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    return midi_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_directory = './lmd_full'\n",
    "    midi_files = collect_midi_files(root_directory)\n",
    "\n",
    "    songs = []\n",
    "    errors_processing = 0\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(process_midi_file, midi_files), total=len(midi_files)))\n",
    "\n",
    "    for song, error in results:\n",
    "        if song:\n",
    "            songs.append(song)\n",
    "        if error:\n",
    "            errors_processing += 1\n",
    "\n",
    "    print(f\"Errors processing: {errors_processing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of MIDI files: 178561\n",
      "MThd not found. Probably not a MIDI file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegocaples/miniconda3/envs/ai/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of notes: 380165\n",
      "notes per file: 3801.65\n"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "import os\n",
    "root_directory = './lmd_full'\n",
    "midi_files = []\n",
    "for dirpath, _, filenames in os.walk(root_directory):\n",
    "    for filename in filenames:\n",
    "\n",
    "        if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "            midi_files.append(\n",
    "                os.path.join(dirpath, filename)\n",
    "            )\n",
    "\n",
    "songs_to_count = 100\n",
    "print(f\"Total number of MIDI files: {len(midi_files)}\")\n",
    "errors_processing = 0\n",
    "total_number_of_notes = 0\n",
    "for i, midi_data in enumerate(midi_files):\n",
    "    try:\n",
    "        # get the total number of notes\n",
    "        notes_per_song = 0\n",
    "        midi_file = pretty_midi.PrettyMIDI(midi_data)\n",
    "        for track in midi_file.instruments:\n",
    "            notes_per_song += len(track.notes)\n",
    "        # print(f\"notes per song: {notes_per_song}\")\n",
    "        total_number_of_notes += notes_per_song\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors_processing += 1\n",
    "    if i > songs_to_count:\n",
    "        break\n",
    "\n",
    "    \n",
    "print(f\"Total number of notes: {total_number_of_notes}\")\n",
    "notes_per_file = total_number_of_notes / songs_to_count\n",
    "print(f\"notes per file: {notes_per_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "songs = []\n",
    "errors_processing = 0\n",
    "for midi_data in tqdm(midi_files):\n",
    "    try:\n",
    "        song = Song(midi_data)\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        errors_processing += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n",
      "torch.Size([4, 3, 32, 32]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# Generate some random data\n",
    "data = torch.randn(100, 3, 32, 32)  # 100 samples, 3 channels, 32x32 images\n",
    "labels = torch.randint(0, 10, (100,))  # 100 labels for 10 classes\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MyDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch in dataloader:\n",
    "    data_batch = batch['data']\n",
    "    labels_batch = batch['label']\n",
    "    print(data_batch.shape, labels_batch.shape)\n",
    "    # Add your training code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SongData(Dataset):\n",
    "    def __init__(self, songs):\n",
    "        self.songs = songs\n",
    "        self.global_track_id_to_song_id = []\n",
    "        for i, song in enumerate(songs):\n",
    "            for track in song.instruments:\n",
    "                self.global_track_id_to_song_id.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_track_id_to_song_id)\n",
    "\n",
    "    def __getitem__(self, instance_idx):  \n",
    "        song_idx = self.global_track_id_to_song_id[instance_idx]\n",
    "        song = self.songs[song_idx]\n",
    "        track_idx = instance_idx - song_idx\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# Generate some random data\n",
    "data = torch.randn(100, 3, 32, 32)  # 100 samples, 3 channels, 32x32 images\n",
    "labels = torch.randint(0, 10, (100,))  # 100 labels for 10 classes\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MyDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch in dataloader:\n",
    "    data_batch = batch['data']\n",
    "    labels_batch = batch['label']\n",
    "    print(data_batch.shape, labels_batch.shape)\n",
    "    # Add your training code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4972757\n",
      "TPQN: 384\n",
      "Time per tick: 0.0015625\n",
      "103823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Instrument(program=57, is_drum=False, name=\"Melodie 4\"),\n",
       " Instrument(program=42, is_drum=False, name=\"Violoncl2\"),\n",
       " Instrument(program=24, is_drum=False, name=\"GuitarAc3\"),\n",
       " Instrument(program=24, is_drum=False, name=\"GuitarAc5\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "def get_recursive_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds the total memory usage of an object.\"\"\"\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    \n",
    "    obj_id = id(obj)\n",
    "    \n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    \n",
    "    seen.add(obj_id)\n",
    "    \n",
    "    size = sys.getsizeof(obj)\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(get_recursive_size(v, seen) for v in obj.values())\n",
    "        size += sum(get_recursive_size(k, seen) for k in obj.keys())\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_recursive_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum(get_recursive_size(i, seen) for i in obj)\n",
    "    \n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "example_object = [1, 2, {3: \"a\", 4: [\"b\", \"c\"]}]\n",
    "object_1 = [([1.1]*500) for _ in range(1000)]\n",
    "object_2 = [torch.tensor([1.1]*500) for _ in range(1000)]\n",
    "\n",
    "print(\"python\")\n",
    "print(get_recursive_size(object_1))\n",
    "print(\"tensors\")\n",
    "print(get_recursive_size(object_2))\n",
    "\n",
    "# # get the number of instruments\n",
    "# midi_data.instruments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing All Data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the filenames\n",
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_all_filenames(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    return midi_files\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "files = get_all_filenames(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: ./lmd_full/6/659414fa1c3016d94d997128f528a73b.mid\n",
      "first note:\n",
      "track: 2.0\n",
      "start: 3.2987499237060547\n",
      "duration: 3.619999885559082\n",
      "pitch: 60.0\n",
      "\n",
      "second note:\n",
      "track: 2.0\n",
      "start: 3.2987499237060547\n",
      "duration: 3.640000104904175\n",
      "pitch: 72.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "def process_midi_data(midi_filename):\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_filename)\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32), None\n",
    "\n",
    "# Usage\n",
    "filename = random.choice(files)\n",
    "song, error = process_midi_data(filename)\n",
    "print(f\"Filename: {filename}\")\n",
    "\n",
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "print(f\"first note:\")\n",
    "print(f\"track: {song[1][0]}\")\n",
    "print(f\"start: {song[1][1]}\")\n",
    "print(f\"duration: {song[1][2]}\")\n",
    "print(f\"pitch: {song[1][3]}\")\n",
    "\n",
    "print(f\"\\nsecond note:\")\n",
    "print(f\"track: {song[2][0]}\")\n",
    "print(f\"start: {song[2][1]}\")\n",
    "print(f\"duration: {song[2][2]}\")\n",
    "print(f\"pitch: {song[2][3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00% complete (0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegocaples/miniconda3/envs/ai/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01% complete (1000)\n",
      "0.01% complete (2000)\n",
      "0.02% complete (3000)\n",
      "0.02% complete (4000)\n",
      "0.03% complete (5000)\n",
      "0.03% complete (6000)\n",
      "0.04% complete (7000)\n",
      "0.04% complete (8000)\n",
      "0.05% complete (9000)\n",
      "0.06% complete (10000)\n",
      "0.06% complete (11000)\n",
      "0.07% complete (12000)\n",
      "0.07% complete (13000)\n",
      "0.08% complete (14000)\n",
      "0.08% complete (15000)\n",
      "0.09% complete (16000)\n",
      "0.10% complete (17000)\n",
      "0.10% complete (18000)\n",
      "0.11% complete (19000)\n",
      "0.11% complete (20000)\n",
      "0.12% complete (21000)\n",
      "0.12% complete (22000)\n",
      "0.13% complete (23000)\n",
      "0.13% complete (24000)\n",
      "unusual error: index -16307 is out of bounds for axis 0 with size 14508\n",
      "0.14% complete (25000)\n",
      "0.15% complete (26000)\n",
      "0.15% complete (27000)\n",
      "0.16% complete (28000)\n",
      "0.16% complete (29000)\n",
      "0.17% complete (30000)\n",
      "0.17% complete (31000)\n",
      "0.18% complete (32000)\n",
      "0.18% complete (33000)\n",
      "0.19% complete (34000)\n",
      "0.20% complete (35000)\n",
      "0.20% complete (36000)\n",
      "0.21% complete (37000)\n",
      "0.21% complete (38000)\n",
      "0.22% complete (39000)\n",
      "0.22% complete (40000)\n",
      "0.23% complete (41000)\n",
      "0.24% complete (42000)\n",
      "0.24% complete (43000)\n",
      "0.25% complete (44000)\n",
      "0.25% complete (45000)\n",
      "0.26% complete (46000)\n",
      "0.26% complete (47000)\n",
      "0.27% complete (48000)\n",
      "0.27% complete (49000)\n",
      "0.28% complete (50000)\n",
      "0.29% complete (51000)\n",
      "0.29% complete (52000)\n",
      "0.30% complete (53000)\n",
      "0.30% complete (54000)\n",
      "0.31% complete (55000)\n",
      "0.31% complete (56000)\n",
      "0.32% complete (57000)\n",
      "0.32% complete (58000)\n",
      "0.33% complete (59000)\n",
      "0.34% complete (60000)\n",
      "0.34% complete (61000)\n",
      "0.35% complete (62000)\n",
      "0.35% complete (63000)\n",
      "0.36% complete (64000)\n",
      "unusual error: index -7580 is out of bounds for axis 0 with size 2\n",
      "0.36% complete (65000)\n",
      "0.37% complete (66000)\n",
      "0.38% complete (67000)\n",
      "0.38% complete (68000)\n",
      "0.39% complete (69000)\n",
      "0.39% complete (70000)\n",
      "0.40% complete (71000)\n",
      "0.40% complete (72000)\n",
      "0.41% complete (73000)\n",
      "0.41% complete (74000)\n",
      "0.42% complete (75000)\n",
      "0.43% complete (76000)\n",
      "0.43% complete (77000)\n",
      "0.44% complete (78000)\n",
      "0.44% complete (79000)\n",
      "0.45% complete (80000)\n",
      "0.45% complete (81000)\n",
      "0.46% complete (82000)\n",
      "0.46% complete (83000)\n",
      "0.47% complete (84000)\n",
      "0.48% complete (85000)\n",
      "0.48% complete (86000)\n",
      "0.49% complete (87000)\n",
      "0.49% complete (88000)\n",
      "0.50% complete (89000)\n",
      "0.50% complete (90000)\n",
      "0.51% complete (91000)\n",
      "0.52% complete (92000)\n",
      "0.52% complete (93000)\n",
      "0.53% complete (94000)\n",
      "0.53% complete (95000)\n",
      "0.54% complete (96000)\n",
      "0.54% complete (97000)\n",
      "0.55% complete (98000)\n",
      "0.55% complete (99000)\n",
      "0.56% complete (100000)\n",
      "0.57% complete (101000)\n",
      "0.57% complete (102000)\n",
      "0.58% complete (103000)\n",
      "0.58% complete (104000)\n",
      "0.59% complete (105000)\n",
      "0.59% complete (106000)\n",
      "0.60% complete (107000)\n",
      "0.60% complete (108000)\n",
      "0.61% complete (109000)\n",
      "0.62% complete (110000)\n",
      "0.62% complete (111000)\n",
      "0.63% complete (112000)\n",
      "0.63% complete (113000)\n",
      "0.64% complete (114000)\n",
      "0.64% complete (115000)\n",
      "0.65% complete (116000)\n",
      "0.66% complete (117000)\n",
      "0.66% complete (118000)\n",
      "0.67% complete (119000)\n",
      "0.67% complete (120000)\n",
      "0.68% complete (121000)\n",
      "0.68% complete (122000)\n",
      "0.69% complete (123000)\n",
      "0.69% complete (124000)\n",
      "0.70% complete (125000)\n",
      "0.71% complete (126000)\n",
      "0.71% complete (127000)\n",
      "0.72% complete (128000)\n",
      "0.72% complete (129000)\n",
      "0.73% complete (130000)\n",
      "0.73% complete (131000)\n",
      "0.74% complete (132000)\n",
      "0.74% complete (133000)\n",
      "0.75% complete (134000)\n",
      "0.76% complete (135000)\n",
      "0.76% complete (136000)\n",
      "0.77% complete (137000)\n",
      "0.77% complete (138000)\n",
      "0.78% complete (139000)\n",
      "0.78% complete (140000)\n",
      "0.79% complete (141000)\n",
      "0.80% complete (142000)\n",
      "0.80% complete (143000)\n",
      "0.81% complete (144000)\n",
      "0.81% complete (145000)\n",
      "0.82% complete (146000)\n",
      "0.82% complete (147000)\n",
      "0.83% complete (148000)\n",
      "0.83% complete (149000)\n",
      "0.84% complete (150000)\n",
      "0.85% complete (151000)\n",
      "0.85% complete (152000)\n",
      "0.86% complete (153000)\n",
      "0.86% complete (154000)\n",
      "0.87% complete (155000)\n",
      "0.87% complete (156000)\n",
      "0.88% complete (157000)\n",
      "0.88% complete (158000)\n",
      "0.89% complete (159000)\n",
      "0.90% complete (160000)\n",
      "0.90% complete (161000)\n",
      "0.91% complete (162000)\n",
      "0.91% complete (163000)\n",
      "0.92% complete (164000)\n",
      "0.92% complete (165000)\n",
      "0.93% complete (166000)\n",
      "0.94% complete (167000)\n",
      "0.94% complete (168000)\n",
      "0.95% complete (169000)\n",
      "0.95% complete (170000)\n",
      "0.96% complete (171000)\n",
      "0.96% complete (172000)\n",
      "0.97% complete (173000)\n",
      "0.97% complete (174000)\n",
      "0.98% complete (175000)\n",
      "0.99% complete (176000)\n",
      "0.99% complete (177000)\n",
      "1.00% complete (178000)\n"
     ]
    }
   ],
   "source": [
    "# simple example of loading all the files (single process)\n",
    "errors = []\n",
    "songs = []\n",
    "totalsongs = len(files)\n",
    "for i, file in enumerate(files):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i/totalsongs:.2f}% complete ({i})\")\n",
    "    try:\n",
    "        song, error = process_midi_data(file)\n",
    "        if error is not None:\n",
    "            errors.append(error)\n",
    "            continue\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        print(f\"unusual error: {e}\")\n",
    "    # if i > 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of groups to split into\n",
    "num_groups = 10\n",
    "\n",
    "# Calculate the size of each group\n",
    "group_size = len(songs) // num_groups\n",
    "\n",
    "# Save each group directly\n",
    "for i in range(num_groups):\n",
    "    start_idx = i * group_size\n",
    "    end_idx = (i + 1) * group_size if i < num_groups - 1 else len(songs)\n",
    "    torch.save(songs[start_idx:end_idx], f'./dataset/song_group_{i}.pth')\n",
    "\n",
    "# Handle any remaining tensors if the list size isn't perfectly divisible\n",
    "if len(songs) % num_groups != 0:\n",
    "    torch.save(songs[num_groups * group_size:], f'./dataset/song_group_{num_groups - 1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# loading all the data\n",
    "# loaded_song_groups = []\n",
    "# for i in range(num_groups):\n",
    "#     loaded_song_groups.append(torch.load(f'./dataset/song_group_{i}.pth'))\n",
    "\n",
    "# # Optionally, you can concatenate all groups back into a single list if needed\n",
    "# loaded_songs = [song for group in loaded_song_groups for song in group]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the first group of tensors\n",
    "first_group = torch.load('./dataset/song_group_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7059,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  1.4118,  0.1765, 43.0000],\n",
       "        [ 0.0000,  1.4118,  1.0588, 55.0000],\n",
       "        ...,\n",
       "        [ 0.0000, 57.9706,  2.6471, 50.0000],\n",
       "        [ 0.0000, 58.0147,  2.6471, 55.0000],\n",
       "        [ 0.0000, 58.0588,  2.6471, 59.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "first_group[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how the dataset building would work on a mock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "train_idxs = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(0, len(song), test_context_length):\n",
    "        train_idxs.append([i, j])\n",
    "\n",
    "\n",
    "\n",
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "data = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(len(song) - test_context_length):\n",
    "        if j % test_context_length == 0:\n",
    "            data.append([i, j])\n",
    "\n",
    "\n",
    "def build_idxs(dataset, context_length):\n",
    "    idxs = []\n",
    "    for i, song in enumerate(dataset):\n",
    "        for j in range(len(song) - context_length):\n",
    "            if j % context_length == 0:\n",
    "                idxs.append([i, j])\n",
    "    return idxs\n",
    "\n",
    "# Usage\n",
    "test_context_length = 3\n",
    "dataset = [a, b]\n",
    "train_idxs = build_idxs(dataset, test_context_length)\n",
    "\n",
    "song, note_start = train_idxs[1]\n",
    "dataset[song][note_start:note_start + test_context_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is the first_group, but remove the first element of each song\n",
    "# first element is the header\n",
    "# each song is a tensor\n",
    "dataset = []\n",
    "for song in first_group:\n",
    "    dataset.append(song[1:])\n",
    "\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "dataset_idxs = build_idxs(dataset, context_length)\n",
    "\n",
    "song, note_start = dataset_idxs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SongDataSet(Dataset):\n",
    "    def __init__(self, songs, context_length=128):\n",
    "        self.songs = songs\n",
    "        self.dataset_idxs = build_idxs(songs, context_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        song, note_start = self.dataset_idxs[idx]\n",
    "        notes = self.songs[song][note_start:note_start + context_length]\n",
    "        # pad notes if the length is less than context_length\n",
    "        if len(notes) < context_length:\n",
    "            padding = torch.zeros(context_length - len(notes), 4) # I DON\"T HAVE THE BRAINPOWER TO FIGURE OUT RN\n",
    "            notes = torch.cat([notes, padding])\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
