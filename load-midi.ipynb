{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_random_midi_file(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    \n",
    "    return random.choice(midi_files)\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "random_midi_file = get_random_midi_file(root_directory)\n",
    "print(f\"Random MIDI file: {random_midi_file}\")\n",
    "\n",
    "filename = random_midi_file\n",
    "\n",
    "def get_random_pretty_midi_file():\n",
    "    filename = get_random_midi_file(root_directory)\n",
    "    return pretty_midi.PrettyMIDI(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def process_midi_data(midi_data):\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32)\n",
    "\n",
    "song_lengths = []\n",
    "songs = []\n",
    "for i in range(100):\n",
    "    try:\n",
    "        midi_data = get_random_pretty_midi_file()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        continue\n",
    "\n",
    "    song = process_midi_data(midi_data)\n",
    "    songs.append(song)\n",
    "\n",
    "def get_memory_used(tensor):\n",
    "    return tensor.element_size() * tensor.nelement()\n",
    "\n",
    "total_memory = 0\n",
    "for song in songs:\n",
    "    total_memory += get_memory_used(song)\n",
    "\n",
    "print(f\"Total memory used: {total_memory} bytes\")\n",
    "# in MB\n",
    "print(f\"Total memory used: {total_memory / 1024 / 1024} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "def process_midi(filename, track_index=0):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "\n",
    "    # Print the TPQN (Ticks Per Quarter Note)\n",
    "    print(\"TPQN:\", midi_data.resolution)\n",
    "\n",
    "    # Print the time per tick\n",
    "    time_per_tick = midi_data.tick_to_time(1)\n",
    "    print(\"Time per tick:\", time_per_tick)\n",
    "\n",
    "    # Get the specified track of the MIDI file\n",
    "    track = midi_data.instruments[track_index]\n",
    "\n",
    "    # Get all the \"note on\" messages in the track\n",
    "    note_on_messages = [note for note in track.notes if note.velocity > 0]\n",
    "\n",
    "    # Assuming 4/4 time signature\n",
    "    ticks_per_measure = 4 * midi_data.resolution\n",
    "\n",
    "    # Create a list to store the annotated notes\n",
    "    annotated_notes = []\n",
    "\n",
    "    # Iterate over each note on message\n",
    "    for i, note in enumerate(note_on_messages):\n",
    "        # Get the measure index of the note\n",
    "        tick = midi_data.time_to_tick(note.start)\n",
    "        measure_index = tick // ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last measure for the note\n",
    "        ticks_since_last_measure = tick % ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last note\n",
    "        if i > 0:\n",
    "            ticks_since_last_note = tick - midi_data.time_to_tick(note_on_messages[i-1].start)\n",
    "        else:\n",
    "            ticks_since_last_note = tick\n",
    "\n",
    "        # Create a dictionary to store the annotated note information\n",
    "        annotated_note = {\n",
    "            'note': note,\n",
    "            'measure_index': measure_index,\n",
    "            'ticks_since_last_measure': ticks_since_last_measure,\n",
    "            'ticks_since_last_note': ticks_since_last_note\n",
    "        }\n",
    "\n",
    "        # Append the annotated note to the list\n",
    "        annotated_notes.append(annotated_note)\n",
    "\n",
    "    # Print the first 10 annotated notes\n",
    "    # for i, annotated_note in enumerate(annotated_notes[:10]):\n",
    "    #     print(\"Note:\", annotated_note['note'])\n",
    "    #     print(\"Measure Index:\", annotated_note['measure_index'])\n",
    "    #     print(\"Ticks since last measure:\", annotated_note['ticks_since_last_measure'])\n",
    "    #     print(\"Ticks since last note:\", annotated_note['ticks_since_last_note'])\n",
    "    #     print(\"---\")\n",
    "\n",
    "    return annotated_notes\n",
    "\n",
    "# Usage\n",
    "filename = random_midi_file\n",
    "track_index = 0\n",
    "\n",
    "annotated_notes = process_midi(filename, track_index)\n",
    "print(len(annotated_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "# class for a song\n",
    "\n",
    "class Song:\n",
    "    def __init__(self, filename):\n",
    "        midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "        self.time_per_measure = midi_data.tick_to_time(midi_data.resolution * 4)\n",
    "        self.tracks = []\n",
    "        for track in midi_data.instruments:\n",
    "            track_data = []\n",
    "            for note in track.notes:\n",
    "                start_tick = note.start\n",
    "                pitch = note.pitch\n",
    "                track_data.append([start_tick, pitch])\n",
    "            # add the track data to the list of tracks in a torch tensor\n",
    "            self.tracks.append(torch.tensor(track_data, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_midi_file(filename):\n",
    "    try:\n",
    "        song = Song(filename)\n",
    "        return song, None\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "\n",
    "def collect_midi_files(root_directory):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    return midi_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_directory = './lmd_full'\n",
    "    midi_files = collect_midi_files(root_directory)\n",
    "\n",
    "    songs = []\n",
    "    errors_processing = 0\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(process_midi_file, midi_files), total=len(midi_files)))\n",
    "\n",
    "    for song, error in results:\n",
    "        if song:\n",
    "            songs.append(song)\n",
    "        if error:\n",
    "            errors_processing += 1\n",
    "\n",
    "    print(f\"Errors processing: {errors_processing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import os\n",
    "root_directory = './lmd_full'\n",
    "midi_files = []\n",
    "for dirpath, _, filenames in os.walk(root_directory):\n",
    "    for filename in filenames:\n",
    "\n",
    "        if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "            midi_files.append(\n",
    "                os.path.join(dirpath, filename)\n",
    "            )\n",
    "\n",
    "songs_to_count = 100\n",
    "print(f\"Total number of MIDI files: {len(midi_files)}\")\n",
    "errors_processing = 0\n",
    "total_number_of_notes = 0\n",
    "for i, midi_data in enumerate(midi_files):\n",
    "    try:\n",
    "        # get the total number of notes\n",
    "        notes_per_song = 0\n",
    "        midi_file = pretty_midi.PrettyMIDI(midi_data)\n",
    "        for track in midi_file.instruments:\n",
    "            notes_per_song += len(track.notes)\n",
    "        # print(f\"notes per song: {notes_per_song}\")\n",
    "        total_number_of_notes += notes_per_song\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors_processing += 1\n",
    "    if i > songs_to_count:\n",
    "        break\n",
    "\n",
    "    \n",
    "print(f\"Total number of notes: {total_number_of_notes}\")\n",
    "notes_per_file = total_number_of_notes / songs_to_count\n",
    "print(f\"notes per file: {notes_per_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "songs = []\n",
    "errors_processing = 0\n",
    "for midi_data in tqdm(midi_files):\n",
    "    try:\n",
    "        song = Song(midi_data)\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        errors_processing += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SongData(Dataset):\n",
    "    def __init__(self, songs):\n",
    "        self.songs = songs\n",
    "        self.global_track_id_to_song_id = []\n",
    "        for i, song in enumerate(songs):\n",
    "            for track in song.instruments:\n",
    "                self.global_track_id_to_song_id.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_track_id_to_song_id)\n",
    "\n",
    "    def __getitem__(self, instance_idx):  \n",
    "        song_idx = self.global_track_id_to_song_id[instance_idx]\n",
    "        song = self.songs[song_idx]\n",
    "        track_idx = instance_idx - song_idx\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# Generate some random data\n",
    "data = torch.randn(100, 3, 32, 32)  # 100 samples, 3 channels, 32x32 images\n",
    "labels = torch.randint(0, 10, (100,))  # 100 labels for 10 classes\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MyDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch in dataloader:\n",
    "    data_batch = batch['data']\n",
    "    labels_batch = batch['label']\n",
    "    print(data_batch.shape, labels_batch.shape)\n",
    "    # Add your training code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "def get_recursive_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds the total memory usage of an object.\"\"\"\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    \n",
    "    obj_id = id(obj)\n",
    "    \n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    \n",
    "    seen.add(obj_id)\n",
    "    \n",
    "    size = sys.getsizeof(obj)\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(get_recursive_size(v, seen) for v in obj.values())\n",
    "        size += sum(get_recursive_size(k, seen) for k in obj.keys())\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_recursive_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum(get_recursive_size(i, seen) for i in obj)\n",
    "    \n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "example_object = [1, 2, {3: \"a\", 4: [\"b\", \"c\"]}]\n",
    "object_1 = [([1.1]*500) for _ in range(1000)]\n",
    "object_2 = [torch.tensor([1.1]*500) for _ in range(1000)]\n",
    "\n",
    "print(\"python\")\n",
    "print(get_recursive_size(object_1))\n",
    "print(\"tensors\")\n",
    "print(get_recursive_size(object_2))\n",
    "\n",
    "# # get the number of instruments\n",
    "# midi_data.instruments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing All Data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the filenames\n",
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_all_filenames(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    return midi_files\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "files = get_all_filenames(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "def process_midi_data(midi_filename):\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_filename)\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32), None\n",
    "\n",
    "# Usage\n",
    "filename = random.choice(files)\n",
    "song, error = process_midi_data(filename)\n",
    "print(f\"Filename: {filename}\")\n",
    "\n",
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "print(f\"first note:\")\n",
    "print(f\"track: {song[1][0]}\")\n",
    "print(f\"start: {song[1][1]}\")\n",
    "print(f\"duration: {song[1][2]}\")\n",
    "print(f\"pitch: {song[1][3]}\")\n",
    "\n",
    "print(f\"\\nsecond note:\")\n",
    "print(f\"track: {song[2][0]}\")\n",
    "print(f\"start: {song[2][1]}\")\n",
    "print(f\"duration: {song[2][2]}\")\n",
    "print(f\"pitch: {song[2][3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example of loading all the files (single process)\n",
    "errors = []\n",
    "songs = []\n",
    "totalsongs = len(files)\n",
    "for i, file in enumerate(files):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i/totalsongs:.2f}% complete ({i})\")\n",
    "    try:\n",
    "        song, error = process_midi_data(file)\n",
    "        if error is not None:\n",
    "            errors.append(error)\n",
    "            continue\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        print(f\"unusual error: {e}\")\n",
    "    # if i > 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of groups to split into\n",
    "num_groups = 10\n",
    "\n",
    "# Calculate the size of each group\n",
    "group_size = len(songs) // num_groups\n",
    "\n",
    "# Save each group directly\n",
    "for i in range(num_groups):\n",
    "    start_idx = i * group_size\n",
    "    end_idx = (i + 1) * group_size if i < num_groups - 1 else len(songs)\n",
    "    torch.save(songs[start_idx:end_idx], f'./dataset/song_group_{i}.pth')\n",
    "\n",
    "# Handle any remaining tensors if the list size isn't perfectly divisible\n",
    "if len(songs) % num_groups != 0:\n",
    "    torch.save(songs[num_groups * group_size:], f'./dataset/song_group_{num_groups - 1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building SongDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# loading all the data\n",
    "# loaded_song_groups = []\n",
    "# for i in range(num_groups):\n",
    "#     loaded_song_groups.append(torch.load(f'./dataset/song_group_{i}.pth'))\n",
    "\n",
    "# # Optionally, you can concatenate all groups back into a single list if needed\n",
    "# loaded_songs = [song for group in loaded_song_groups for song in group]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# set default device to cuda, raise an error if cuda is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"CUDA is not available. Please install a CUDA-enabled version of PyTorch.\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Load the first group of tensors and move it to the GPU\n",
    "first_group = torch.load('./dataset/song_group_0.pth', map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "print(first_group[4])\n",
    "print(first_group[4].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how the dataset building would work on a mock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "train_idxs = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(0, len(song), test_context_length):\n",
    "        train_idxs.append([i, j])\n",
    "\n",
    "\n",
    "\n",
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "data = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(len(song) - test_context_length):\n",
    "        if j % test_context_length == 0:\n",
    "            data.append([i, j])\n",
    "\n",
    "\n",
    "def build_idxs(dataset, context_length, ignore_header=True):\n",
    "    idxs = []\n",
    "    if ignore_header:\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    for i, song in enumerate(dataset):\n",
    "        for j in range(start_idx, len(song) - context_length, context_length):\n",
    "            idxs.append([i, j])\n",
    "    return idxs\n",
    "\n",
    "# Usage\n",
    "test_context_length = 3\n",
    "dataset = [a, b]\n",
    "train_idxs = build_idxs(dataset, test_context_length)\n",
    "\n",
    "song, note_start = train_idxs[0]\n",
    "dataset[song][note_start:note_start + test_context_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is the first_group, but remove the first element of each song\n",
    "# first element is the header\n",
    "# each song is a tensor\n",
    "dataset = []\n",
    "for song in first_group:\n",
    "    dataset.append(song)\n",
    "\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "dataset_idxs = build_idxs(dataset, context_length)\n",
    "\n",
    "song, note_start = dataset_idxs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SongDataSet(Dataset):\n",
    "    def __init__(self, songs, context_length=128, quantize_divisor=64):\n",
    "        self.songs = songs\n",
    "        self.dataset_idxs = build_idxs(songs, context_length)\n",
    "        self.quantize_divisor = quantize_divisor\n",
    "        self.perturbation_std = 0.05\n",
    "        self.context_length = context_length\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_idxs)\n",
    "\n",
    "    def __getitem__(self, idx, different_perturbation_std=False):\n",
    "        song_idx, note_start_idx = self.dataset_idxs[idx]\n",
    "        song = self.songs[song_idx]\n",
    "        notes = song[note_start_idx:note_start_idx + context_length]\n",
    "        # Note format: [track, start, duration, pitch]\n",
    "\n",
    "        time_per_quarter_note = song[0][0] # header, first element\n",
    "        time_per_tick = (time_per_quarter_note * 4 / self.quantize_divisor).item() # quantize_divisor is ticks per measure\n",
    "\n",
    "        perturbed_notes = notes.clone()\n",
    "        # add gausian perturbations to start and duration, note index 1 and 2\n",
    "        # return torch.normal(0, self.perturbation_std, perturbed_notes[:, 1].shape).shape\n",
    "        perturbation_std = self.perturbation_std\n",
    "        if different_perturbation_std is not False:\n",
    "            perturbation_std = different_perturbation_std\n",
    "\n",
    "        perturbed_notes[:, 1] += torch.normal(0, perturbation_std, perturbed_notes[:, 1].shape, device=device)\n",
    "        perturbed_notes[:, 2] += torch.normal(0, perturbation_std, perturbed_notes[:, 2].shape, device=device)\n",
    "\n",
    "        \n",
    "        \n",
    "        perturbed_notes_quantized = self.quantize_notes(perturbed_notes, time_per_tick)\n",
    "\n",
    "        notes_quantized = self.quantize_notes(notes, time_per_tick)\n",
    "\n",
    "        return {\"notes\": notes_quantized, \"perturbed_notes\": perturbed_notes_quantized, \"time_per_tick\": time_per_tick}\n",
    "        \n",
    "\n",
    "\n",
    "    def quantize_notes(self, notes, time_per_tick):\n",
    "        # subtract the start time of the note with the smallest start time from all notes (zero-based start times, even if first is negative or positive)\n",
    "        min_start_time = notes[:, 1].min()\n",
    "        notes[:, 1] -= min_start_time\n",
    "\n",
    "        quantized_notes = []\n",
    "        for note in notes:\n",
    "            quantized_note = self.quantize_note(note, time_per_tick)\n",
    "            quantized_notes.append(quantized_note) # quantizes to about 3/100 of a second (depending on the song's time signature)\n",
    "        \n",
    "        while len(quantized_notes) < self.context_length:\n",
    "            quantized_notes.append(torch.tensor([99999, 99999, 99999, 99999, 99999], dtype=torch.int64))\n",
    "        return torch.stack(quantized_notes)\n",
    "    \n",
    "        \n",
    "\n",
    "    def quantize_note(self, note, time_per_tick):\n",
    "        track, start, duration, pitch = note\n",
    "\n",
    "        track = track.item()\n",
    "        start = start.item()\n",
    "        duration = duration.item()\n",
    "        pitch = pitch.item()\n",
    "\n",
    "        start = round(start / time_per_tick)\n",
    "        duration = round(duration / time_per_tick)\n",
    "\n",
    "        measure = start // self.quantize_divisor\n",
    "        measuretick = start % self.quantize_divisor\n",
    "\n",
    "        # get the modulo of the measure so it doesn't exceed 32\n",
    "        measure = measure % 32\n",
    "        track = track % 32\n",
    "        duration = max(min(duration, 127), 0) # bind duration index between 0 and 127\n",
    "        pitch = max(min(pitch, 127), 0) # bind pitch index between 0 and 127\n",
    "\n",
    "        # track: 0-31\n",
    "        # measure: 0-31\n",
    "        # measuretick: 0-63\n",
    "        # duration: 0-128\n",
    "        # pitch: 0-127\n",
    "        return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int64)\n",
    "\n",
    "song_dataset = SongDataSet(dataset, context_length=context_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "song_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Format Analysis and Music Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_measure_indexes = []\n",
    "\n",
    "for i in range(10000):\n",
    "    songsection = song_dataset[i]['notes']\n",
    "    maximum_measure_indexes.append(songsection[:, 1].max().item())\n",
    "\n",
    "print(\"done with the loop\")\n",
    "# for song in song_dataset.songs:\n",
    "#     maximum_time_indexes.append(song[:, 1].max())\n",
    "\n",
    "# plot the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(maximum_measure_indexes, bins=32)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_track(song_tensor, track_number):\n",
    "    \"\"\"\n",
    "    Plots the notes of a specific track from a song tensor.\n",
    "\n",
    "    Paramet        # track: 0-31\n",
    "        # measure: 0-31\n",
    "        # measuretick: 0-63\n",
    "        # duration: 0-128\n",
    "        # pitch: 0-127\n",
    "        return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)ers:\n",
    "    - song_tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    - track_number: int, the track number to plot.\n",
    "    \"\"\"\n",
    "    # Filter notes for the specified track\n",
    "    track_notes = song_tensor[song_tensor[:, 0] == track_number]\n",
    "\n",
    "    if track_notes.size(0) == 0:\n",
    "        print(f\"No notes found for track {track_number}\")\n",
    "        return\n",
    "\n",
    "    # Extract start times, durations, and pitches\n",
    "    start_times = track_notes[:, 1].numpy()\n",
    "    durations = track_notes[:, 2].numpy()\n",
    "    pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot each note as a horizontal bar\n",
    "    for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "        ax.broken_barh([(start, duration)], (pitch - 0.4, 0.8), facecolors='blue')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Time (quantized ticks)')\n",
    "    ax.set_ylabel('Pitch')\n",
    "    ax.set_title(f'Track {track_number} Notes')\n",
    "\n",
    "    # Show grid\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming song_dataset is an instance of SongDataSet and song_tensor is a tensor from the dataset\n",
    "song_tensor = song_dataset[1]  # Get the first song tensor\n",
    "plot_track(song_tensor, track_number=0)  # Plot the first track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_tracks(song_tensor):\n",
    "    \"\"\"\n",
    "    Plots the notes of all tracks from a song tensor, each in its own subplot.\n",
    "\n",
    "    Parameters:\n",
    "    - song_tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    \"\"\"\n",
    "    # Get unique track numbers\n",
    "    track_numbers = song_tensor[:, 0].unique().numpy()\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(len(track_numbers), 1, figsize=(10, 5 * len(track_numbers)), sharex=True)\n",
    "\n",
    "    if len(track_numbers) == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable if there's only one track\n",
    "\n",
    "    # Plot each track\n",
    "    for ax, track_number in zip(axes, track_numbers):\n",
    "        # Filter notes for the current track\n",
    "        track_notes = song_tensor[song_tensor[:, 0] == track_number]\n",
    "\n",
    "        if track_notes.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract start times, durations, and pitches\n",
    "        start_times = track_notes[:, 1].numpy()\n",
    "        durations = track_notes[:, 2].numpy()\n",
    "        pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "        # Plot each note as a horizontal bar\n",
    "        for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "            ax.broken_barh([(start, duration)], (pitch - 0.4, 0.8), facecolors='blue')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_ylabel('Pitch')\n",
    "        ax.set_title(f'Track {track_number} Notes')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Set common x-label\n",
    "    axes[-1].set_xlabel('Time (quantized ticks)')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming song_dataset is an instance of SongDataSet and song_tensor is a tensor from the dataset\n",
    "song_tensor = song_dataset[1]  # Get the first song tensor\n",
    "plot_all_tracks(song_tensor)  # Plot all tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import torch\n",
    "\n",
    "def tensor_to_midi(tensor, output_filename, time_per_tick):\n",
    "    \"\"\"\n",
    "    Converts a tensor representation of a song back to a MIDI file.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    - output_filename: str, the filename for the output MIDI file.\n",
    "    \"\"\"\n",
    "    # Create a PrettyMIDI object\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "\n",
    "    # Get unique track numbers\n",
    "    track_numbers = tensor[:, 0].unique().numpy()\n",
    "\n",
    "    for track_number in track_numbers:\n",
    "        # Create an Instrument instance for each track\n",
    "        instrument = pretty_midi.Instrument(program=0)  # Default to Acoustic Grand Piano\n",
    "\n",
    "        # Filter notes for the current track\n",
    "        track_notes = tensor[tensor[:, 0] == track_number]\n",
    "\n",
    "        # Extract start times, durations, and pitches\n",
    "        start_times = track_notes[:, 1].numpy()\n",
    "        durations = track_notes[:, 2].numpy()\n",
    "        pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "        # Create Note objects and add them to the instrument\n",
    "        for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=100,  # Default velocity\n",
    "                pitch=int(pitch),\n",
    "                start=(start*time_per_tick).item(),\n",
    "                end=((start + duration)*time_per_tick).item()\n",
    "            )\n",
    "            instrument.notes.append(note)\n",
    "\n",
    "        # Add the instrument to the PrettyMIDI object\n",
    "        midi.instruments.append(instrument)\n",
    "\n",
    "    # Write out the MIDI data\n",
    "    midi.write(output_filename)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "song_example = song_dataset[1]  # Get the first song tensor\n",
    "notes = song_example[\"notes\"]\n",
    "perturbed_notes = song_example[\"perturbed_notes\"]\n",
    "time_per_tick = song_example[\"time_per_tick\"]\n",
    "\n",
    "tensor_to_midi(perturbed_notes, 'output_song.mid', time_per_tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "def play_midi_file(midi_filename):\n",
    "    \"\"\"\n",
    "    Plays a MIDI file using pygame.\n",
    "\n",
    "    Parameters:\n",
    "    - midi_filename: str, the filename of the MIDI file to play.\n",
    "    \"\"\"\n",
    "    # Initialize pygame\n",
    "    pygame.init()\n",
    "\n",
    "    # Set up the mixer to play MIDI\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(midi_filename)\n",
    "\n",
    "    # Play the MIDI file\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Keep the program running until the music stops\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Example usage\n",
    "midi_filename = 'output_song.mid'\n",
    "play_midi_file(midi_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.set_default_device(device)\n",
    "# assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 500,\n",
    "    \"max_iters\": 3000, \n",
    "    \"num_epochs\": 2,\n",
    "    \"H\": 16,\n",
    "    \"B\": 64,\n",
    "    \"T\": 128,\n",
    "    \"C\": 128,\n",
    "    \"pitches\": 128,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 6,\n",
    "    \"dropout\": 0.4,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 6,\n",
    "    \"tokenizer_vocab_size\": 4096,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip(),\n",
    "    \"a\": 8,\n",
    "    \"b\": 32,\n",
    "    \"c\": 32,\n",
    "    \"d\": 16,\n",
    "    \"e\": 32,\n",
    "    \"f\": 8\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H, cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.cross_attention = cross_attention\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "\n",
    "        if self.cross_attention == True:\n",
    "            key_vectors = self.key(cross_attention_sequence)\n",
    "            assert key_vectors.shape[-2] == T, \"cross_attention_sequence must be the same length as the input sequence\"\n",
    "        else:\n",
    "            key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        if self.cross_attention == False:\n",
    "            tril = self.tril\n",
    "            wei = torch.zeros(T, T) \n",
    "            wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "\n",
    "        if self.cross_attention == False: # we are doing self-attention, causal masking\n",
    "            attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        else:\n",
    "            # we are doing cross attention, so we don't need to mask the attention weights\n",
    "            attention_weights = F.softmax(attention_pattern, dim=-1)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "# x = torch.randn(B,T,C)\n",
    "# x = torch.randn(T,C)\n",
    "# head_self_attention = Head(H, cross_attention=False)\n",
    "\n",
    "# print(head_cross_attention(x, x))\n",
    "# print(\"=\"*40)\n",
    "# print(head_self_attention(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention or cross-attention'''\n",
    "    def __init__(self, H, C, n_heads, cross_attention=False): # H is head embedding space size, n_heads is number of heads, cross_attention flag\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H, cross_attention=cross_attention) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cross_attention = cross_attention\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        if self.cross_attention and cross_attention_sequence is not None:\n",
    "            x = torch.cat([head(x, cross_attention_sequence) for head in self.heads], dim=-1)\n",
    "        else:\n",
    "            x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "multi_head_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "print(multi_head_attention(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            # nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    '''Transformer encoder block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attention(x, x))\n",
    "        x = self.norm2(x + self.feedforward(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.randn(T,C)\n",
    "\n",
    "enc = EncoderBlock(H, C, n_heads)\n",
    "\n",
    "print(enc(x).shape)\n",
    "print(T,C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    '''Transformer decoder block'''\n",
    "    def __init__(self, H, C, n_heads, feedforward_factor):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.cross_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "        self.norm3 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence):\n",
    "        x = x + self.self_attention(self.norm1(x))\n",
    "        x = x + self.cross_attention(self.norm2(x), cross_attention_sequence)\n",
    "        x = x + self.feedforward(self.norm3(x))\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "cross_attention_sequence = torch.randn(T,C)\n",
    "\n",
    "dec = DecoderBlock(H, C, n_heads, feedforward_factor)\n",
    "dec(x, cross_attention_sequence).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timingtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimingTransformer(nn.Module):\n",
    "\n",
    "    # track: 0-31\n",
    "    # measure: 0-31\n",
    "    # measuretick: 0-63\n",
    "    # duration: 0-128\n",
    "    # pitch: 0-127\n",
    "    # return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        # embedding dimensions (go into model)\n",
    "        assert a + b + c + d + e + f == C, f\"embedding dimensions must sum to C:{C}\"\n",
    "        self.track_embedding_table = nn.Embedding(32, a) # one-hot vector of length 32 (because there are 32 possible tracks) -> a\n",
    "        self.measure_embedding_table = nn.Embedding(32, b) # 32 possible measures, stored in b dimensions 32 -> b\n",
    "        self.measuretick_embedding_table = nn.Embedding(64, c)\n",
    "        self.duration_embedding_table = nn.Embedding(128, d)\n",
    "        self.pitch_embedding_table = nn.Embedding(128, e)\n",
    "        self.position_embedding_table = nn.Embedding(T, f) \n",
    "\n",
    "\n",
    "\n",
    "        # model\n",
    "        self.encoderlayers = nn.ModuleList([EncoderBlock(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.decoderlayers = nn.ModuleList([DecoderBlock(H, C, n_heads, feedforward_factor) for _ in range(n_layers)])\n",
    "\n",
    "        # de-embedding dimensions (go out of model)\n",
    "        # Output scheme: a:b track, b:c measure, c:d measuretick, d:e duration, e:f pitch, f:g position\n",
    "        # it doesn't really matter because we just try and minimize log-likelyhood of the target for each token\n",
    "\n",
    "        # first 8 dimensions are track, so we want to predict the next track, which is a distribution over the 32 tracks, so output is 32\n",
    "        # 8 -> 32\n",
    "        self.track_head = nn.Linear(a, 32)\n",
    "        # next 32 dimensions are measure, so we want to predict the next measure, which is a distribution over the 32 measures, so output is 32\n",
    "        # 32 -> 32\n",
    "        self.measure_head = nn.Linear(b, 32)\n",
    "        self.measuretick_head = nn.Linear(c, 64)\n",
    "        self.duration_head = nn.Linear(d, 128)\n",
    "        self.pitch_head = nn.Linear(e, 128)\n",
    "        self.position_head = nn.Linear(f, T) # last f dimensions are position, so we want to predict the next position, which is a distribution over the T positions, so output is T\n",
    "\n",
    "\n",
    "\n",
    "        # LEGACY GPT REFERENCE CODE\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, C) # REMOVE\n",
    "        # self.position_embedding_table = nn.Embedding(T, C) # REMOVE\n",
    "\n",
    "        # self.lm_head = nn.Linear(C, vocab_size) # REMOVE\n",
    "        # self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        # self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def encode(self, idx):\n",
    "        B, T, C = idx.shape # idx is a list of the indices of the tokens, without batch it would be:\n",
    "        # [track, measure, measuretick, duration, pitch], [track, measure, measuretick, duration, pitch], ...\n",
    "        # Channel for a given token is whether we are looking at track, measure, measuretick, duration, or pitch\n",
    "\n",
    "        tracks = idx[:, :, 0]\n",
    "        measures = idx[:, :, 1]\n",
    "        measureticks = idx[:, :, 2]\n",
    "        durations = idx[:, :, 3]\n",
    "        pitches = idx[:, :, 4]\n",
    "\n",
    "        track_emb = self.track_embedding_table(tracks)\n",
    "        measure_emb = self.measure_embedding_table(measures)\n",
    "        measuretick_emb = self.measuretick_embedding_table(measureticks)\n",
    "        duration_emb = self.duration_embedding_table(durations)\n",
    "        pitch_emb = self.pitch_embedding_table(pitches)\n",
    "\n",
    "        position_sequence = self.position_embedding_table(torch.arange(T)).unsqueeze(0) # list of position embeddings, with a batch dimension, but only 1 batch\n",
    "        pos_emb = position_sequence.repeat(B, 1, 1) # repeat across the batch dimension\n",
    "\n",
    "        assert track_emb.shape == (B, T, 8), f\"track_emb shape is {track_emb.shape}\"\n",
    "        assert measure_emb.shape == (B, T, 32), f\"measure_emb shape is {measure_emb.shape}\"\n",
    "        assert measuretick_emb.shape == (B, T, 32), f\"measuretick_emb shape is {measuretick_emb.shape}\"\n",
    "        assert duration_emb.shape == (B, T, 16), f\"duration_emb shape is {duration_emb.shape}\"\n",
    "        assert pitch_emb.shape == (B, T, 32), f\"pitch_emb shape is {pitch_emb.shape}\"\n",
    "        assert pos_emb.shape == (B, T, 8), f\"pos_emb shape is {pos_emb.shape}\"\n",
    "\n",
    "\n",
    "        return torch.cat((track_emb, measure_emb, measuretick_emb, duration_emb, pitch_emb, pos_emb), dim=-1)\n",
    "\n",
    "\n",
    "    def decode(self, latent_dimension):\n",
    "        a_start, a_end = 0, a\n",
    "        b_start, b_end = a_end, a_end + b\n",
    "        c_start, c_end = b_end, b_end + c\n",
    "        d_start, d_end = c_end, c_end + d\n",
    "        e_start, e_end = d_end, d_end + e\n",
    "        f_start, f_end = e_end, e_end + f\n",
    "\n",
    "        track_emb = latent_dimension[:,:,a_start:a_end]\n",
    "        measure_emb = latent_dimension[:,:,b_start:b_end]\n",
    "        measuretick_emb = latent_dimension[:,:,c_start:c_end]\n",
    "        duration_emb = latent_dimension[:,:,d_start:d_end]\n",
    "        pitch_emb = latent_dimension[:,:,e_start:e_end]\n",
    "\n",
    "        # input representation: [track, measure, measuretick, duration, pitch, pos] (all idx's)\n",
    "\n",
    "        track_dist = self.track_head(track_emb)\n",
    "        measure_dist = self.measure_head(measure_emb)\n",
    "        measuretick_dist = self.measuretick_head(measuretick_emb)\n",
    "        duration_dist = self.duration_head(duration_emb)\n",
    "        pitch_dist = self.pitch_head(pitch_emb)\n",
    "\n",
    "        return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, idx_dirty,  targets):\n",
    "        # ---- old GPT forward for reference ----\n",
    "\n",
    "        # B, T = idx.shape # idx is the indices of the tokens\n",
    "        # token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        # pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        # x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     x = layer(x)\n",
    "\n",
    "        # logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size. basically next-token output distribution\n",
    "\n",
    "        # batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        # if targets is None:\n",
    "        #     return logits, None\n",
    "        # else:\n",
    "        #     # a list of all the predictions, reguardles of batch.\n",
    "        #     # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "        #     # ydim: all predictions for all batches and sequences flattened (batch_dim*sequence_dim)\n",
    "        #     logits_loss_view = logits.view(-1, vocab_size) \n",
    "        #     # targets loss view\n",
    "        #     # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "        #     # so this would be like, [1,4,5,1,2,3, ...]\n",
    "        #     # where each number is the correct next index of the one hot vector\n",
    "        #     targets_loss_view = targets.view(-1)\n",
    "        #     loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "        #     return logits, loss\n",
    "        # ---- end old GPT forward for reference ----\n",
    "        \n",
    "\n",
    "\n",
    "        # idx shape: Batch, Time, [track, measure, measuretick, duration, pitch]\n",
    "        # idx shape: Batch, Time, Channel\n",
    "        b, t, c = idx_dirty.shape\n",
    "\n",
    "        encoder_sequence = self.encode(idx_dirty)\n",
    "\n",
    "        decoder_sequence = self.encode(targets) # during train, targets=correct notes, during test targets=autoregressive_sequence\n",
    "\n",
    "\n",
    "        for encoderlayer in self.encoderlayers:\n",
    "            encoder_sequence = encoderlayer(encoder_sequence)\n",
    "\n",
    "        for decoderlayer in self.decoderlayers:\n",
    "            decoder_sequence = decoderlayer(decoder_sequence, encoder_sequence)\n",
    "\n",
    "        # decode the output\n",
    "        track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist = self.decode(decoder_sequence)\n",
    "\n",
    "        track_targets = targets[:, :, 0]\n",
    "        measure_targets = targets[:, :, 1]\n",
    "        measuretick_targets = targets[:, :, 2]\n",
    "        duration_targets = targets[:, :, 3]\n",
    "        pitch_targets = targets[:, :, 4]\n",
    "\n",
    "\n",
    "        track_loss = F.cross_entropy(track_dist.view(b*t, 32), track_targets.view(b*t))\n",
    "        measure_loss = F.cross_entropy(measure_dist.view(b*t, 32), measure_targets.view(b*t))\n",
    "        measuretick_loss = F.cross_entropy(measuretick_dist.view(b*t, 64), measuretick_targets.view(b*t))\n",
    "        duration_loss = F.cross_entropy(duration_dist.view(b*t, 128), duration_targets.view(b*t))\n",
    "        pitch_loss = F.cross_entropy(pitch_dist.view(b*t, 128), pitch_targets.view(b*t))\n",
    "\n",
    "        total_loss = track_loss + measure_loss + measuretick_loss + duration_loss + pitch_loss\n",
    "\n",
    "\n",
    "\n",
    "        return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self,  full_sequence, denoised_sequence, temperature=1.0):\n",
    "        # full sequence has batch dimension + padding, denoised sequence has no batch or padding\n",
    "\n",
    "        def predict_next_token(full_sequence, denoised_sequence):\n",
    "\n",
    "            # predict the next element in denoised_sequence\n",
    "            # full_sequence: the full sequence of tokens (noisy)\n",
    "            # denoised_sequence: the denoised sequence of tokens (no batch dimension, just the sequence)\n",
    "            denoised_sequence_length = denoised_sequence.shape[0]\n",
    "            last_token_idx = denoised_sequence_length - 1\n",
    "            # pad the denoised sequence to the full sequence length with tokens of [99999, 99999, 99999, 99999, 99999]\n",
    "            denoised_sequence_padded = torch.cat((denoised_sequence, torch.ones(1, T - denoised_sequence_length, 5, dtype=torch.int64) * 99999), dim=1)\n",
    "            # add batch dimension\n",
    "            # denoised_sequence_padded = denoised_sequence_padded.unsqueeze(0)\n",
    "\n",
    "            track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist = self(full_sequence, decoder_autoregressive_input=denoised_sequence_padded)\n",
    "            return\n",
    "            # get only the first batch, so we don't deal with the batch dimension\n",
    "            track_dist = track_dist[0] # shape: [T, 32] (32 possible tracks)\n",
    "            measure_dist = measure_dist[0] # shape: [T, 32] (32 possible measures)\n",
    "            measuretick_dist = measuretick_dist[0] # shape: [T, 64] (64 possible measureticks)\n",
    "            duration_dist = duration_dist[0] # shape: [T, 128] (128 possible durations)\n",
    "            pitch_dist = pitch_dist[0] # shape: [T, 128] (128 possible pitches)\n",
    "\n",
    "            # get the predictions of the last token\n",
    "            last_note_track_logits = track_dist[last_token_idx]\n",
    "            last_note_measure_logits = measure_dist[last_token_idx]\n",
    "            last_note_measuretick_logits = measuretick_dist[last_token_idx]\n",
    "            last_note_duration_logits = duration_dist[last_token_idx]\n",
    "            last_note_pitch_logits = pitch_dist[last_token_idx]\n",
    "\n",
    "            # get the most likely track, measure, etc.\n",
    "            next_track = torch.argmax(last_note_track_logits)\n",
    "            next_measure = torch.argmax(last_note_measure_logits)\n",
    "            next_measuretick = torch.argmax(last_note_measuretick_logits)\n",
    "            next_duration = torch.argmax(last_note_duration_logits)\n",
    "            next_pitch = torch.argmax(last_note_pitch_logits)\n",
    "\n",
    "            next_token =  torch.tensor([next_track, next_measure, next_measuretick, next_duration, next_pitch])\n",
    "            return next_token\n",
    "        \n",
    "        next_token = predict_next_token(full_sequence, denoised_sequence)\n",
    "        return next_token\n",
    "\n",
    "\n",
    "        # for _ in range(max_new_tokens):\n",
    "        #     logits, loss = self(idx[:,-T:])\n",
    "        #     # get the predictions of the last token\n",
    "        #     last_token_logits = logits[:, -1, :] / temperature # all batches, last token, all probabilities\n",
    "        #     # softmax to get probabilities\n",
    "        #     probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "        #     # sample from the probabilities\n",
    "        #     next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        #     # add the new token to the idx tensor\n",
    "        #     idx = torch.cat((idx, next_token), dim=1)\n",
    "        # return idx\n",
    "    \n",
    "\n",
    "\n",
    "xb = torch.randint(0, 8, (B, T, 5))\n",
    "yb = torch.randint(0, 8, (B, T, 5))\n",
    "\n",
    "model = TimingTransformer(n_layers)\n",
    "\n",
    "\n",
    "# logits, loss = model(xb, yb)\n",
    "track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, loss = model.forward(xb, targets=yb)\n",
    "print(f\"track dist shape: {track_dist.shape}\")\n",
    "print(f\"measure dist shape: {measure_dist.shape}\")\n",
    "print(f\"measuretick dist shape: {measuretick_dist.shape}\")\n",
    "print(f\"duration dist shape: {duration_dist.shape}\")\n",
    "print(f\"pitch dist shape: {pitch_dist.shape}\")\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_idx = torch.zeros(1, T).long()\n",
    "# model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a,b,c,d,e,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we are running on cuda\n",
    "print(model.track_head.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old loop\n",
    "# eval_iters = 10\n",
    "# eval_interval = 300\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss(is_last=False):\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in ['train', 'val']:\n",
    "#         real_iters = eval_iters\n",
    "#         if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "#             real_iters *= 10 \n",
    "#         losses = torch.zeros(real_iters)\n",
    "#         for k in range(real_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean() / chars_per_token\n",
    "#     model.train()\n",
    "#     return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.softmax(torch.tensor([[0, 2, 0], [0,0,2]], dtype=torch.float), dim=1)\n",
    "print(output)\n",
    "labels = torch.tensor([1, 2], dtype=torch.int64)\n",
    "F.cross_entropy(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example forward pass using the dataset\n",
    "notes = song_dataset[3][\"notes\"]\n",
    "print(f\"original\")\n",
    "print(notes[1])\n",
    "\n",
    "notes_perturbed = song_dataset[3][\"perturbed_notes\"]\n",
    "print(f\"perturbed\")\n",
    "print(notes_perturbed[1])\n",
    "\n",
    "notes_batched = notes.unsqueeze(0)\n",
    "notes_perturbed_batched = notes_perturbed.unsqueeze(0)\n",
    "print(model(notes_batched, targets=notes_perturbed_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# assuming `song_dataset` is already created\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(song_dataset))\n",
    "val_size = len(song_dataset) - train_size\n",
    "\n",
    "# split the dataset\n",
    "train_dataset, val_dataset = random_split(song_dataset, [train_size, val_size], generator=generator)\n",
    "batch_size=B\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, generator=generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(num_samples=None):\n",
    "    model.eval()\n",
    "    losses = {'train': 0, 'val': 0}\n",
    "    for split, dataloader in [('train', train_dataloader), ('val', val_dataloader)]:\n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        if num_samples is not None:\n",
    "            num_batches = max(min(num_batches, num_samples // dataloader.batch_size), 1)\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if num_samples is not None and i >= num_batches:\n",
    "                break\n",
    "            inputs = batch['notes']\n",
    "            targets = batch['perturbed_notes']\n",
    "            _, _, _, _, _, loss = model(inputs, targets=targets)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        losses[split] = avg_loss\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_loss(num_samples = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# define the number of epochs for training\n",
    "num_epochs = config['num_epochs']\n",
    "eval_interval = config['eval_interval']\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        inputs = batch['notes']\n",
    "        targets = batch['perturbed_notes']  # or whatever your target is\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute the model output\n",
    "        _, _, _, _, _, loss = model(inputs, targets)\n",
    "\n",
    "        # backward pass: compute the gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 and i > 0:\n",
    "            eval_loss = estimate_loss(num_samples=50)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {eval_loss['val']}, Training Loss: {eval_loss['train']}\")\n",
    "\n",
    "    # compute the average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "# save the model after training\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training profiling\n",
    "```\n",
    "Epoch 1/4:   1%|          | 200/16315 [01:10<1:34:04,  2.85it/s]\n",
    "Average Data loading time: 0.0000s\n",
    "Average Forward pass time: 0.0143s\n",
    "Average Backward pass time: 0.0194s\n",
    "Average Gradient update time: 0.0105s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(device)\n",
    "torch.set_default_device(device)\n",
    "# load model.pth\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(model.track_embedding_table.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default device to cuda\n",
    "# NOTE TO SELF: Load the song_dataset above before running this code\n",
    "# first_song = song_dataset[0]\n",
    "\n",
    "# # get the notes tensor\n",
    "# notes = first_song['notes']\n",
    "# perturbed_notes = first_song['perturbed_notes']\n",
    "\n",
    "\n",
    "# perturbed_notes_batched = perturbed_notes.unsqueeze(0)\n",
    "# denoised_sequence = torch.tensor([], dtype=torch.int64)\n",
    "# # model.generate(perturbed_notes_batched, denoised_sequence)\n",
    "# model(perturbed_notes_batched)\n",
    "\n",
    "\n",
    "\n",
    "# Example forward pass using the dataset\n",
    "# notes = song_dataset[3][\"notes\"].to(device)\n",
    "# print(f\"original\")\n",
    "# print(notes[1])\n",
    "\n",
    "notes_perturbed = song_dataset[3][\"perturbed_notes\"]\n",
    "# print(f\"perturbed\")\n",
    "# print(notes_perturbed[1])\n",
    "\n",
    "notes_batched = notes_perturbed.unsqueeze(0)\n",
    "print(notes_batched.shape)\n",
    "\n",
    "autoregressive_sequence = torch.tensor([], dtype=torch.int64, device=device)\n",
    "\n",
    "# pad the autoregressive sequence to the full sequence length with tokens of [99999, 99999, 99999, 99999, 99999]\n",
    "autoregressive_sequence_padded = torch.cat((autoregressive_sequence, torch.ones(1, T - autoregressive_sequence.shape[1], 5, dtype=torch.int64) * 99999), dim=1)\n",
    "\n",
    "model(notes_batched)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model(notes_batched, decoder_autoregressive_input=autoregressive_sequence)\n",
    "pad_tensor = torch.tensor([[99999, 99999, 99999, 99999, 99999]], dtype=torch.int64)\n",
    "real_tensor = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.int64)\n",
    "while len(real_tensor) < 10:\n",
    "    real_tensor = torch.cat((real_tensor, pad_tensor), dim=0)\n",
    "\n",
    "print(real_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n",
      "tensor([[  0,   0,   0,  16,  71],\n",
      "        [  1,   0,   0, 127,  67],\n",
      "        [  2,   0,   0, 127,  59],\n",
      "        [  3,   0,   0, 127,  50],\n",
      "        [  4,   0,   0, 127,  43],\n",
      "        [  0,   0,  16,  16,  74],\n",
      "        [  0,   0,  32,  24,  74],\n",
      "        [  0,   0,  56,   8,  76],\n",
      "        [  0,   1,   0,  16,  74],\n",
      "        [  0,   1,  16,  32,  71]], device='cuda:0')\n",
      "perturbed\n",
      "tensor([[  0,   0,   0,  15,  71],\n",
      "        [  1,   0,   2, 127,  67],\n",
      "        [  2,   0,   2, 127,  59],\n",
      "        [  3,   0,   1, 127,  50],\n",
      "        [  4,   0,   1, 127,  43],\n",
      "        [  0,   0,  19,  16,  74],\n",
      "        [  0,   0,  32,  23,  74],\n",
      "        [  0,   0,  54,   4,  76],\n",
      "        [  0,   1,   3,  16,  74],\n",
      "        [  0,   1,  17,  31,  71]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# set default device to cuda, raise an error if cuda is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"CUDA is not available. Please install a CUDA-enabled version of PyTorch.\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Load the first group of tensors and move it to the GPU\n",
    "first_group = torch.load('./dataset/song_group_0.pth', map_location=device)\n",
    "\n",
    "\n",
    "\n",
    "def build_idxs(dataset, context_length, ignore_header=True):\n",
    "    idxs = []\n",
    "    if ignore_header:\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    for i, song in enumerate(dataset):\n",
    "        for j in range(start_idx, len(song) - context_length, context_length):\n",
    "            idxs.append([i, j])\n",
    "    return idxs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset is the first_group, but remove the first element of each song\n",
    "# first element is the header\n",
    "# each song is a tensor\n",
    "dataset = []\n",
    "for song in first_group:\n",
    "    dataset.append(song)\n",
    "\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "dataset_idxs = build_idxs(dataset, context_length)\n",
    "\n",
    "song, note_start = dataset_idxs[1]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SongDataSet(Dataset):\n",
    "    def __init__(self, songs, context_length=128, quantize_divisor=64):\n",
    "        self.songs = songs\n",
    "        self.dataset_idxs = build_idxs(songs, context_length)\n",
    "        self.quantize_divisor = quantize_divisor\n",
    "        self.perturbation_std = 0.05\n",
    "        self.context_length = context_length\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_idxs)\n",
    "\n",
    "    def __getitem__(self, idx, different_perturbation_std=False):\n",
    "        song_idx, note_start_idx = self.dataset_idxs[idx]\n",
    "        song = self.songs[song_idx]\n",
    "        notes = song[note_start_idx:note_start_idx + context_length]\n",
    "        # Note format: [track, start, duration, pitch]\n",
    "\n",
    "        time_per_quarter_note = song[0][0] # header, first element\n",
    "        time_per_tick = (time_per_quarter_note * 4 / self.quantize_divisor).item() # quantize_divisor is ticks per measure\n",
    "\n",
    "        perturbed_notes = notes.clone()\n",
    "        # add gausian perturbations to start and duration, note index 1 and 2\n",
    "        # return torch.normal(0, self.perturbation_std, perturbed_notes[:, 1].shape).shape\n",
    "        perturbation_std = self.perturbation_std\n",
    "        if different_perturbation_std is not False:\n",
    "            perturbation_std = different_perturbation_std\n",
    "\n",
    "        perturbed_notes[:, 1] += torch.normal(0, perturbation_std, perturbed_notes[:, 1].shape, device=device)\n",
    "        perturbed_notes[:, 2] += torch.normal(0, perturbation_std, perturbed_notes[:, 2].shape, device=device)\n",
    "\n",
    "        \n",
    "        \n",
    "        perturbed_notes_quantized = self.quantize_notes(perturbed_notes, time_per_tick)\n",
    "\n",
    "        notes_quantized = self.quantize_notes(notes, time_per_tick)\n",
    "\n",
    "        # autoregressive_sequence = [start token, note1, note2, ... , noteN]\n",
    "        # labels = [note1, note2, ... , noteN, noteN+1]\n",
    "        # start token is: [32, 32, 64, 128, 128]\n",
    "\n",
    "        autoregressive_sequence = torch.cat((torch.tensor([[32, 32, 64, 128, 128]], dtype=torch.int64, device=device), notes_quantized[:-1]), dim=0)\n",
    "        labels = notes_quantized\n",
    "\n",
    "\n",
    "\n",
    "        return {\"notes\": notes_quantized, \"perturbed_notes\": perturbed_notes_quantized, \"time_per_tick\": time_per_tick, \"autoregressive_sequence\": autoregressive_sequence, \"labels\": labels}\n",
    "        \n",
    "\n",
    "\n",
    "    def quantize_notes(self, notes, time_per_tick):\n",
    "        # subtract the start time of the note with the smallest start time from all notes (zero-based start times, even if first is negative or positive)\n",
    "        min_start_time = notes[:, 1].min()\n",
    "        notes[:, 1] -= min_start_time\n",
    "\n",
    "        quantized_notes = []\n",
    "        for note in notes:\n",
    "            quantized_note = self.quantize_note(note, time_per_tick)\n",
    "            quantized_notes.append(quantized_note) # quantizes to about 3/100 of a second (depending on the song's time signature)\n",
    "        \n",
    "        while len(quantized_notes) < self.context_length:\n",
    "            quantized_notes.append(torch.tensor([1, 1, 1, 1, 1], dtype=torch.int64))\n",
    "        return torch.stack(quantized_notes)\n",
    "    \n",
    "        \n",
    "\n",
    "    def quantize_note(self, note, time_per_tick):\n",
    "        track, start, duration, pitch = note\n",
    "\n",
    "        track = track.item()\n",
    "        start = start.item()\n",
    "        duration = duration.item()\n",
    "        pitch = pitch.item()\n",
    "\n",
    "        start = round(start / time_per_tick)\n",
    "        duration = round(duration / time_per_tick)\n",
    "\n",
    "        measure = start // self.quantize_divisor\n",
    "        measuretick = start % self.quantize_divisor\n",
    "\n",
    "        # get the modulo of the measure so it doesn't exceed 32\n",
    "        measure = measure % 32\n",
    "        track = track % 32\n",
    "        duration = max(min(duration, 127), 0) # bind duration index between 0 and 127\n",
    "        pitch = max(min(pitch, 127), 0) # bind pitch index between 0 and 127\n",
    "\n",
    "        # track: 0-31\n",
    "        # measure: 0-31\n",
    "        # measuretick: 0-63\n",
    "        # duration: 0-128\n",
    "        # pitch: 0-127\n",
    "        return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int64)\n",
    "\n",
    "song_dataset = SongDataSet(dataset, context_length=context_length)\n",
    "\n",
    "\n",
    "print(\"original\")\n",
    "print(song_dataset[0][\"notes\"][:10])\n",
    "\n",
    "print(\"perturbed\")\n",
    "print(song_dataset[0][\"perturbed_notes\"][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 22.518814086914062\n",
      "correct note: 32\n",
      "noisy note that the model got: 35\n",
      "model note: 24\n",
      "ablated sequence: tensor([[ 32,  32,  64, 128, 128],\n",
      "        [  0,   0,   0,  16,  71],\n",
      "        [  1,   0,   0, 127,  67],\n",
      "        [  2,   0,   0, 127,  59],\n",
      "        [  3,   0,   0, 127,  50],\n",
      "        [  4,   0,   0, 127,  43],\n",
      "        [  0,   0,  16,  16,  74],\n",
      "        [ 32,  32,  64, 128, 128],\n",
      "        [ 32,  32,  64, 128, 128],\n",
      "        [ 32,  32,  64, 128, 128]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "# set default device to cuda, raise an error if cuda is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"CUDA is not available. Please install a CUDA-enabled version of PyTorch.\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.set_default_device(device)\n",
    "# assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 200,\n",
    "    \"num_epochs\": 1,\n",
    "    \"H\": 16,\n",
    "    \"B\": 128,\n",
    "    \"T\": 128,\n",
    "    \"C\": 256,\n",
    "    \"pitches\": 128,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 6,\n",
    "    \"dropout\": 0.0,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 8,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip(),\n",
    "    \"a\": 16,\n",
    "    \"b\": 64,\n",
    "    \"c\": 64,\n",
    "    \"d\": 32,\n",
    "    \"e\": 64,\n",
    "    \"f\": 16\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H, cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.cross_attention = cross_attention\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "\n",
    "        if self.cross_attention == True:\n",
    "            key_vectors = self.key(cross_attention_sequence)\n",
    "            assert key_vectors.shape[-2] == T, \"cross_attention_sequence must be the same length as the input sequence\"\n",
    "            value_vectors = self.value(cross_attention_sequence)\n",
    "        else:\n",
    "            key_vectors = self.key(x)\n",
    "            value_vectors = self.value(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        if self.cross_attention == False:\n",
    "            tril = self.tril\n",
    "            wei = torch.zeros(T, T) \n",
    "            wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "\n",
    "        if self.cross_attention == False: # we are doing self-attention, causal masking\n",
    "            attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        else:\n",
    "            # we are doing cross attention, so we don't need to mask the attention weights\n",
    "            attention_weights = F.softmax(attention_pattern, dim=-1)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "# x = torch.randn(B,T,C)\n",
    "# x = torch.randn(T,C)\n",
    "# head_self_attention = Head(H, cross_attention=False)\n",
    "\n",
    "# print(head_cross_attention(x, x))\n",
    "# print(\"=\"*40)\n",
    "# print(head_self_attention(x))\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention or cross-attention'''\n",
    "    def __init__(self, H, C, n_heads, cross_attention=False): # H is head embedding space size, n_heads is number of heads, cross_attention flag\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H, cross_attention=cross_attention) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cross_attention = cross_attention\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        if self.cross_attention and cross_attention_sequence is not None:\n",
    "            x = torch.cat([head(x, cross_attention_sequence) for head in self.heads], dim=-1)\n",
    "        else:\n",
    "            x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "multi_head_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "# print(multi_head_attention(x, x))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            # nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    '''Transformer encoder block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attention(x, x))\n",
    "        x = self.norm2(x + self.feedforward(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.randn(T,C)\n",
    "\n",
    "enc = EncoderBlock(H, C, n_heads)\n",
    "\n",
    "# print(enc(x).shape)\n",
    "# print(T,C)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    '''Transformer decoder block'''\n",
    "    def __init__(self, H, C, n_heads, feedforward_factor):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.cross_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "        self.norm3 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence):\n",
    "        x = x + self.self_attention(self.norm1(x))\n",
    "        x = x + self.cross_attention(self.norm2(x), cross_attention_sequence)\n",
    "        x = x + self.feedforward(self.norm3(x))\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "cross_attention_sequence = torch.randn(T,C)\n",
    "\n",
    "dec = DecoderBlock(H, C, n_heads, feedforward_factor)\n",
    "dec(x, cross_attention_sequence).shape\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class TimingTransformer(nn.Module):\n",
    "\n",
    "    # track: 0-31\n",
    "    # measure: 0-31\n",
    "    # measuretick: 0-63\n",
    "    # duration: 0-128\n",
    "    # pitch: 0-127\n",
    "    # return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        # embedding dimensions (go into model)\n",
    "        assert a + b + c + d + e + f == C, f\"embedding dimensions must sum to C:{C}\"\n",
    "        self.track_embedding_table = nn.Embedding(32+1, a) # one-hot vector of length 32 (because there are 32 possible tracks) -> a. The +1 is for the start token\n",
    "        self.measure_embedding_table = nn.Embedding(32+1, b) # 32 possible measures, stored in b dimensions 32 -> b\n",
    "        self.measuretick_embedding_table = nn.Embedding(64+1, c)\n",
    "        self.duration_embedding_table = nn.Embedding(128+1, d)\n",
    "        self.pitch_embedding_table = nn.Embedding(128+1, e)\n",
    "        self.position_embedding_table = nn.Embedding(T, f)  # there are still only T positions, not t+1 because having start token in vocab means 1 less position,\n",
    "        #                                                   but in the case of the others, just because there is a start token doesn't mean there is one less, say pitch.\n",
    "        # start token is: [32, 32, 64, 128, 128]\n",
    "\n",
    "\n",
    "\n",
    "        # model\n",
    "        self.encoderlayers = nn.ModuleList([EncoderBlock(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.decoderlayers = nn.ModuleList([DecoderBlock(H, C, n_heads, feedforward_factor) for _ in range(n_layers)])\n",
    "\n",
    "        # de-embedding dimensions (go out of model)\n",
    "        # Output scheme: a:b track, b:c measure, c:d measuretick, d:e duration, e:f pitch, f:g position\n",
    "        # it doesn't really matter because we just try and minimize log-likelyhood of the target for each token\n",
    "\n",
    "        # first 8 dimensions are track, so we want to predict the next track, which is a distribution over the 32 tracks, so output is 32\n",
    "        # 8 -> 32\n",
    "        self.track_head = nn.Linear(a, 32) # no +1 because we would never predict the padding token\n",
    "        # next 32 dimensions are measure, so we want to predict the next measure, which is a distribution over the 32 measures, so output is 32\n",
    "        # 32 -> 32\n",
    "        self.measure_head = nn.Linear(b, 32)\n",
    "        self.measuretick_head = nn.Linear(c, 64)\n",
    "        self.duration_head = nn.Linear(d, 128)\n",
    "        self.pitch_head = nn.Linear(e, 128)\n",
    "        self.position_head = nn.Linear(f, T) # last f dimensions are position, so we want to predict the next position, which is a distribution over the T positions, so output is T\n",
    "\n",
    "\n",
    "\n",
    "        # LEGACY GPT REFERENCE CODE\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, C) # REMOVE\n",
    "        # self.position_embedding_table = nn.Embedding(T, C) # REMOVE\n",
    "\n",
    "        # self.lm_head = nn.Linear(C, vocab_size) # REMOVE\n",
    "        # self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        # self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def encode(self, idx):\n",
    "        B, T, C = idx.shape # idx is a list of the indices of the tokens, without batch it would be:\n",
    "        # [track, measure, measuretick, duration, pitch], [track, measure, measuretick, duration, pitch], ...\n",
    "        # Channel for a given token is whether we are looking at track, measure, measuretick, duration, or pitch\n",
    "\n",
    "        tracks = idx[:, :, 0]\n",
    "        measures = idx[:, :, 1]\n",
    "        measureticks = idx[:, :, 2]\n",
    "        durations = idx[:, :, 3]\n",
    "        pitches = idx[:, :, 4]\n",
    "\n",
    "\n",
    "        # assert tracks is within 0-32, measures is within 0-32, measureticks is within 0-64, durations is within 0-128, pitches is within 0-128\n",
    "        if not ((tracks >= 0).all() and (tracks <= 32).all()):\n",
    "            print(f\"tracks out of range: {tracks[(tracks < 0) | (tracks > 32)]}\")\n",
    "        if not ((measures >= 0).all() and (measures <= 32).all()):\n",
    "            print(f\"measures out of range: {measures[(measures < 0) | (measures > 32)]}\")\n",
    "        if not ((measureticks >= 0).all() and (measureticks <= 64).all()):\n",
    "            print(f\"measureticks out of range: {measureticks[(measureticks < 0) | (measureticks > 64)]}\")\n",
    "        if not ((durations >= 0).all() and (durations <= 128).all()):\n",
    "            print(f\"durations out of range: {durations[(durations < 0) | (durations > 128)]}\")\n",
    "        if not ((pitches >= 0).all() and (pitches <= 128).all()):\n",
    "            print(f\"pitches out of range: {pitches[(pitches < 0) | (pitches > 128)]}\")\n",
    "        assert (tracks >= 0).all() and (tracks <= 32).all(), f\"tracks must be within 0-32\"\n",
    "        assert (measures >= 0).all() and (measures <= 32).all(), f\"measures must be within 0-32\"\n",
    "        assert (measureticks >= 0).all() and (measureticks <= 64).all(), f\"measureticks must be within 0-64\"\n",
    "        assert (durations >= 0).all() and (durations <= 128).all(), f\"durations must be within 0-128\"\n",
    "        assert (pitches >= 0).all() and (pitches <= 128).all(), f\"pitches must be within 0-128\"\n",
    "\n",
    "        track_emb = self.track_embedding_table(tracks)\n",
    "        measure_emb = self.measure_embedding_table(measures)\n",
    "        measuretick_emb = self.measuretick_embedding_table(measureticks)\n",
    "        duration_emb = self.duration_embedding_table(durations)\n",
    "        pitch_emb = self.pitch_embedding_table(pitches)\n",
    "\n",
    "\n",
    "        position_sequence = self.position_embedding_table(torch.arange(T)).unsqueeze(0) # list of position embeddings, with a batch dimension, but only 1 batch\n",
    "        pos_emb = position_sequence.repeat(B, 1, 1) # repeat across the batch dimension\n",
    "\n",
    "        assert track_emb.shape == (B, T, a), f\"track_emb shape is {track_emb.shape}\"\n",
    "        assert measure_emb.shape == (B, T, b), f\"measure_emb shape is {measure_emb.shape}\"\n",
    "        assert measuretick_emb.shape == (B, T, c), f\"measuretick_emb shape is {measuretick_emb.shape}\"\n",
    "        assert duration_emb.shape == (B, T, d), f\"duration_emb shape is {duration_emb.shape}\"\n",
    "        assert pitch_emb.shape == (B, T, e), f\"pitch_emb shape is {pitch_emb.shape}\"\n",
    "        assert pos_emb.shape == (B, T, f), f\"pos_emb shape is {pos_emb.shape}\"\n",
    "\n",
    "\n",
    "        return torch.cat((track_emb, measure_emb, measuretick_emb, duration_emb, pitch_emb, pos_emb), dim=-1)\n",
    "\n",
    "\n",
    "    def decode(self, latent_dimension):\n",
    "        a_start, a_end = 0, a\n",
    "        b_start, b_end = a_end, a_end + b\n",
    "        c_start, c_end = b_end, b_end + c\n",
    "        d_start, d_end = c_end, c_end + d\n",
    "        e_start, e_end = d_end, d_end + e\n",
    "        f_start, f_end = e_end, e_end + f\n",
    "\n",
    "        track_emb = latent_dimension[:,:,a_start:a_end]\n",
    "        measure_emb = latent_dimension[:,:,b_start:b_end]\n",
    "        measuretick_emb = latent_dimension[:,:,c_start:c_end]\n",
    "        duration_emb = latent_dimension[:,:,d_start:d_end]\n",
    "        pitch_emb = latent_dimension[:,:,e_start:e_end]\n",
    "\n",
    "        # input representation: [track, measure, measuretick, duration, pitch, pos] (all idx's)\n",
    "\n",
    "        track_dist = self.track_head(track_emb)\n",
    "        measure_dist = self.measure_head(measure_emb)\n",
    "        measuretick_dist = self.measuretick_head(measuretick_emb)\n",
    "        duration_dist = self.duration_head(duration_emb)\n",
    "        pitch_dist = self.pitch_head(pitch_emb)\n",
    "\n",
    "        return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, autoregressive_sequence, full_encoder_sequence, targets=None):\n",
    "    \n",
    "        b, t, c = full_encoder_sequence.shape\n",
    "\n",
    "        encoder_sequence = self.encode(full_encoder_sequence)\n",
    "\n",
    "        decoder_sequence = self.encode(autoregressive_sequence)\n",
    "\n",
    "        for encoderlayer in self.encoderlayers:\n",
    "            encoder_sequence = encoderlayer(encoder_sequence)\n",
    "\n",
    "        for decoderlayer in self.decoderlayers:\n",
    "            decoder_sequence = decoderlayer(decoder_sequence, encoder_sequence)\n",
    "\n",
    "        # decode the output\n",
    "        track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist = self.decode(decoder_sequence)\n",
    "\n",
    "        if targets is None:\n",
    "            return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, None\n",
    "\n",
    "        track_targets = targets[:, :, 0]\n",
    "        measure_targets = targets[:, :, 1]\n",
    "        measuretick_targets = targets[:, :, 2]\n",
    "        duration_targets = targets[:, :, 3]\n",
    "        pitch_targets = targets[:, :, 4]\n",
    "\n",
    "        track_loss = F.cross_entropy(track_dist.view(b*t, 32), track_targets.view(b*t))\n",
    "        measure_loss = F.cross_entropy(measure_dist.view(b*t, 32), measure_targets.view(b*t))\n",
    "        measuretick_loss = F.cross_entropy(measuretick_dist.view(b*t, 64), measuretick_targets.view(b*t))\n",
    "        duration_loss = F.cross_entropy(duration_dist.view(b*t, 128), duration_targets.view(b*t))\n",
    "        pitch_loss = F.cross_entropy(pitch_dist.view(b*t, 128), pitch_targets.view(b*t))\n",
    "\n",
    "        total_loss = measuretick_loss + measure_loss + track_loss + duration_loss + pitch_loss\n",
    "\n",
    "        return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # for _ in range(max_new_tokens):\n",
    "        #     logits, loss = self(idx[:,-T:])\n",
    "        #     # get the predictions of the last token\n",
    "        #     last_token_logits = logits[:, -1, :] / temperature # all batches, last token, all probabilities\n",
    "        #     # softmax to get probabilities\n",
    "        #     probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "        #     # sample from the probabilities\n",
    "        #     next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        #     # add the new token to the idx tensor\n",
    "        #     idx = torch.cat((idx, next_token), dim=1)\n",
    "        # return idx\n",
    "    \n",
    "\n",
    "\n",
    "model = TimingTransformer(n_layers)\n",
    "# load the model weights\n",
    "# model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "\n",
    "\n",
    "# logits, loss = model(xb, yb)\n",
    "# track_dist, measure Usually there is no benefit to enforcing causality for tasks other than autoregressive generation. Non-causal connectivity is strictly more powerful. shape: {track_dist.shape}\")\n",
    "# print(f\"measure dist shape: {measure_dist.shape}\")\n",
    "# print(f\"measuretick dist shape: {measuretick_dist.shape}\")\n",
    "# print(f\"duration dist shape: {duration_dist.shape}\")\n",
    "# print(f\"pitch dist shape: {pitch_dist.shape}\")\n",
    "# print(f\"loss: {loss}\")\n",
    "\n",
    "\n",
    "# forward pass\n",
    "\n",
    "\n",
    "targets = song_dataset[0][\"labels\"]\n",
    "autoregressive_sequence = song_dataset[0][\"autoregressive_sequence\"]\n",
    "encoder_sequence = song_dataset[0][\"perturbed_notes\"]\n",
    "\n",
    "encoder_sequence = encoder_sequence.unsqueeze(0)\n",
    "autoregressive_sequence = autoregressive_sequence.unsqueeze(0)\n",
    "\n",
    "\n",
    "token_idx = 6\n",
    "\n",
    "ablate_idx = token_idx + 1\n",
    "# ablate the autoregressive sequence, replacing tokens at ablate_idx and beyond with [32, 32, 64, 128, 128]\n",
    "ablated_autoregressive_sequence = autoregressive_sequence.clone()\n",
    "ablated_autoregressive_sequence[0, ablate_idx:] = torch.tensor([32, 32, 64, 128, 128], dtype=torch.int64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, loss = model.forward(autoregressive_sequence=autoregressive_sequence, full_encoder_sequence=encoder_sequence)\n",
    "track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, loss = model.forward(autoregressive_sequence=ablated_autoregressive_sequence, full_encoder_sequence=encoder_sequence, targets=targets.unsqueeze(0))\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_channel_index_from_name(name):\n",
    "    if name == 'track':\n",
    "        return 0\n",
    "    if name == 'measure':\n",
    "        return 1\n",
    "    if name == 'measuretick':\n",
    "        return 2\n",
    "    if name == 'duration':\n",
    "        return 3\n",
    "    if name == 'pitch':\n",
    "        return 4\n",
    "\n",
    "\n",
    "channel_name = 'measuretick'\n",
    "channel_index = get_channel_index_from_name(channel_name)\n",
    "\n",
    "second_token_dist = measuretick_dist[0][token_idx]\n",
    "\n",
    "\n",
    "token_measuretick_idx_label = targets[token_idx][channel_index].item()\n",
    "\n",
    "print(\"correct note:\", token_measuretick_idx_label)\n",
    "\n",
    "incorrect_measuretick = encoder_sequence[0][token_idx]\n",
    "print(\"noisy note that the model got:\", incorrect_measuretick[channel_index].item())\n",
    "\n",
    "print(\"model note:\", torch.argmax(second_token_dist).item())\n",
    "\n",
    "print(\"ablated sequence:\", ablated_autoregressive_sequence[0][:10])\n",
    "\n",
    "\n",
    "\n",
    "# convert from logprobs to probs\n",
    "# Convert from log probabilities to probabilities\n",
    "probs = torch.exp(second_token_dist)\n",
    "\n",
    "# Plot the probabilities\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(probs)), probs.cpu().detach().numpy())\n",
    "plt.xlabel('Token Index')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Token Probability Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,736,496 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# count the # of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522073"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(song_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 22.127466201782227, 'val': 22.098102569580078}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   5%|         | 201/3671 [04:53<2:57:58,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Validation Loss: 8.438929557800293, Training Loss: 8.366434097290039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   7%|         | 246/3671 [05:58<1:21:15,  1.42s/it]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# assuming `song_dataset` is already created\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(song_dataset))\n",
    "val_size = len(song_dataset) - train_size\n",
    "\n",
    "# split the dataset\n",
    "train_dataset, val_dataset = random_split(song_dataset, [train_size, val_size], generator=generator)\n",
    "batch_size=B\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, generator=generator)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(num_samples=None):\n",
    "    model.eval()\n",
    "    losses = {'train': 0, 'val': 0}\n",
    "    for split, dataloader in [('train', train_dataloader), ('val', val_dataloader)]:\n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        if num_samples is not None:\n",
    "            num_batches = max(min(num_batches, num_samples // dataloader.batch_size), 1)\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if num_samples is not None and i >= num_batches:\n",
    "                break\n",
    "            autoregressive_sequence = batch['autoregressive_sequence']\n",
    "            targets = batch['labels']\n",
    "            encoder_sequence = batch['perturbed_notes']\n",
    "            inputs = autoregressive_sequence\n",
    "            _, _, _, _, _, loss = model(\n",
    "                autoregressive_sequence=autoregressive_sequence, \n",
    "                full_encoder_sequence=encoder_sequence,\n",
    "                targets=targets\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        losses[split] = avg_loss\n",
    "    model.train()\n",
    "    return losses\n",
    "\n",
    "print(estimate_loss(num_samples = 50))\n",
    "\n",
    "\n",
    "# set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# define the number of epochs for training\n",
    "num_epochs = config['num_epochs']\n",
    "eval_interval = config['eval_interval']\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        # dict_keys(['notes', 'perturbed_notes', 'time_per_tick', 'autoregressive_sequence', 'labels'])\n",
    "        autoregressive_sequence = batch['autoregressive_sequence']\n",
    "        targets = batch['labels']\n",
    "        encoder_sequence = batch['perturbed_notes']\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute the model output\n",
    "        _, _, _, _, _, loss = model(\n",
    "            autoregressive_sequence=autoregressive_sequence, \n",
    "            full_encoder_sequence=encoder_sequence,\n",
    "            targets=targets\n",
    "        )\n",
    "\n",
    "        # backward pass: compute the gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 and i > 0:\n",
    "            eval_loss = estimate_loss(num_samples=50)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {eval_loss['val']}, Training Loss: {eval_loss['train']}\")\n",
    "\n",
    "    # compute the average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "# save the model after training\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# set default device to cuda, raise an error if cuda is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"CUDA is not available. Please install a CUDA-enabled version of PyTorch.\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "\n",
    "print(device)\n",
    "torch.set_default_device(device)\n",
    "# load model.pth\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(model.track_embedding_table.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# assuming `song_dataset` is already created\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(song_dataset))\n",
    "val_size = len(song_dataset) - train_size\n",
    "\n",
    "# split the dataset\n",
    "train_dataset, val_dataset = random_split(song_dataset, [train_size, val_size], generator=generator)\n",
    "batch_size=B\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, generator=generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate_loss(num_samples = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned sequence:\n",
      "tensor([[  1,   0,   0, 127,  67],\n",
      "        [  2,   0,   0, 127,  59],\n",
      "        [  3,   0,   0, 127,  50],\n",
      "        [  4,   0,   0, 127,  43],\n",
      "        [  0,   0,  16,  16,  74],\n",
      "        [  0,   0,   0,  32,  50],\n",
      "        [  0,   0,   0,  16,  71],\n",
      "        [  0,   1,   0,  16,  74],\n",
      "        [  0,   1,  16,  32,  71],\n",
      "        [  0,   1,  48,  16,  67],\n",
      "        [  0,   2,   0,  16,  69],\n",
      "        [  2,   2,   0,  32,  57],\n",
      "        [  3,   2,   0,  32,  50],\n",
      "        [  0,   2,  16,  16,  69]], device='cuda:0')\n",
      "original sequence\n",
      "tensor([[  0,   0,   0,  16,  71],\n",
      "        [  1,   0,   0, 127,  67],\n",
      "        [  2,   0,   0, 127,  59],\n",
      "        [  3,   0,   0, 127,  50],\n",
      "        [  4,   0,   0, 127,  43],\n",
      "        [  0,   0,  16,  16,  74],\n",
      "        [  0,   0,  32,  24,  74],\n",
      "        [  0,   0,  56,   8,  76],\n",
      "        [  0,   1,   0,  16,  74],\n",
      "        [  0,   1,  16,  32,  71],\n",
      "        [  0,   1,  48,  16,  67],\n",
      "        [  0,   2,   0,  16,  69],\n",
      "        [  2,   2,   0,  32,  57],\n",
      "        [  3,   2,   0,  32,  50]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def pad_sequence(sequence):\n",
    "    desired_sequence_length = T\n",
    "    # ensure the sequence does not have a batch dimension (it is 2 dimensional)\n",
    "    assert len(sequence.shape) == 2, f\"Sequence has {len(sequence.shape)} dimensions, but 2 are expected\"\n",
    "\n",
    "    sequence_length = sequence.shape[-2]  # 2nd dimension from the right, aka time dimension\n",
    "\n",
    "    if sequence_length < desired_sequence_length:\n",
    "        # pad to be the desired sequence length\n",
    "        pad_tensor = torch.tensor([[32, 32, 64, 128, 128]], dtype=torch.int64)\n",
    "        while len(sequence) < desired_sequence_length:\n",
    "            sequence = torch.cat((sequence, pad_tensor), dim=0)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def predict_next_token(autoregressive_seq, encoder_sequence):\n",
    "    desired_sequence_length = T\n",
    "    prediction_idx = autoregressive_seq.shape[-2]\n",
    "\n",
    "    autoregressive_seq = pad_sequence(autoregressive_seq)\n",
    "    autoregressive_seq = autoregressive_seq.unsqueeze(0)\n",
    "    encoder_sequence = encoder_sequence.unsqueeze(0)\n",
    "\n",
    "    # run a forward pass\n",
    "    track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, loss = model(\n",
    "        autoregressive_sequence=autoregressive_seq,\n",
    "        full_encoder_sequence=encoder_sequence\n",
    "    )\n",
    "\n",
    "    predicted_track = torch.argmax(track_dist[0][prediction_idx]).item()\n",
    "    predicted_measure = torch.argmax(measure_dist[0][prediction_idx]).item()\n",
    "    predicted_measuretick = torch.argmax(measuretick_dist[0][prediction_idx]).item()\n",
    "    predicted_duration = torch.argmax(duration_dist[0][prediction_idx]).item()\n",
    "    predicted_pitch = torch.argmax(pitch_dist[0][prediction_idx]).item()\n",
    "\n",
    "    return torch.tensor([predicted_track, predicted_measure, predicted_measuretick, predicted_duration, predicted_pitch], dtype=torch.int64)\n",
    "\n",
    "# Example usage\n",
    "# autoregressive_seq = torch.tensor([[32, 32, 64, 128, 128]], dtype=torch.int64)\n",
    "\n",
    "# generated_token = predict_next_token(autoregressive_seq, encoder_sequence)\n",
    "# print(f\"Generated token: {generated_token}\")\n",
    "\n",
    "def generate_n_tokens(encoder_sequence, n):\n",
    "    autoregressive_seq = torch.tensor([[32, 32, 64, 128, 128]], dtype=torch.int64)\n",
    "    for _ in range(n):\n",
    "        generated_token = predict_next_token(autoregressive_seq, pad_sequence(encoder_sequence))\n",
    "        autoregressive_seq = torch.cat((autoregressive_seq, generated_token.unsqueeze(0)), dim=0)\n",
    "\n",
    "    return autoregressive_seq\n",
    "\n",
    "\n",
    "def correct_sequence_timing(noisy_sequence):\n",
    "    generated_sequence = generate_n_tokens(noisy_sequence, 127)\n",
    "    # remove the start token\n",
    "    generated_sequence = generated_sequence[1:]\n",
    "    return generated_sequence\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# song_data = song_dataset[0]\n",
    "# encoder_sequence = song_data[\"perturbed_notes\"]\n",
    "# generated_sequence = generate_n_tokens(encoder_sequence, 15)\n",
    "\n",
    "# print(generated_sequence[1:])\n",
    "\n",
    "# original_sequence = song_data[\"notes\"]\n",
    "# print(\"original sequence:\")\n",
    "# print(original_sequence[:14])\n",
    "\n",
    "song_data = song_dataset[0]\n",
    "encoder_sequence = song_data[\"perturbed_notes\"]\n",
    "time_per_tick = song_data[\"time_per_tick\"]\n",
    "clean_sequence = correct_sequence_timing(encoder_sequence)\n",
    "\n",
    "print(\"cleaned sequence:\")\n",
    "print(clean_sequence[:14])\n",
    "\n",
    "print(\"original sequence\")\n",
    "print(song_data[\"notes\"][:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sequence to a list of notes if format, [track, start, duration, pitch]\n",
    "def sequence_to_notes(sequence):\n",
    "    notes = []\n",
    "    for i, token in enumerate(sequence):\n",
    "        track, measure, measuretick, duration, pitch = token\n",
    "        start = (measure * 64) + measuretick\n",
    "        duration = duration\n",
    "        pitch = pitch\n",
    "        notes.append([track, start, duration, pitch])\n",
    "    return notes\n",
    "\n",
    "cleaned_notes = torch.tensor(sequence_to_notes(clean_sequence))\n",
    "dirty_notes = torch.tensor(sequence_to_notes(encoder_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import torch\n",
    "\n",
    "def tensor_to_midi(tensor, output_filename, time_per_tick):\n",
    "    \"\"\"\n",
    "    Converts a tensor representation of a song back to a MIDI file.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    - output_filename: str, the filename for the output MIDI file.\n",
    "    \"\"\"\n",
    "    # Create a PrettyMIDI object\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "\n",
    "    # Get unique track numbers\n",
    "    track_numbers = tensor[:, 0].unique().numpy()\n",
    "\n",
    "    for track_number in track_numbers:\n",
    "        # Create an Instrument instance for each track\n",
    "        instrument = pretty_midi.Instrument(program=0)  # Default to Acoustic Grand Piano\n",
    "\n",
    "        # Filter notes for the current track\n",
    "        track_notes = tensor[tensor[:, 0] == track_number]\n",
    "\n",
    "        # Extract start times, durations, and pitches\n",
    "        start_times = track_notes[:, 1].numpy()\n",
    "        durations = track_notes[:, 2].numpy()\n",
    "        pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "        # Create Note objects and add them to the instrument\n",
    "        for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=100,  # Default velocity\n",
    "                pitch=int(pitch),\n",
    "                start=(start*time_per_tick).item(),\n",
    "                end=((start + duration)*time_per_tick).item()\n",
    "            )\n",
    "            instrument.notes.append(note)\n",
    "\n",
    "        # Add the instrument to the PrettyMIDI object\n",
    "        midi.instruments.append(instrument)\n",
    "\n",
    "    # Write out the MIDI data\n",
    "    midi.write(output_filename)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# song_example = song_dataset[1]  # Get the first song tensor\n",
    "# notes = song_example[\"notes\"]\n",
    "# perturbed_notes = song_example[\"perturbed_notes\"]\n",
    "# time_per_tick = song_example[\"time_per_tick\"]\n",
    "time_per_tick = time_per_tick\n",
    "notes = cleaned_notes\n",
    "\n",
    "\n",
    "tensor_to_midi(notes.cpu(), 'output_song.mid', time_per_tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     25\u001b[0m midi_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_song.mid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mplay_midi_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmidi_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 22\u001b[0m, in \u001b[0;36mplay_midi_file\u001b[0;34m(midi_filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Keep the program running until the music stops\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mget_busy():\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "def play_midi_file(midi_filename):\n",
    "    \"\"\"\n",
    "    Plays a MIDI file using pygame.\n",
    "\n",
    "    Parameters:\n",
    "    - midi_filename: str, the filename of the MIDI file to play.\n",
    "    \"\"\"\n",
    "    # Initialize pygame\n",
    "    pygame.init()\n",
    "\n",
    "    # Set up the mixer to play MIDI\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(midi_filename)\n",
    "\n",
    "    # Play the MIDI file\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Keep the program running until the music stops\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Example usage\n",
    "midi_filename = 'output_song.mid'\n",
    "play_midi_file(midi_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
