{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_random_midi_file(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    \n",
    "    return random.choice(midi_files)\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "random_midi_file = get_random_midi_file(root_directory)\n",
    "print(f\"Random MIDI file: {random_midi_file}\")\n",
    "\n",
    "filename = random_midi_file\n",
    "\n",
    "def get_random_pretty_midi_file():\n",
    "    filename = get_random_midi_file(root_directory)\n",
    "    return pretty_midi.PrettyMIDI(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def process_midi_data(midi_data):\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32)\n",
    "\n",
    "song_lengths = []\n",
    "songs = []\n",
    "for i in range(100):\n",
    "    try:\n",
    "        midi_data = get_random_pretty_midi_file()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        continue\n",
    "\n",
    "    song = process_midi_data(midi_data)\n",
    "    songs.append(song)\n",
    "\n",
    "def get_memory_used(tensor):\n",
    "    return tensor.element_size() * tensor.nelement()\n",
    "\n",
    "total_memory = 0\n",
    "for song in songs:\n",
    "    total_memory += get_memory_used(song)\n",
    "\n",
    "print(f\"Total memory used: {total_memory} bytes\")\n",
    "# in MB\n",
    "print(f\"Total memory used: {total_memory / 1024 / 1024} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "def process_midi(filename, track_index=0):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "\n",
    "    # Print the TPQN (Ticks Per Quarter Note)\n",
    "    print(\"TPQN:\", midi_data.resolution)\n",
    "\n",
    "    # Print the time per tick\n",
    "    time_per_tick = midi_data.tick_to_time(1)\n",
    "    print(\"Time per tick:\", time_per_tick)\n",
    "\n",
    "    # Get the specified track of the MIDI file\n",
    "    track = midi_data.instruments[track_index]\n",
    "\n",
    "    # Get all the \"note on\" messages in the track\n",
    "    note_on_messages = [note for note in track.notes if note.velocity > 0]\n",
    "\n",
    "    # Assuming 4/4 time signature\n",
    "    ticks_per_measure = 4 * midi_data.resolution\n",
    "\n",
    "    # Create a list to store the annotated notes\n",
    "    annotated_notes = []\n",
    "\n",
    "    # Iterate over each note on message\n",
    "    for i, note in enumerate(note_on_messages):\n",
    "        # Get the measure index of the note\n",
    "        tick = midi_data.time_to_tick(note.start)\n",
    "        measure_index = tick // ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last measure for the note\n",
    "        ticks_since_last_measure = tick % ticks_per_measure\n",
    "\n",
    "        # Get the ticks since the last note\n",
    "        if i > 0:\n",
    "            ticks_since_last_note = tick - midi_data.time_to_tick(note_on_messages[i-1].start)\n",
    "        else:\n",
    "            ticks_since_last_note = tick\n",
    "\n",
    "        # Create a dictionary to store the annotated note information\n",
    "        annotated_note = {\n",
    "            'note': note,\n",
    "            'measure_index': measure_index,\n",
    "            'ticks_since_last_measure': ticks_since_last_measure,\n",
    "            'ticks_since_last_note': ticks_since_last_note\n",
    "        }\n",
    "\n",
    "        # Append the annotated note to the list\n",
    "        annotated_notes.append(annotated_note)\n",
    "\n",
    "    # Print the first 10 annotated notes\n",
    "    # for i, annotated_note in enumerate(annotated_notes[:10]):\n",
    "    #     print(\"Note:\", annotated_note['note'])\n",
    "    #     print(\"Measure Index:\", annotated_note['measure_index'])\n",
    "    #     print(\"Ticks since last measure:\", annotated_note['ticks_since_last_measure'])\n",
    "    #     print(\"Ticks since last note:\", annotated_note['ticks_since_last_note'])\n",
    "    #     print(\"---\")\n",
    "\n",
    "    return annotated_notes\n",
    "\n",
    "# Usage\n",
    "filename = random_midi_file\n",
    "track_index = 0\n",
    "\n",
    "annotated_notes = process_midi(filename, track_index)\n",
    "print(len(annotated_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "# class for a song\n",
    "\n",
    "class Song:\n",
    "    def __init__(self, filename):\n",
    "        midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "        self.time_per_measure = midi_data.tick_to_time(midi_data.resolution * 4)\n",
    "        self.tracks = []\n",
    "        for track in midi_data.instruments:\n",
    "            track_data = []\n",
    "            for note in track.notes:\n",
    "                start_tick = note.start\n",
    "                pitch = note.pitch\n",
    "                track_data.append([start_tick, pitch])\n",
    "            # add the track data to the list of tracks in a torch tensor\n",
    "            self.tracks.append(torch.tensor(track_data, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_midi_file(filename):\n",
    "    try:\n",
    "        song = Song(filename)\n",
    "        return song, None\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "\n",
    "def collect_midi_files(root_directory):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    return midi_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_directory = './lmd_full'\n",
    "    midi_files = collect_midi_files(root_directory)\n",
    "\n",
    "    songs = []\n",
    "    errors_processing = 0\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(process_midi_file, midi_files), total=len(midi_files)))\n",
    "\n",
    "    for song, error in results:\n",
    "        if song:\n",
    "            songs.append(song)\n",
    "        if error:\n",
    "            errors_processing += 1\n",
    "\n",
    "    print(f\"Errors processing: {errors_processing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import os\n",
    "root_directory = './lmd_full'\n",
    "midi_files = []\n",
    "for dirpath, _, filenames in os.walk(root_directory):\n",
    "    for filename in filenames:\n",
    "\n",
    "        if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "            midi_files.append(\n",
    "                os.path.join(dirpath, filename)\n",
    "            )\n",
    "\n",
    "songs_to_count = 100\n",
    "print(f\"Total number of MIDI files: {len(midi_files)}\")\n",
    "errors_processing = 0\n",
    "total_number_of_notes = 0\n",
    "for i, midi_data in enumerate(midi_files):\n",
    "    try:\n",
    "        # get the total number of notes\n",
    "        notes_per_song = 0\n",
    "        midi_file = pretty_midi.PrettyMIDI(midi_data)\n",
    "        for track in midi_file.instruments:\n",
    "            notes_per_song += len(track.notes)\n",
    "        # print(f\"notes per song: {notes_per_song}\")\n",
    "        total_number_of_notes += notes_per_song\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors_processing += 1\n",
    "    if i > songs_to_count:\n",
    "        break\n",
    "\n",
    "    \n",
    "print(f\"Total number of notes: {total_number_of_notes}\")\n",
    "notes_per_file = total_number_of_notes / songs_to_count\n",
    "print(f\"notes per file: {notes_per_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "songs = []\n",
    "errors_processing = 0\n",
    "for midi_data in tqdm(midi_files):\n",
    "    try:\n",
    "        song = Song(midi_data)\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        errors_processing += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SongData(Dataset):\n",
    "    def __init__(self, songs):\n",
    "        self.songs = songs\n",
    "        self.global_track_id_to_song_id = []\n",
    "        for i, song in enumerate(songs):\n",
    "            for track in song.instruments:\n",
    "                self.global_track_id_to_song_id.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_track_id_to_song_id)\n",
    "\n",
    "    def __getitem__(self, instance_idx):  \n",
    "        song_idx = self.global_track_id_to_song_id[instance_idx]\n",
    "        song = self.songs[song_idx]\n",
    "        track_idx = instance_idx - song_idx\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample\n",
    "\n",
    "# Generate some random data\n",
    "data = torch.randn(100, 3, 32, 32)  # 100 samples, 3 channels, 32x32 images\n",
    "labels = torch.randint(0, 10, (100,))  # 100 labels for 10 classes\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MyDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for batch in dataloader:\n",
    "    data_batch = batch['data']\n",
    "    labels_batch = batch['label']\n",
    "    print(data_batch.shape, labels_batch.shape)\n",
    "    # Add your training code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "def get_recursive_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds the total memory usage of an object.\"\"\"\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    \n",
    "    obj_id = id(obj)\n",
    "    \n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    \n",
    "    seen.add(obj_id)\n",
    "    \n",
    "    size = sys.getsizeof(obj)\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(get_recursive_size(v, seen) for v in obj.values())\n",
    "        size += sum(get_recursive_size(k, seen) for k in obj.keys())\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_recursive_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum(get_recursive_size(i, seen) for i in obj)\n",
    "    \n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "example_object = [1, 2, {3: \"a\", 4: [\"b\", \"c\"]}]\n",
    "object_1 = [([1.1]*500) for _ in range(1000)]\n",
    "object_2 = [torch.tensor([1.1]*500) for _ in range(1000)]\n",
    "\n",
    "print(\"python\")\n",
    "print(get_recursive_size(object_1))\n",
    "print(\"tensors\")\n",
    "print(get_recursive_size(object_2))\n",
    "\n",
    "# # get the number of instruments\n",
    "# midi_data.instruments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing All Data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the filenames\n",
    "import os\n",
    "import random\n",
    "import pretty_midi\n",
    "\n",
    "def get_all_filenames(root_dir):\n",
    "    midi_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.endswith('.mid') or filename.endswith('.midi'):\n",
    "                midi_files.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    if not midi_files:\n",
    "        raise FileNotFoundError(f\"No MIDI files found in {root_dir}\")\n",
    "    return midi_files\n",
    "\n",
    "# Usage\n",
    "root_directory = './lmd_full'\n",
    "files = get_all_filenames(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretty_midi\n",
    "def process_midi_data(midi_filename):\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_filename)\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "    song = []\n",
    "    time_per_quarter_note = midi_data.tick_to_time(midi_data.resolution)\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "        for note in instrument.notes:\n",
    "            track = i\n",
    "            start = note.start\n",
    "            duration = note.end - note.start\n",
    "            pitch = note.pitch\n",
    "\n",
    "            notedata = [track, start, duration, pitch]\n",
    "            song.append(notedata)\n",
    "        # sort by start time\n",
    "        song.sort(key=lambda x: x[1])\n",
    "        # prepend the time_per_quarter_note\n",
    "    song.insert(0, [time_per_quarter_note, 0, 0, 0])\n",
    "    return torch.tensor(song, dtype=torch.float32), None\n",
    "\n",
    "# Usage\n",
    "filename = random.choice(files)\n",
    "song, error = process_midi_data(filename)\n",
    "print(f\"Filename: {filename}\")\n",
    "\n",
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "print(f\"first note:\")\n",
    "print(f\"track: {song[1][0]}\")\n",
    "print(f\"start: {song[1][1]}\")\n",
    "print(f\"duration: {song[1][2]}\")\n",
    "print(f\"pitch: {song[1][3]}\")\n",
    "\n",
    "print(f\"\\nsecond note:\")\n",
    "print(f\"track: {song[2][0]}\")\n",
    "print(f\"start: {song[2][1]}\")\n",
    "print(f\"duration: {song[2][2]}\")\n",
    "print(f\"pitch: {song[2][3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example of loading all the files (single process)\n",
    "errors = []\n",
    "songs = []\n",
    "totalsongs = len(files)\n",
    "for i, file in enumerate(files):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i/totalsongs:.2f}% complete ({i})\")\n",
    "    try:\n",
    "        song, error = process_midi_data(file)\n",
    "        if error is not None:\n",
    "            errors.append(error)\n",
    "            continue\n",
    "        songs.append(song)\n",
    "    except Exception as e:\n",
    "        print(f\"unusual error: {e}\")\n",
    "    # if i > 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of groups to split into\n",
    "num_groups = 10\n",
    "\n",
    "# Calculate the size of each group\n",
    "group_size = len(songs) // num_groups\n",
    "\n",
    "# Save each group directly\n",
    "for i in range(num_groups):\n",
    "    start_idx = i * group_size\n",
    "    end_idx = (i + 1) * group_size if i < num_groups - 1 else len(songs)\n",
    "    torch.save(songs[start_idx:end_idx], f'./dataset/song_group_{i}.pth')\n",
    "\n",
    "# Handle any remaining tensors if the list size isn't perfectly divisible\n",
    "if len(songs) % num_groups != 0:\n",
    "    torch.save(songs[num_groups * group_size:], f'./dataset/song_group_{num_groups - 1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building SongDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# loading all the data\n",
    "# loaded_song_groups = []\n",
    "# for i in range(num_groups):\n",
    "#     loaded_song_groups.append(torch.load(f'./dataset/song_group_{i}.pth'))\n",
    "\n",
    "# # Optionally, you can concatenate all groups back into a single list if needed\n",
    "# loaded_songs = [song for group in loaded_song_groups for song in group]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# set default device to cuda, raise an error if cuda is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"CUDA is not available. Please install a CUDA-enabled version of PyTorch.\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Load the first group of tensors and move it to the GPU\n",
    "first_group = torch.load('./dataset/song_group_0.pth', map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7059,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.4118,  0.1765, 43.0000],\n",
      "        [ 0.0000,  1.4118,  1.0588, 55.0000],\n",
      "        ...,\n",
      "        [ 0.0000, 57.9706,  2.6471, 50.0000],\n",
      "        [ 0.0000, 58.0147,  2.6471, 55.0000],\n",
      "        [ 0.0000, 58.0588,  2.6471, 59.0000]], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# the reuslting format:\n",
    "# header: [time_per_quarter_note, 0, 0, 0]\n",
    "# each note: [track, start, duration, pitch]\n",
    "\n",
    "print(first_group[4])\n",
    "print(first_group[4].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how the dataset building would work on a mock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "train_idxs = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(0, len(song), test_context_length):\n",
    "        train_idxs.append([i, j])\n",
    "\n",
    "\n",
    "\n",
    "test_context_length = 3\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "dataset = [a, b]\n",
    "\n",
    "# desired datastructure:\n",
    "# index 1: song [0], notes 0-2\n",
    "# index 2: song [0], notes 2-4\n",
    "# index 3: song [0], notes 4-6\n",
    "# index 4: song [0], notes 6-8\n",
    "# index 5: song [0], notes 8-9\n",
    "# index 6: song [1], notes 0-2\n",
    "# ...\n",
    "\n",
    "# stored in the format: [song_index, note_start_index]\n",
    "\n",
    "# Create the dataset\n",
    "data = []\n",
    "for i, song in enumerate(dataset):\n",
    "    for j in range(len(song) - test_context_length):\n",
    "        if j % test_context_length == 0:\n",
    "            data.append([i, j])\n",
    "\n",
    "\n",
    "def build_idxs(dataset, context_length, ignore_header=True):\n",
    "    idxs = []\n",
    "    if ignore_header:\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    for i, song in enumerate(dataset):\n",
    "        for j in range(start_idx, len(song) - context_length, context_length):\n",
    "            idxs.append([i, j])\n",
    "    return idxs\n",
    "\n",
    "# Usage\n",
    "test_context_length = 3\n",
    "dataset = [a, b]\n",
    "train_idxs = build_idxs(dataset, test_context_length)\n",
    "\n",
    "song, note_start = train_idxs[0]\n",
    "dataset[song][note_start:note_start + test_context_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is the first_group, but remove the first element of each song\n",
    "# first element is the header\n",
    "# each song is a tensor\n",
    "dataset = []\n",
    "for song in first_group:\n",
    "    dataset.append(song)\n",
    "\n",
    "\n",
    "context_length = 128\n",
    "\n",
    "dataset_idxs = build_idxs(dataset, context_length)\n",
    "\n",
    "song, note_start = dataset_idxs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SongDataSet(Dataset):\n",
    "    def __init__(self, songs, context_length=128, quantize_divisor=64):\n",
    "        self.songs = songs\n",
    "        self.dataset_idxs = build_idxs(songs, context_length)\n",
    "        self.quantize_divisor = quantize_divisor\n",
    "        self.perturbation_std = 0.05\n",
    "        self.context_length = context_length\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_idxs)\n",
    "\n",
    "    def __getitem__(self, idx, different_perturbation_std=False):\n",
    "        song_idx, note_start_idx = self.dataset_idxs[idx]\n",
    "        song = self.songs[song_idx]\n",
    "        notes = song[note_start_idx:note_start_idx + context_length]\n",
    "        # Note format: [track, start, duration, pitch]\n",
    "\n",
    "        time_per_quarter_note = song[0][0] # header, first element\n",
    "        time_per_tick = (time_per_quarter_note * 4 / self.quantize_divisor).item() # quantize_divisor is ticks per measure\n",
    "\n",
    "        perturbed_notes = notes.clone()\n",
    "        # add gausian perturbations to start and duration, note index 1 and 2\n",
    "        # return torch.normal(0, self.perturbation_std, perturbed_notes[:, 1].shape).shape\n",
    "        perturbation_std = self.perturbation_std\n",
    "        if different_perturbation_std is not False:\n",
    "            perturbation_std = different_perturbation_std\n",
    "\n",
    "        perturbed_notes[:, 1] += torch.normal(0, perturbation_std, perturbed_notes[:, 1].shape, device=device)\n",
    "        perturbed_notes[:, 2] += torch.normal(0, perturbation_std, perturbed_notes[:, 2].shape, device=device)\n",
    "\n",
    "        \n",
    "        \n",
    "        perturbed_notes_quantized = self.quantize_notes(perturbed_notes, time_per_tick)\n",
    "\n",
    "        notes_quantized = self.quantize_notes(notes, time_per_tick)\n",
    "\n",
    "        return {\"notes\": notes_quantized, \"perturbed_notes\": perturbed_notes_quantized, \"time_per_tick\": time_per_tick}\n",
    "        \n",
    "\n",
    "\n",
    "    def quantize_notes(self, notes, time_per_tick):\n",
    "        # subtract the start time of the note with the smallest start time from all notes (zero-based start times, even if first is negative or positive)\n",
    "        min_start_time = notes[:, 1].min()\n",
    "        notes[:, 1] -= min_start_time\n",
    "\n",
    "        quantized_notes = []\n",
    "        for note in notes:\n",
    "            quantized_note = self.quantize_note(note, time_per_tick)\n",
    "            quantized_notes.append(quantized_note) # quantizes to about 3/100 of a second (depending on the song's time signature)\n",
    "        \n",
    "        while len(quantized_notes) < self.context_length:\n",
    "            quantized_notes.append(torch.tensor([99999, 99999, 99999, 99999, 99999], dtype=torch.int64))\n",
    "        return torch.stack(quantized_notes)\n",
    "    \n",
    "        \n",
    "\n",
    "    def quantize_note(self, note, time_per_tick):\n",
    "        track, start, duration, pitch = note\n",
    "\n",
    "        track = track.item()\n",
    "        start = start.item()\n",
    "        duration = duration.item()\n",
    "        pitch = pitch.item()\n",
    "\n",
    "        start = round(start / time_per_tick)\n",
    "        duration = round(duration / time_per_tick)\n",
    "\n",
    "        measure = start // self.quantize_divisor\n",
    "        measuretick = start % self.quantize_divisor\n",
    "\n",
    "        # get the modulo of the measure so it doesn't exceed 32\n",
    "        measure = measure % 32\n",
    "        track = track % 32\n",
    "        duration = max(min(duration, 127), 0) # bind duration index between 0 and 127\n",
    "        pitch = max(min(pitch, 127), 0) # bind pitch index between 0 and 127\n",
    "\n",
    "        # track: 0-31\n",
    "        # measure: 0-31\n",
    "        # measuretick: 0-63\n",
    "        # duration: 0-128\n",
    "        # pitch: 0-127\n",
    "        return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int64)\n",
    "\n",
    "song_dataset = SongDataSet(dataset, context_length=context_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'notes': tensor([[  0,   0,   0,  16,  71],\n",
       "         [  1,   0,   0, 127,  67],\n",
       "         [  2,   0,   0, 127,  59],\n",
       "         [  3,   0,   0, 127,  50],\n",
       "         [  4,   0,   0, 127,  43],\n",
       "         [  0,   0,  16,  16,  74],\n",
       "         [  0,   0,  32,  24,  74],\n",
       "         [  0,   0,  56,   8,  76],\n",
       "         [  0,   1,   0,  16,  74],\n",
       "         [  0,   1,  16,  32,  71],\n",
       "         [  0,   1,  48,  16,  67],\n",
       "         [  0,   2,   0,  16,  69],\n",
       "         [  2,   2,   0,  32,  57],\n",
       "         [  3,   2,   0,  32,  50],\n",
       "         [  0,   2,  16,  16,  69],\n",
       "         [  0,   2,  32,  16,  67],\n",
       "         [  2,   2,  32,  16,  60],\n",
       "         [  3,   2,  32,  16,  48],\n",
       "         [  0,   2,  48,  16,  69],\n",
       "         [  2,   2,  48,  16,  57],\n",
       "         [  3,   2,  48,  16,  50],\n",
       "         [  0,   3,   0,  48,  71],\n",
       "         [  1,   3,   0,  64,  62],\n",
       "         [  2,   3,   0,  64,  59],\n",
       "         [  3,   3,   0,  64,  50],\n",
       "         [  4,   3,   0,  64,  43],\n",
       "         [  0,   3,  48,  16,  67],\n",
       "         [  0,   4,   0,  16,  71],\n",
       "         [  1,   4,   0, 127,  67],\n",
       "         [  2,   4,   0, 127,  59],\n",
       "         [  3,   4,   0, 127,  50],\n",
       "         [  4,   4,   0, 127,  43],\n",
       "         [  0,   4,  16,  16,  74],\n",
       "         [  0,   4,  32,  24,  74],\n",
       "         [  0,   4,  56,   8,  76],\n",
       "         [  0,   5,   0,  16,  74],\n",
       "         [  0,   5,  16,  32,  71],\n",
       "         [  0,   5,  48,  16,  67],\n",
       "         [  0,   6,   0,  16,  69],\n",
       "         [  1,   6,   0,  32,  66],\n",
       "         [  2,   6,   0,  32,  57],\n",
       "         [  3,   6,   0,  32,  50],\n",
       "         [  5,   6,   0,  32,  62],\n",
       "         [  0,   6,  16,  16,  69],\n",
       "         [  0,   6,  32,  16,  67],\n",
       "         [  1,   6,  32,  16,  64],\n",
       "         [  2,   6,  32,  16,  60],\n",
       "         [  3,   6,  32,  16,  48],\n",
       "         [  0,   6,  48,  16,  64],\n",
       "         [  1,   6,  48,  16,  60],\n",
       "         [  2,   6,  48,  16,  55],\n",
       "         [  3,   6,  48,  16,  48],\n",
       "         [  0,   7,   0,  64,  67],\n",
       "         [  1,   7,   0,  64,  62],\n",
       "         [  2,   7,   0,  16,  55],\n",
       "         [  3,   7,   0,  16,  43],\n",
       "         [  4,   7,   0,  64,  43],\n",
       "         [  5,   7,   0,  64,  59],\n",
       "         [  2,   7,  16,  12,  50],\n",
       "         [  3,   7,  16,  12,  38],\n",
       "         [  2,   7,  28,   4,  50],\n",
       "         [  3,   7,  28,   4,  38],\n",
       "         [  2,   7,  32,  12,  52],\n",
       "         [  3,   7,  32,  12,  40],\n",
       "         [  2,   7,  44,   4,  55],\n",
       "         [  3,   7,  44,   4,  43],\n",
       "         [  2,   7,  48,  12,  55],\n",
       "         [  3,   7,  48,  12,  43],\n",
       "         [  2,   7,  60,   4,  52],\n",
       "         [  3,   7,  60,   4,  40],\n",
       "         [  0,   8,   0,  32,  71],\n",
       "         [  1,   8,   0,  16,  67],\n",
       "         [  2,   8,   0,  32,  55],\n",
       "         [  3,   8,   0,  32,  43],\n",
       "         [  5,   8,   0,  16,  62],\n",
       "         [  1,   8,  16,  32,  67],\n",
       "         [  5,   8,  16,  32,  62],\n",
       "         [  0,   8,  32,  12,  71],\n",
       "         [  2,   8,  32,  16,  50],\n",
       "         [  0,   8,  44,   4,  69],\n",
       "         [  0,   8,  48,  12,  67],\n",
       "         [  1,   8,  48,  16,  64],\n",
       "         [  2,   8,  48,  16,  48],\n",
       "         [  5,   8,  48,  16,  60],\n",
       "         [  0,   8,  60,   4,  64],\n",
       "         [  0,   9,   0,  32,  62],\n",
       "         [  1,   9,   0,  16,  59],\n",
       "         [  3,   9,   0,  32,  43],\n",
       "         [  5,   9,   0,  16,  55],\n",
       "         [  1,   9,  16,  16,  59],\n",
       "         [  5,   9,  16,  16,  55],\n",
       "         [  0,   9,  32,  32,  67],\n",
       "         [  1,   9,  32,  32,  62],\n",
       "         [  2,   9,  32,  12,  55],\n",
       "         [  3,   9,  32,  12,  43],\n",
       "         [  5,   9,  32,  32,  59],\n",
       "         [  2,   9,  44,   4,  55],\n",
       "         [  3,   9,  44,   4,  43],\n",
       "         [  2,   9,  48,  12,  54],\n",
       "         [  3,   9,  48,  12,  42],\n",
       "         [  2,   9,  60,   4,  52],\n",
       "         [  3,   9,  60,   4,  40],\n",
       "         [  0,  10,   0,   8,  69],\n",
       "         [  1,  10,   0,   8,  66],\n",
       "         [  2,  10,   0,  32,  50],\n",
       "         [  3,  10,   0,  32,  38],\n",
       "         [  5,  10,   0,   8,  62],\n",
       "         [  0,  10,   8,  16,  69],\n",
       "         [  1,  10,   8,  16,  66],\n",
       "         [  5,  10,   8,  16,  62],\n",
       "         [  0,  10,  24,   8,  67],\n",
       "         [  1,  10,  24,   8,  64],\n",
       "         [  5,  10,  24,   8,  60],\n",
       "         [  0,  10,  32,  16,  69],\n",
       "         [  1,  10,  32,  16,  66],\n",
       "         [  2,  10,  32,  16,  50],\n",
       "         [  3,  10,  32,  16,  38],\n",
       "         [  5,  10,  32,  16,  62],\n",
       "         [  0,  10,  48,  16,  67],\n",
       "         [  1,  10,  48,  16,  64],\n",
       "         [  2,  10,  48,  16,  48],\n",
       "         [  3,  10,  48,  16,  36],\n",
       "         [  5,  10,  48,  16,  60],\n",
       "         [  0,  11,   0,  16,  71],\n",
       "         [  1,  11,   0,  16,  67],\n",
       "         [  2,  11,   0,  16,  43],\n",
       "         [  5,  11,   0,  16,  62],\n",
       "         [  0,  11,  16,  16,  74]], device='cuda:0'),\n",
       " 'perturbed_notes': tensor([[  0,   0,   3,  16,  71],\n",
       "         [  1,   0,   1, 126,  67],\n",
       "         [  2,   0,   1, 127,  59],\n",
       "         [  3,   0,   2, 125,  50],\n",
       "         [  4,   0,   0, 127,  43],\n",
       "         [  0,   0,  14,  15,  74],\n",
       "         [  0,   0,  34,  24,  74],\n",
       "         [  0,   0,  56,   8,  76],\n",
       "         [  0,   1,   1,  17,  74],\n",
       "         [  0,   1,  18,  31,  71],\n",
       "         [  0,   1,  48,  12,  67],\n",
       "         [  0,   2,   1,  16,  69],\n",
       "         [  2,   2,   1,  35,  57],\n",
       "         [  3,   2,   0,  31,  50],\n",
       "         [  0,   2,  15,  16,  69],\n",
       "         [  0,   2,  31,  18,  67],\n",
       "         [  2,   2,  33,  17,  60],\n",
       "         [  3,   2,  34,  15,  48],\n",
       "         [  0,   2,  51,  17,  69],\n",
       "         [  2,   2,  50,  17,  57],\n",
       "         [  3,   2,  48,  19,  50],\n",
       "         [  0,   2,  62,  46,  71],\n",
       "         [  1,   2,  61,  63,  62],\n",
       "         [  2,   3,   1,  67,  59],\n",
       "         [  3,   2,  62,  68,  50],\n",
       "         [  4,   3,   2,  66,  43],\n",
       "         [  0,   3,  48,  15,  67],\n",
       "         [  0,   3,  63,  16,  71],\n",
       "         [  1,   4,   0, 127,  67],\n",
       "         [  2,   3,  63, 127,  59],\n",
       "         [  3,   4,   1, 127,  50],\n",
       "         [  4,   4,   0, 127,  43],\n",
       "         [  0,   4,  16,  20,  74],\n",
       "         [  0,   4,  32,  26,  74],\n",
       "         [  0,   4,  57,  12,  76],\n",
       "         [  0,   5,   3,  14,  74],\n",
       "         [  0,   5,  18,  33,  71],\n",
       "         [  0,   5,  48,  15,  67],\n",
       "         [  0,   6,   2,  16,  69],\n",
       "         [  1,   6,   2,  34,  66],\n",
       "         [  2,   6,   2,  32,  57],\n",
       "         [  3,   5,  63,  31,  50],\n",
       "         [  5,   6,   1,  35,  62],\n",
       "         [  0,   6,  17,  16,  69],\n",
       "         [  0,   6,  32,  14,  67],\n",
       "         [  1,   6,  33,  20,  64],\n",
       "         [  2,   6,  33,  16,  60],\n",
       "         [  3,   6,  34,  17,  48],\n",
       "         [  0,   6,  50,  15,  64],\n",
       "         [  1,   6,  48,  15,  60],\n",
       "         [  2,   6,  48,  13,  55],\n",
       "         [  3,   6,  47,  17,  48],\n",
       "         [  0,   7,   0,  66,  67],\n",
       "         [  1,   7,   0,  65,  62],\n",
       "         [  2,   7,   2,  17,  55],\n",
       "         [  3,   6,  63,  18,  43],\n",
       "         [  4,   7,   3,  64,  43],\n",
       "         [  5,   6,  62,  64,  59],\n",
       "         [  2,   7,  16,  10,  50],\n",
       "         [  3,   7,  17,   8,  38],\n",
       "         [  2,   7,  30,   4,  50],\n",
       "         [  3,   7,  27,   3,  38],\n",
       "         [  2,   7,  35,  10,  52],\n",
       "         [  3,   7,  29,  10,  40],\n",
       "         [  2,   7,  48,   4,  55],\n",
       "         [  3,   7,  43,   4,  43],\n",
       "         [  2,   7,  47,  12,  55],\n",
       "         [  3,   7,  48,  12,  43],\n",
       "         [  2,   7,  61,   5,  52],\n",
       "         [  3,   7,  59,   1,  40],\n",
       "         [  0,   8,   4,  36,  71],\n",
       "         [  1,   8,   1,  17,  67],\n",
       "         [  2,   8,   2,  33,  55],\n",
       "         [  3,   8,   3,  31,  43],\n",
       "         [  5,   8,   1,  14,  62],\n",
       "         [  1,   8,  15,  30,  67],\n",
       "         [  5,   8,  15,  34,  62],\n",
       "         [  0,   8,  29,  12,  71],\n",
       "         [  2,   8,  32,  17,  50],\n",
       "         [  0,   8,  46,   8,  69],\n",
       "         [  0,   8,  49,  14,  67],\n",
       "         [  1,   8,  47,  18,  64],\n",
       "         [  2,   8,  48,  16,  48],\n",
       "         [  5,   8,  49,  18,  60],\n",
       "         [  0,   8,  57,   3,  64],\n",
       "         [  0,   8,  63,  33,  62],\n",
       "         [  1,   8,  63,  13,  59],\n",
       "         [  3,   9,   3,  30,  43],\n",
       "         [  5,   9,   1,  18,  55],\n",
       "         [  1,   9,  14,  16,  59],\n",
       "         [  5,   9,  16,  15,  55],\n",
       "         [  0,   9,  34,  31,  67],\n",
       "         [  1,   9,  31,  31,  62],\n",
       "         [  2,   9,  34,  10,  55],\n",
       "         [  3,   9,  33,  15,  43],\n",
       "         [  5,   9,  32,  32,  59],\n",
       "         [  2,   9,  45,   5,  55],\n",
       "         [  3,   9,  45,   4,  43],\n",
       "         [  2,   9,  47,  12,  54],\n",
       "         [  3,   9,  49,  11,  42],\n",
       "         [  2,   9,  61,   6,  52],\n",
       "         [  3,   9,  62,   4,  40],\n",
       "         [  0,  10,   0,  10,  69],\n",
       "         [  1,  10,   4,   8,  66],\n",
       "         [  2,  10,   2,  33,  50],\n",
       "         [  3,   9,  63,  34,  38],\n",
       "         [  5,  10,   0,   9,  62],\n",
       "         [  0,  10,  12,  15,  69],\n",
       "         [  1,  10,  10,  16,  66],\n",
       "         [  5,  10,  10,  14,  62],\n",
       "         [  0,  10,  24,   7,  67],\n",
       "         [  1,  10,  27,   9,  64],\n",
       "         [  5,  10,  23,  10,  60],\n",
       "         [  0,  10,  30,  14,  69],\n",
       "         [  1,  10,  30,  16,  66],\n",
       "         [  2,  10,  35,  15,  50],\n",
       "         [  3,  10,  34,  16,  38],\n",
       "         [  5,  10,  34,  16,  62],\n",
       "         [  0,  10,  48,  17,  67],\n",
       "         [  1,  10,  44,  16,  64],\n",
       "         [  2,  10,  49,  15,  48],\n",
       "         [  3,  10,  49,  17,  36],\n",
       "         [  5,  10,  50,  16,  60],\n",
       "         [  0,  11,   1,  17,  71],\n",
       "         [  1,  11,   2,  17,  67],\n",
       "         [  2,  11,   0,  16,  43],\n",
       "         [  5,  10,  61,  15,  62],\n",
       "         [  0,  11,  18,  18,  74]], device='cuda:0'),\n",
       " 'time_per_tick': 0.03125}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "song_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Format Analysis and Music Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_measure_indexes = []\n",
    "\n",
    "for i in range(10000):\n",
    "    songsection = song_dataset[i]['notes']\n",
    "    maximum_measure_indexes.append(songsection[:, 1].max().item())\n",
    "\n",
    "print(\"done with the loop\")\n",
    "# for song in song_dataset.songs:\n",
    "#     maximum_time_indexes.append(song[:, 1].max())\n",
    "\n",
    "# plot the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(maximum_measure_indexes, bins=32)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_track(song_tensor, track_number):\n",
    "    \"\"\"\n",
    "    Plots the notes of a specific track from a song tensor.\n",
    "\n",
    "    Paramet        # track: 0-31\n",
    "        # measure: 0-31\n",
    "        # measuretick: 0-63\n",
    "        # duration: 0-128\n",
    "        # pitch: 0-127\n",
    "        return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)ers:\n",
    "    - song_tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    - track_number: int, the track number to plot.\n",
    "    \"\"\"\n",
    "    # Filter notes for the specified track\n",
    "    track_notes = song_tensor[song_tensor[:, 0] == track_number]\n",
    "\n",
    "    if track_notes.size(0) == 0:\n",
    "        print(f\"No notes found for track {track_number}\")\n",
    "        return\n",
    "\n",
    "    # Extract start times, durations, and pitches\n",
    "    start_times = track_notes[:, 1].numpy()\n",
    "    durations = track_notes[:, 2].numpy()\n",
    "    pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot each note as a horizontal bar\n",
    "    for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "        ax.broken_barh([(start, duration)], (pitch - 0.4, 0.8), facecolors='blue')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Time (quantized ticks)')\n",
    "    ax.set_ylabel('Pitch')\n",
    "    ax.set_title(f'Track {track_number} Notes')\n",
    "\n",
    "    # Show grid\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming song_dataset is an instance of SongDataSet and song_tensor is a tensor from the dataset\n",
    "song_tensor = song_dataset[1]  # Get the first song tensor\n",
    "plot_track(song_tensor, track_number=0)  # Plot the first track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_tracks(song_tensor):\n",
    "    \"\"\"\n",
    "    Plots the notes of all tracks from a song tensor, each in its own subplot.\n",
    "\n",
    "    Parameters:\n",
    "    - song_tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    \"\"\"\n",
    "    # Get unique track numbers\n",
    "    track_numbers = song_tensor[:, 0].unique().numpy()\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(len(track_numbers), 1, figsize=(10, 5 * len(track_numbers)), sharex=True)\n",
    "\n",
    "    if len(track_numbers) == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable if there's only one track\n",
    "\n",
    "    # Plot each track\n",
    "    for ax, track_number in zip(axes, track_numbers):\n",
    "        # Filter notes for the current track\n",
    "        track_notes = song_tensor[song_tensor[:, 0] == track_number]\n",
    "\n",
    "        if track_notes.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract start times, durations, and pitches\n",
    "        start_times = track_notes[:, 1].numpy()\n",
    "        durations = track_notes[:, 2].numpy()\n",
    "        pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "        # Plot each note as a horizontal bar\n",
    "        for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "            ax.broken_barh([(start, duration)], (pitch - 0.4, 0.8), facecolors='blue')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_ylabel('Pitch')\n",
    "        ax.set_title(f'Track {track_number} Notes')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Set common x-label\n",
    "    axes[-1].set_xlabel('Time (quantized ticks)')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming song_dataset is an instance of SongDataSet and song_tensor is a tensor from the dataset\n",
    "song_tensor = song_dataset[1]  # Get the first song tensor\n",
    "plot_all_tracks(song_tensor)  # Plot all tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import torch\n",
    "\n",
    "def tensor_to_midi(tensor, output_filename, time_per_tick):\n",
    "    \"\"\"\n",
    "    Converts a tensor representation of a song back to a MIDI file.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: torch.Tensor, the quantized notes tensor of the song.\n",
    "    - output_filename: str, the filename for the output MIDI file.\n",
    "    \"\"\"\n",
    "    # Create a PrettyMIDI object\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "\n",
    "    # Get unique track numbers\n",
    "    track_numbers = tensor[:, 0].unique().numpy()\n",
    "\n",
    "    for track_number in track_numbers:\n",
    "        # Create an Instrument instance for each track\n",
    "        instrument = pretty_midi.Instrument(program=0)  # Default to Acoustic Grand Piano\n",
    "\n",
    "        # Filter notes for the current track\n",
    "        track_notes = tensor[tensor[:, 0] == track_number]\n",
    "\n",
    "        # Extract start times, durations, and pitches\n",
    "        start_times = track_notes[:, 1].numpy()\n",
    "        durations = track_notes[:, 2].numpy()\n",
    "        pitches = track_notes[:, 3].numpy()\n",
    "\n",
    "        # Create Note objects and add them to the instrument\n",
    "        for start, duration, pitch in zip(start_times, durations, pitches):\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=100,  # Default velocity\n",
    "                pitch=int(pitch),\n",
    "                start=(start*time_per_tick).item(),\n",
    "                end=((start + duration)*time_per_tick).item()\n",
    "            )\n",
    "            instrument.notes.append(note)\n",
    "\n",
    "        # Add the instrument to the PrettyMIDI object\n",
    "        midi.instruments.append(instrument)\n",
    "\n",
    "    # Write out the MIDI data\n",
    "    midi.write(output_filename)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "song_example = song_dataset[1]  # Get the first song tensor\n",
    "notes = song_example[\"notes\"]\n",
    "perturbed_notes = song_example[\"perturbed_notes\"]\n",
    "time_per_tick = song_example[\"time_per_tick\"]\n",
    "\n",
    "tensor_to_midi(perturbed_notes, 'output_song.mid', time_per_tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "def play_midi_file(midi_filename):\n",
    "    \"\"\"\n",
    "    Plays a MIDI file using pygame.\n",
    "\n",
    "    Parameters:\n",
    "    - midi_filename: str, the filename of the MIDI file to play.\n",
    "    \"\"\"\n",
    "    # Initialize pygame\n",
    "    pygame.init()\n",
    "\n",
    "    # Set up the mixer to play MIDI\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(midi_filename)\n",
    "\n",
    "    # Play the MIDI file\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Keep the program running until the music stops\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Example usage\n",
    "midi_filename = 'output_song.mid'\n",
    "play_midi_file(midi_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.set_default_device(device)\n",
    "# assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"eval_interval\": 500,\n",
    "    \"max_iters\": 3000, \n",
    "    \"num_epochs\": 2,\n",
    "    \"H\": 16,\n",
    "    \"B\": 64,\n",
    "    \"T\": 128,\n",
    "    \"C\": 128,\n",
    "    \"pitches\": 128,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 6,\n",
    "    \"dropout\": 0.4,\n",
    "    \"l2_penalty\": 0.0,\n",
    "    \"n_layers\": 6,\n",
    "    \"tokenizer_vocab_size\": 4096,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip(),\n",
    "    \"a\": 8,\n",
    "    \"b\": 32,\n",
    "    \"c\": 32,\n",
    "    \"d\": 16,\n",
    "    \"e\": 32,\n",
    "    \"f\": 8\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H, cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.cross_attention = cross_attention\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "\n",
    "        if self.cross_attention == True:\n",
    "            key_vectors = self.key(cross_attention_sequence)\n",
    "            assert key_vectors.shape[-2] == T, \"cross_attention_sequence must be the same length as the input sequence\"\n",
    "        else:\n",
    "            key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        if self.cross_attention == False:\n",
    "            tril = self.tril\n",
    "            wei = torch.zeros(T, T) \n",
    "            wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "\n",
    "        if self.cross_attention == False: # we are doing self-attention, causal masking\n",
    "            attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "        else:\n",
    "            # we are doing cross attention, so we don't need to mask the attention weights\n",
    "            attention_weights = F.softmax(attention_pattern, dim=-1)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "# x = torch.randn(B,T,C)\n",
    "# x = torch.randn(T,C)\n",
    "# head_self_attention = Head(H, cross_attention=False)\n",
    "\n",
    "# print(head_cross_attention(x, x))\n",
    "# print(\"=\"*40)\n",
    "# print(head_self_attention(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000,  0.3373,  0.0875,  ..., -0.0000,  0.1044, -0.0394],\n",
      "        [-0.2252,  0.2746,  0.0063,  ..., -0.0468,  0.1242, -0.0421],\n",
      "        [-0.1656,  0.2630,  0.1539,  ..., -0.0227,  0.1937, -0.0510],\n",
      "        ...,\n",
      "        [-0.0000,  0.1527,  0.1089,  ..., -0.1539,  0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0768,  ..., -0.0000,  0.0000,  0.0326],\n",
      "        [-0.0000,  0.0000,  0.1228,  ..., -0.0224,  0.0000, -0.0622]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention or cross-attention'''\n",
    "    def __init__(self, H, C, n_heads, cross_attention=False): # H is head embedding space size, n_heads is number of heads, cross_attention flag\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H, cross_attention=cross_attention) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cross_attention = cross_attention\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence=None):\n",
    "        if self.cross_attention and cross_attention_sequence is not None:\n",
    "            x = torch.cat([head(x, cross_attention_sequence) for head in self.heads], dim=-1)\n",
    "        else:\n",
    "            x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return self.dropout(x)\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "multi_head_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "print(multi_head_attention(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "            # nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n",
      "128 128\n"
     ]
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    '''Transformer encoder block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attention(x, x))\n",
    "        x = self.norm2(x + self.feedforward(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.randn(T,C)\n",
    "\n",
    "enc = EncoderBlock(H, C, n_heads)\n",
    "\n",
    "print(enc(x).shape)\n",
    "print(T,C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    '''Transformer decoder block'''\n",
    "    def __init__(self, H, C, n_heads, feedforward_factor):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.cross_attention = MultiHeadAttention(H, C, n_heads, cross_attention=True)\n",
    "        self.feedforward = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C)\n",
    "        self.norm2 = LayerNorm(C)\n",
    "        self.norm3 = LayerNorm(C)\n",
    "\n",
    "    def forward(self, x, cross_attention_sequence):\n",
    "        x = x + self.self_attention(self.norm1(x))\n",
    "        x = x + self.cross_attention(self.norm2(x), cross_attention_sequence)\n",
    "        x = x + self.feedforward(self.norm3(x))\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(T,C)\n",
    "cross_attention_sequence = torch.randn(T,C)\n",
    "\n",
    "dec = DecoderBlock(H, C, n_heads, feedforward_factor)\n",
    "dec(x, cross_attention_sequence).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timingtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track dist shape: torch.Size([64, 128, 32])\n",
      "measure dist shape: torch.Size([64, 128, 32])\n",
      "measuretick dist shape: torch.Size([64, 128, 64])\n",
      "duration dist shape: torch.Size([64, 128, 128])\n",
      "pitch dist shape: torch.Size([64, 128, 128])\n",
      "loss: 23.001394271850586\n"
     ]
    }
   ],
   "source": [
    "class TimingTransformer(nn.Module):\n",
    "\n",
    "    # track: 0-31\n",
    "    # measure: 0-31\n",
    "    # measuretick: 0-63\n",
    "    # duration: 0-128\n",
    "    # pitch: 0-127\n",
    "    # return torch.tensor([track, measure, measuretick, duration, pitch], dtype=torch.int32)\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        # embedding dimensions (go into model)\n",
    "        assert a + b + c + d + e + f == C, f\"embedding dimensions must sum to C:{C}\"\n",
    "        self.track_embedding_table = nn.Embedding(32, a) # one-hot vector of length 32 (because there are 32 possible tracks) -> a\n",
    "        self.measure_embedding_table = nn.Embedding(32, b) # 32 possible measures, stored in b dimensions 32 -> b\n",
    "        self.measuretick_embedding_table = nn.Embedding(64, c)\n",
    "        self.duration_embedding_table = nn.Embedding(128, d)\n",
    "        self.pitch_embedding_table = nn.Embedding(128, e)\n",
    "        self.position_embedding_table = nn.Embedding(T, f) \n",
    "\n",
    "\n",
    "\n",
    "        # model\n",
    "        self.encoderlayers = nn.ModuleList([EncoderBlock(H, C, n_heads) for _ in range(n_layers)])\n",
    "        self.decoderlayers = nn.ModuleList([DecoderBlock(H, C, n_heads, feedforward_factor) for _ in range(n_layers)])\n",
    "\n",
    "        # de-embedding dimensions (go out of model)\n",
    "        # Output scheme: a:b track, b:c measure, c:d measuretick, d:e duration, e:f pitch, f:g position\n",
    "        # it doesn't really matter because we just try and minimize log-likelyhood of the target for each token\n",
    "\n",
    "        # first 8 dimensions are track, so we want to predict the next track, which is a distribution over the 32 tracks, so output is 32\n",
    "        # 8 -> 32\n",
    "        self.track_head = nn.Linear(a, 32)\n",
    "        # next 32 dimensions are measure, so we want to predict the next measure, which is a distribution over the 32 measures, so output is 32\n",
    "        # 32 -> 32\n",
    "        self.measure_head = nn.Linear(b, 32)\n",
    "        self.measuretick_head = nn.Linear(c, 64)\n",
    "        self.duration_head = nn.Linear(d, 128)\n",
    "        self.pitch_head = nn.Linear(e, 128)\n",
    "        self.position_head = nn.Linear(f, T) # last f dimensions are position, so we want to predict the next position, which is a distribution over the T positions, so output is T\n",
    "\n",
    "\n",
    "\n",
    "        # LEGACY GPT REFERENCE CODE\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, C) # REMOVE\n",
    "        # self.position_embedding_table = nn.Embedding(T, C) # REMOVE\n",
    "\n",
    "        # self.lm_head = nn.Linear(C, vocab_size) # REMOVE\n",
    "        # self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "        # self.block = nn.ModuleList([Block(H, C, n_heads)])\n",
    "    \n",
    "    def encode(self, idx):\n",
    "        B, T, C = idx.shape # idx is a list of the indices of the tokens, without batch it would be:\n",
    "        # [track, measure, measuretick, duration, pitch], [track, measure, measuretick, duration, pitch], ...\n",
    "        # Channel for a given token is whether we are looking at track, measure, measuretick, duration, or pitch\n",
    "\n",
    "        tracks = idx[:, :, 0]\n",
    "        measures = idx[:, :, 1]\n",
    "        measureticks = idx[:, :, 2]\n",
    "        durations = idx[:, :, 3]\n",
    "        pitches = idx[:, :, 4]\n",
    "\n",
    "        track_emb = self.track_embedding_table(tracks)\n",
    "        measure_emb = self.measure_embedding_table(measures)\n",
    "        measuretick_emb = self.measuretick_embedding_table(measureticks)\n",
    "        duration_emb = self.duration_embedding_table(durations)\n",
    "        pitch_emb = self.pitch_embedding_table(pitches)\n",
    "\n",
    "        position_sequence = self.position_embedding_table(torch.arange(T)).unsqueeze(0) # list of position embeddings, with a batch dimension, but only 1 batch\n",
    "        pos_emb = position_sequence.repeat(B, 1, 1) # repeat across the batch dimension\n",
    "\n",
    "        assert track_emb.shape == (B, T, 8), f\"track_emb shape is {track_emb.shape}\"\n",
    "        assert measure_emb.shape == (B, T, 32), f\"measure_emb shape is {measure_emb.shape}\"\n",
    "        assert measuretick_emb.shape == (B, T, 32), f\"measuretick_emb shape is {measuretick_emb.shape}\"\n",
    "        assert duration_emb.shape == (B, T, 16), f\"duration_emb shape is {duration_emb.shape}\"\n",
    "        assert pitch_emb.shape == (B, T, 32), f\"pitch_emb shape is {pitch_emb.shape}\"\n",
    "        assert pos_emb.shape == (B, T, 8), f\"pos_emb shape is {pos_emb.shape}\"\n",
    "\n",
    "\n",
    "        return torch.cat((track_emb, measure_emb, measuretick_emb, duration_emb, pitch_emb, pos_emb), dim=-1)\n",
    "\n",
    "\n",
    "    def decode(self, latent_dimension):\n",
    "        a_start, a_end = 0, a\n",
    "        b_start, b_end = a_end, a_end + b\n",
    "        c_start, c_end = b_end, b_end + c\n",
    "        d_start, d_end = c_end, c_end + d\n",
    "        e_start, e_end = d_end, d_end + e\n",
    "        f_start, f_end = e_end, e_end + f\n",
    "\n",
    "        track_emb = latent_dimension[:,:,a_start:a_end]\n",
    "        measure_emb = latent_dimension[:,:,b_start:b_end]\n",
    "        measuretick_emb = latent_dimension[:,:,c_start:c_end]\n",
    "        duration_emb = latent_dimension[:,:,d_start:d_end]\n",
    "        pitch_emb = latent_dimension[:,:,e_start:e_end]\n",
    "\n",
    "        # input representation: [track, measure, measuretick, duration, pitch, pos] (all idx's)\n",
    "\n",
    "        track_dist = self.track_head(track_emb)\n",
    "        measure_dist = self.measure_head(measure_emb)\n",
    "        measuretick_dist = self.measuretick_head(measuretick_emb)\n",
    "        duration_dist = self.duration_head(duration_emb)\n",
    "        pitch_dist = self.pitch_head(pitch_emb)\n",
    "\n",
    "        return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, idx_dirty, decoder_autoregressive_input=None, targets=None):\n",
    "        # ---- old GPT forward for reference ----\n",
    "\n",
    "        # B, T = idx.shape # idx is the indices of the tokens\n",
    "        # token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        # pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        # x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        # for layer in self.layers:\n",
    "        #     x = layer(x)\n",
    "\n",
    "        # logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size. basically next-token output distribution\n",
    "\n",
    "        # batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        # if targets is None:\n",
    "        #     return logits, None\n",
    "        # else:\n",
    "        #     # a list of all the predictions, reguardles of batch.\n",
    "        #     # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "        #     # ydim: all predictions for all batches and sequences flattened (batch_dim*sequence_dim)\n",
    "        #     logits_loss_view = logits.view(-1, vocab_size) \n",
    "        #     # targets loss view\n",
    "        #     # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "        #     # so this would be like, [1,4,5,1,2,3, ...]\n",
    "        #     # where each number is the correct next index of the one hot vector\n",
    "        #     targets_loss_view = targets.view(-1)\n",
    "        #     loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "        #     return logits, loss\n",
    "        # ---- end old GPT forward for reference ----\n",
    "        \n",
    "\n",
    "\n",
    "        # idx shape: Batch, Time, [track, measure, measuretick, duration, pitch]\n",
    "        # idx shape: Batch, Time, Channel\n",
    "        b, t, c = idx_dirty.shape\n",
    "\n",
    "        encoder_sequence = self.encode(idx_dirty)\n",
    "\n",
    "        if decoder_autoregressive_input is None:\n",
    "            decoder_sequence = encoder_sequence.clone()\n",
    "        else:\n",
    "            decoder_sequence = self.encode(decoder_autoregressive_input)\n",
    "\n",
    "\n",
    "        for encoderlayer in self.encoderlayers:\n",
    "            encoder_sequence = encoderlayer(encoder_sequence)\n",
    "\n",
    "        for decoderlayer in self.decoderlayers:\n",
    "            decoder_sequence = decoderlayer(decoder_sequence, encoder_sequence)\n",
    "\n",
    "        # decode the output\n",
    "        track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist = self.decode(decoder_sequence)\n",
    "\n",
    "        if targets is None:\n",
    "            return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, None\n",
    "        else:\n",
    "            track_targets = targets[:, :, 0]\n",
    "            measure_targets = targets[:, :, 1]\n",
    "            measuretick_targets = targets[:, :, 2]\n",
    "            duration_targets = targets[:, :, 3]\n",
    "            pitch_targets = targets[:, :, 4]\n",
    "\n",
    "\n",
    "            track_loss = F.cross_entropy(track_dist.view(b*t, 32), track_targets.view(b*t))\n",
    "            measure_loss = F.cross_entropy(measure_dist.view(b*t, 32), measure_targets.view(b*t))\n",
    "            measuretick_loss = F.cross_entropy(measuretick_dist.view(b*t, 64), measuretick_targets.view(b*t))\n",
    "            duration_loss = F.cross_entropy(duration_dist.view(b*t, 128), duration_targets.view(b*t))\n",
    "            pitch_loss = F.cross_entropy(pitch_dist.view(b*t, 128), pitch_targets.view(b*t))\n",
    "\n",
    "            total_loss = track_loss + measure_loss + measuretick_loss + duration_loss + pitch_loss\n",
    "\n",
    "\n",
    "\n",
    "            return track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self,  full_sequence, denoised_sequence, temperature=1.0):\n",
    "        # full sequence has batch dimension + padding, denoised sequence has no batch or padding\n",
    "\n",
    "        def predict_next_token(full_sequence, denoised_sequence):\n",
    "\n",
    "            # predict the next element in denoised_sequence\n",
    "            # full_sequence: the full sequence of tokens (noisy)\n",
    "            # denoised_sequence: the denoised sequence of tokens (no batch dimension, just the sequence)\n",
    "            denoised_sequence_length = denoised_sequence.shape[0]\n",
    "            last_token_idx = denoised_sequence_length - 1\n",
    "            # pad the denoised sequence to the full sequence length with tokens of [99999, 99999, 99999, 99999, 99999]\n",
    "            denoised_sequence_padded = torch.cat((denoised_sequence, torch.ones(1, T - denoised_sequence_length, 5, dtype=torch.int64) * 99999), dim=1)\n",
    "            # add batch dimension\n",
    "            # denoised_sequence_padded = denoised_sequence_padded.unsqueeze(0)\n",
    "\n",
    "            track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist = self(full_sequence, decoder_autoregressive_input=denoised_sequence_padded)\n",
    "            return\n",
    "            # get only the first batch, so we don't deal with the batch dimension\n",
    "            track_dist = track_dist[0] # shape: [T, 32] (32 possible tracks)\n",
    "            measure_dist = measure_dist[0] # shape: [T, 32] (32 possible measures)\n",
    "            measuretick_dist = measuretick_dist[0] # shape: [T, 64] (64 possible measureticks)\n",
    "            duration_dist = duration_dist[0] # shape: [T, 128] (128 possible durations)\n",
    "            pitch_dist = pitch_dist[0] # shape: [T, 128] (128 possible pitches)\n",
    "\n",
    "            # get the predictions of the last token\n",
    "            last_note_track_logits = track_dist[last_token_idx]\n",
    "            last_note_measure_logits = measure_dist[last_token_idx]\n",
    "            last_note_measuretick_logits = measuretick_dist[last_token_idx]\n",
    "            last_note_duration_logits = duration_dist[last_token_idx]\n",
    "            last_note_pitch_logits = pitch_dist[last_token_idx]\n",
    "\n",
    "            # get the most likely track, measure, etc.\n",
    "            next_track = torch.argmax(last_note_track_logits)\n",
    "            next_measure = torch.argmax(last_note_measure_logits)\n",
    "            next_measuretick = torch.argmax(last_note_measuretick_logits)\n",
    "            next_duration = torch.argmax(last_note_duration_logits)\n",
    "            next_pitch = torch.argmax(last_note_pitch_logits)\n",
    "\n",
    "            next_token =  torch.tensor([next_track, next_measure, next_measuretick, next_duration, next_pitch])\n",
    "            return next_token\n",
    "        \n",
    "        next_token = predict_next_token(full_sequence, denoised_sequence)\n",
    "        return next_token\n",
    "\n",
    "\n",
    "        # for _ in range(max_new_tokens):\n",
    "        #     logits, loss = self(idx[:,-T:])\n",
    "        #     # get the predictions of the last token\n",
    "        #     last_token_logits = logits[:, -1, :] / temperature # all batches, last token, all probabilities\n",
    "        #     # softmax to get probabilities\n",
    "        #     probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "        #     # sample from the probabilities\n",
    "        #     next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        #     # add the new token to the idx tensor\n",
    "        #     idx = torch.cat((idx, next_token), dim=1)\n",
    "        # return idx\n",
    "    \n",
    "\n",
    "\n",
    "xb = torch.randint(0, 8, (B, T, 5))\n",
    "yb = torch.randint(0, 8, (B, T, 5))\n",
    "\n",
    "model = TimingTransformer(n_layers)\n",
    "\n",
    "\n",
    "# logits, loss = model(xb, yb)\n",
    "track_dist, measure_dist, measuretick_dist, duration_dist, pitch_dist, loss = model.forward(xb, targets=yb)\n",
    "print(f\"track dist shape: {track_dist.shape}\")\n",
    "print(f\"measure dist shape: {measure_dist.shape}\")\n",
    "print(f\"measuretick dist shape: {measuretick_dist.shape}\")\n",
    "print(f\"duration dist shape: {duration_dist.shape}\")\n",
    "print(f\"pitch dist shape: {pitch_dist.shape}\")\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_idx = torch.zeros(1, T).long()\n",
    "# model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 32 32 16 32 8\n"
     ]
    }
   ],
   "source": [
    "print(a,b,c,d,e,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ensure we are running on cuda\n",
    "print(model.track_head.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2102016\n"
     ]
    }
   ],
   "source": [
    "# get the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old loop\n",
    "# eval_iters = 10\n",
    "# eval_interval = 300\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss(is_last=False):\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in ['train', 'val']:\n",
    "#         real_iters = eval_iters\n",
    "#         if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "#             real_iters *= 10 \n",
    "#         losses = torch.zeros(real_iters)\n",
    "#         for k in range(real_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean() / chars_per_token\n",
    "#     model.train()\n",
    "#     return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.softmax(torch.tensor([[0, 2, 0], [0,0,2]], dtype=torch.float), dim=1)\n",
    "print(output)\n",
    "labels = torch.tensor([1, 2], dtype=torch.int64)\n",
    "F.cross_entropy(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example forward pass using the dataset\n",
    "notes = song_dataset[3][\"notes\"]\n",
    "print(f\"original\")\n",
    "print(notes[1])\n",
    "\n",
    "notes_perturbed = song_dataset[3][\"perturbed_notes\"]\n",
    "print(f\"perturbed\")\n",
    "print(notes_perturbed[1])\n",
    "\n",
    "notes_batched = notes.unsqueeze(0)\n",
    "notes_perturbed_batched = notes_perturbed.unsqueeze(0)\n",
    "model(notes_batched, targets=notes_perturbed_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# assuming `song_dataset` is already created\n",
    "generator = torch.Generator(device=device)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(song_dataset))\n",
    "val_size = len(song_dataset) - train_size\n",
    "\n",
    "# split the dataset\n",
    "train_dataset, val_dataset = random_split(song_dataset, [train_size, val_size], generator=generator)\n",
    "batch_size=B\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, generator=generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(num_samples=None):\n",
    "    model.eval()\n",
    "    losses = {'train': 0, 'val': 0}\n",
    "    for split, dataloader in [('train', train_dataloader), ('val', val_dataloader)]:\n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        if num_samples is not None:\n",
    "            num_batches = max(min(num_batches, num_samples // dataloader.batch_size), 1)\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if num_samples is not None and i >= num_batches:\n",
    "                break\n",
    "            inputs = batch['notes']\n",
    "            targets = batch['perturbed_notes']\n",
    "            _, _, _, _, _, loss = model(inputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        losses[split] = avg_loss\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_loss(num_samples = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# define the number of epochs for training\n",
    "num_epochs = config['num_epochs']\n",
    "eval_interval = config['eval_interval']\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        inputs = batch['notes']\n",
    "        targets = batch['perturbed_notes']  # or whatever your target is\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute the model output\n",
    "        _, _, _, _, _, loss = model(inputs, targets)\n",
    "\n",
    "        # backward pass: compute the gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 and i > 0:\n",
    "            eval_loss = estimate_loss(num_samples=50)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {eval_loss['val']}, Training Loss: {eval_loss['train']}\")\n",
    "\n",
    "    # compute the average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "# save the model after training\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training profiling\n",
    "```\n",
    "Epoch 1/4:   1%|          | 200/16315 [01:10<1:34:04,  2.85it/s]\n",
    "Average Data loading time: 0.0000s\n",
    "Average Forward pass time: 0.0143s\n",
    "Average Backward pass time: 0.0194s\n",
    "Average Gradient update time: 0.0105s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(device)\n",
    "torch.set_default_device(device)\n",
    "# load model.pth\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(model.track_embedding_table.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# set default device to cuda\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# NOTE TO SELF: Load the song_dataset above before running this code\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# first_song = song_dataset[0]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(f\"original\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(notes[1])\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m notes_perturbed \u001b[38;5;241m=\u001b[39m \u001b[43msong_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperturbed_notes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(f\"perturbed\")\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(notes_perturbed[1])\u001b[39;00m\n\u001b[1;32m     26\u001b[0m notes_batched \u001b[38;5;241m=\u001b[39m notes_perturbed\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mSongDataSet.__getitem__\u001b[0;34m(self, idx, different_perturbation_std)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Note format: [track, start, duration, pitch]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m time_per_quarter_note \u001b[38;5;241m=\u001b[39m song[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# header, first element\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m time_per_tick \u001b[38;5;241m=\u001b[39m (\u001b[43mtime_per_quarter_note\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize_divisor)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# quantize_divisor is ticks per measure\u001b[39;00m\n\u001b[1;32m     25\u001b[0m perturbed_notes \u001b[38;5;241m=\u001b[39m notes\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# add gausian perturbations to start and duration, note index 1 and 2\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# return torch.normal(0, self.perturbation_std, perturbed_notes[:, 1].shape).shape\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# set default device to cuda\n",
    "# NOTE TO SELF: Load the song_dataset above before running this code\n",
    "# first_song = song_dataset[0]\n",
    "\n",
    "# # get the notes tensor\n",
    "# notes = first_song['notes']\n",
    "# perturbed_notes = first_song['perturbed_notes']\n",
    "\n",
    "\n",
    "# perturbed_notes_batched = perturbed_notes.unsqueeze(0)\n",
    "# denoised_sequence = torch.tensor([], dtype=torch.int64)\n",
    "# # model.generate(perturbed_notes_batched, denoised_sequence)\n",
    "# model(perturbed_notes_batched)\n",
    "\n",
    "\n",
    "\n",
    "# Example forward pass using the dataset\n",
    "# notes = song_dataset[3][\"notes\"].to(device)\n",
    "# print(f\"original\")\n",
    "# print(notes[1])\n",
    "\n",
    "notes_perturbed = song_dataset[3][\"perturbed_notes\"]\n",
    "# print(f\"perturbed\")\n",
    "# print(notes_perturbed[1])\n",
    "\n",
    "notes_batched = notes_perturbed.unsqueeze(0)\n",
    "print(notes_batched.shape)\n",
    "\n",
    "autoregressive_sequence = torch.tensor([], dtype=torch.int64, device=device)\n",
    "# pad the autoregressive sequence to the full sequence length with tokens of [99999, 99999, 99999, 99999, 99999]\n",
    "pad_tensor = torch.ones(1, 5, dtype=torch.int64, device=device) * 99999\n",
    "\n",
    "while len(autoregressive_sequence) < T:\n",
    "    autoregressive_sequence = torch.cat((autoregressive_sequence, pad_tensor), dim=0)\n",
    "\n",
    "# add a batch dimension to the autoregressive sequence\n",
    "autoregressive_sequence = autoregressive_sequence.unsqueeze(0)\n",
    "\n",
    "print(notes_batched.shape, notes_batched.device)\n",
    "print(autoregressive_sequence.shape, autoregressive_sequence.device)\n",
    "\n",
    "\n",
    "# model(notes_batched, decoder_autoregressive_input=autoregressive_sequence)\n",
    "model(notes_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model(notes_batched, decoder_autoregressive_input=autoregressive_sequence)\n",
    "pad_tensor = torch.tensor([[99999, 99999, 99999, 99999, 99999]], dtype=torch.int64)\n",
    "real_tensor = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.int64)\n",
    "while len(real_tensor) < 10:\n",
    "    real_tensor = torch.cat((real_tensor, pad_tensor), dim=0)\n",
    "\n",
    "print(real_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
